{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Variational Auto Encoder - Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.datasets as dset\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO, TraceEnum_ELBO, config_enumerate\n",
    "from pyro.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image # Module for image rotation (making use of PIL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FRDEEP import FRDEEPF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyro.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download FRDEEP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader_first():\n",
    "    transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize([0.5],[0.5])])\n",
    "\n",
    "    trainset = FRDEEPF(root='./FIRST_data', train=True, download=True, transform=transform)  \n",
    "    trainloader = torch.utils.data.DataLoader(trainset, shuffle=True, num_workers=2, batch_size=len(trainset))\n",
    "    \n",
    "    classes = ('FRI', 'FRII') #First class if FR1 and second class is FR2\n",
    "    \n",
    "    array_train= next(iter(trainloader))[0].numpy() # Training Datasets is loaded in numpy array\n",
    "    array_label= next(iter(trainloader))[1].numpy()\n",
    "    \n",
    "    augmented_data=np.zeros((19800,1,100,100))\n",
    "    \n",
    "    augmented_data_label = np.zeros((19800,1))\n",
    "    \n",
    "    count=0\n",
    "    \n",
    "    for j in range(0,550):\n",
    "        image_object=Image.fromarray(array_train[j,0,:,:])\n",
    "        for i in range(0,36):\n",
    "            rotated=image_object.rotate(i*10)\n",
    "            imgarr = np.array(rotated)\n",
    "            temp_img_array=imgarr[25:125,25:125]\n",
    "            augmented_data[count,0,:,:]=temp_img_array\n",
    "            augmented_data_label[count,:]=array_label[j]\n",
    "            count+=1\n",
    "    augmented_data=(augmented_data-np.min(augmented_data))/(np.max(augmented_data)-np.min(augmented_data))\n",
    "    \n",
    "    X=augmented_data\n",
    "    Y=augmented_data_label\n",
    "    \n",
    "    X_random_mix=np.take(X,np.random.RandomState(seed=42).permutation(X.shape[0]),axis=0,out=X)\n",
    "    Y_random_mix=np.take(Y,np.random.RandomState(seed=42).permutation(Y.shape[0]),axis=0,out=Y)\n",
    "    \n",
    "    tensor_x = torch.stack([torch.Tensor(i) for i in X_random_mix])\n",
    "    tensor_y = torch.stack([torch.Tensor(i) for i in Y_random_mix])\n",
    "    \n",
    "    first_augmented_dataset = torch.utils.data.TensorDataset(tensor_x,tensor_y)\n",
    "    \n",
    "    first_dataloader = torch.utils.data.DataLoader(first_augmented_dataset,batch_size=100, shuffle=True) # create your dataloader\n",
    "    \n",
    "    #--------------------------------------Add Section for Test data------------------------------------\n",
    "    \n",
    "    return first_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.enable_validation(True)\n",
    "pyro.distributions.enable_validation(False)\n",
    "pyro.set_rng_seed(0)\n",
    "# Enable smoke test - run the notebook cells on CI.\n",
    "smoke_test = 'CI' in os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loading and batching MNIST dataset\n",
    "def setup_data_loaders(batch_size=128, use_cuda=True):\n",
    "    root = './data'\n",
    "    download = True\n",
    "    trans = transforms.ToTensor()\n",
    "    train_set = dset.MNIST(root=root, train=True, transform=trans,\n",
    "                           download=download)\n",
    "    test_set = dset.MNIST(root=root, train=False, transform=trans)\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': use_cuda}\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_set,\n",
    "        batch_size=batch_size, shuffle=False, **kwargs)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rework on EncoderZ and Decoder so that these adapts to FIRST Radio Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderZ(nn.Module):\n",
    "    #def __init__(self, z_dim, hidden_dim):\n",
    "    def __init__(self, x_dim, y_dim, h_dim, z_dim):\n",
    "        super(EncoderZ, self).__init__()\n",
    "        # setup the three linear transformations used\n",
    "        #have to here define the fully connected layers - 784 to 400\n",
    "        #400 to 2\n",
    "        \n",
    "        # setup the non-linearities\n",
    "        self.fc1 = nn.Linear(x_dim+y_dim, h_dim)  \n",
    "        self.fc21 = nn.Linear(h_dim, z_dim) \n",
    "        self.fc22 = nn.Linear(h_dim, z_dim)\n",
    "        \n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, x_y_2):\n",
    "        [x,y]=x_y_2\n",
    "        x = x.reshape(-1, 10000) #@David Change this to reshape if something fucks up\n",
    "        y = y.reshape(-1, 2) #@David Change this to reshape if something fucks up\n",
    "        x_y_1 = torch.cat((x,y), dim=1) #I think that this should concatenate the two inputs if this does work then test it independenlty\n",
    "        x_y_1 = x_y_1.view(x_y_1.size(0), -1)\n",
    "        \n",
    "        hidden = self.softplus(self.fc1(x_y_1))\n",
    "        \n",
    "        z_loc = self.fc21(hidden)\n",
    "        z_scale = torch.exp(self.fc22(hidden)) # mu, log_var\n",
    "        return z_loc, z_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, x_dim, y_dim, h_dim, z_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        # setup the two linear transformations used\n",
    "        self.fc1 = nn.Linear(z_dim+y_dim, h_dim)\n",
    "        self.fc21 = nn.Linear(h_dim, x_dim)\n",
    "        # setup the non-linearities\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,z_y_2):\n",
    "        # define the forward computation on the latent z\n",
    "        # first compute the hidden units\n",
    "        \n",
    "        [z,y]=z_y_2\n",
    "        \n",
    "        z = z.reshape(-1, 2) #@David Change this to reshape if something fucks up\n",
    "        y = y.reshape(-1, 2)\n",
    "        z_y_1 = torch.cat((z,y), dim=1)\n",
    "        z_y_1 = z_y_1.view(z_y_1.size(0), -1)\n",
    "        hidden = self.softplus(self.fc1(z_y_1))\n",
    "        # return the parameter for the output Bernoulli\n",
    "        # each is of size batch_size x 784\n",
    "        loc_img = self.sigmoid(self.fc21(hidden))\n",
    "        return loc_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    # by default our latent space is 50-dimensional\n",
    "    # and we use 400 hidden units\n",
    "    def __init__(self, x_dim = 10000, y_dim = 2, h_dim = 500, z_dim = 2,use_cuda=True):\n",
    "        super(VAE, self).__init__()\n",
    "    \n",
    "        # create the encoder and decoder networks\n",
    "        # a split in the final layer's size is used for multiple outputs\n",
    "        # and potentially applying separate activation functions on them\n",
    "        # e.g. in this network the final output is of size [z_dim,z_dim]\n",
    "        # to produce loc and scale, and apply different activations [None,Exp] on them\n",
    "              \n",
    "        self.encoder_z = EncoderZ(x_dim, y_dim, h_dim, z_dim)\n",
    "        \n",
    "        self.decoder = Decoder(x_dim, y_dim, h_dim, z_dim)\n",
    "\n",
    "        if use_cuda:\n",
    "            # calling cuda() here will put all the parameters of\n",
    "            # the encoder and decoder networks into gpu memory\n",
    "            self.cuda()\n",
    "            \n",
    "            \n",
    "        self.use_cuda = use_cuda\n",
    "        self.z_dim = z_dim\n",
    "        self.output_size = y_dim\n",
    "        \n",
    "        \n",
    "    # define the model p(x|z)p(z)\n",
    "    def model(self, xs, ys):\n",
    "        # register this pytorch module and all of its sub-modules with pyro\n",
    "        pyro.module(\"ss_vae\", self)\n",
    "        batch_size = xs.size(0)\n",
    "\n",
    "        # inform Pyro that the variables in the batch of xs, ys are conditionally independent\n",
    "        with pyro.plate(\"data\"):\n",
    "\n",
    "            # sample the handwriting style from the constant prior distribution\n",
    "            prior_loc = xs.new_zeros([batch_size, self.z_dim])\n",
    "            prior_scale = xs.new_ones([batch_size, self.z_dim])\n",
    "            zs = pyro.sample(\"z\", dist.Normal(prior_loc, prior_scale).to_event(1))\n",
    "\n",
    "            # if the label y (which digit to write) is supervised, sample from the\n",
    "            # constant prior, otherwise, observe the value (i.e. score it against the constant prior)\n",
    "            alpha_prior = xs.new_ones([batch_size, self.output_size]) / (1.0 * self.output_size)\n",
    "            ys = pyro.sample(\"y\", dist.OneHotCategorical(alpha_prior), obs=ys)\n",
    "            \n",
    "            # finally, score the image (x) using the handwriting style (z) and\n",
    "            # the class label y (which digit to write) against the\n",
    "            # parametrized distribution p(x|y,z) = bernoulli(decoder(y,z))\n",
    "            # where `decoder` is a neural network\n",
    "            loc = self.decoder.forward([zs, ys])\n",
    "            pyro.sample(\"x\", dist.Bernoulli(loc).to_event(1), obs=xs)\n",
    "            \n",
    "    def guide(self, xs, ys):\n",
    "        with pyro.plate(\"data\"):\n",
    "           # if the class label (the digit) is not supervised, sample\n",
    "           # (and score) the digit with the variational distribution\n",
    "           # q(y|x) = categorical(alpha(x))\n",
    "           \n",
    "            #-------------------REMOVED THIS PART FOR THE CLASSIFIER ASSUME ALL DATA ARE LABELLED---------\n",
    "\n",
    "           # sample (and score) the latent handwriting-style with the variational\n",
    "           # distribution q(z|x,y) = normal(loc(x,y),scale(x,y))\n",
    "           loc, scale = self.encoder_z.forward([xs, ys])\n",
    "           pyro.sample(\"z\", dist.Normal(loc, scale).to_event(1))\n",
    "\n",
    "    # define a helper function for reconstructing images\n",
    "    def reconstruct_img(self, xs, ys):\n",
    "        # encode image x\n",
    "        z_loc, z_scale = self.encoder_z.forward([xs,ys])\n",
    "        # sample in latent space\n",
    "        z = dist.Normal(z_loc, z_scale).sample()\n",
    "        # decode the image (note we don't sample in image space)\n",
    "        loc_img = self.decoder.forward([zs,ys])\n",
    "        \n",
    "        return loc_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(svi, train_loader, use_cuda=True):\n",
    "    # initialize loss accumulator\n",
    "    epoch_loss = 0.\n",
    "    # do a training epoch over each mini-batch x returned\n",
    "    # by the data loader\n",
    "    for x, y in train_loader:\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "        # do ELBO gradient and accumulate loss\n",
    "        labels_y = torch.tensor(np.zeros((y.shape[0],2)))\n",
    "        for j in range (0,y.shape[0]):\n",
    "            labels_y[j,int(y[j][0].numpy())] = 1\n",
    "        epoch_loss += svi.step(x.reshape(-1,10000),labels_y.cuda().float())\n",
    "\n",
    "    # return epoch loss\n",
    "    normalizer_train = len(train_loader.dataset)\n",
    "    total_epoch_loss_train = epoch_loss / normalizer_train\n",
    "    return total_epoch_loss_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run options\n",
    "LEARNING_RATE = 1.0e-3\n",
    "USE_CUDA = True\n",
    "\n",
    "# Run only for a single iteration for testing\n",
    "NUM_EPOCHS = 3000\n",
    "TEST_FREQUENCY = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "[epoch 000]  average training loss: 610.2937\n",
      "[epoch 001]  average training loss: 191.1655\n",
      "[epoch 002]  average training loss: 181.1567\n",
      "[epoch 003]  average training loss: 174.8715\n",
      "[epoch 004]  average training loss: 169.4573\n",
      "[epoch 005]  average training loss: 165.6234\n",
      "[epoch 006]  average training loss: 162.9624\n",
      "[epoch 007]  average training loss: 160.7584\n",
      "[epoch 008]  average training loss: 159.2194\n",
      "[epoch 009]  average training loss: 158.1322\n",
      "[epoch 010]  average training loss: 157.3086\n",
      "[epoch 011]  average training loss: 156.6182\n",
      "[epoch 012]  average training loss: 156.0975\n",
      "[epoch 013]  average training loss: 155.5964\n",
      "[epoch 014]  average training loss: 155.1647\n",
      "[epoch 015]  average training loss: 154.8228\n",
      "[epoch 016]  average training loss: 154.4763\n",
      "[epoch 017]  average training loss: 154.1744\n",
      "[epoch 018]  average training loss: 153.9066\n",
      "[epoch 019]  average training loss: 153.6657\n",
      "[epoch 020]  average training loss: 153.4260\n",
      "[epoch 021]  average training loss: 153.2275\n",
      "[epoch 022]  average training loss: 153.0734\n",
      "[epoch 023]  average training loss: 152.8810\n",
      "[epoch 024]  average training loss: 152.7662\n",
      "[epoch 025]  average training loss: 152.6291\n",
      "[epoch 026]  average training loss: 152.4835\n",
      "[epoch 027]  average training loss: 152.3981\n",
      "[epoch 028]  average training loss: 152.3268\n",
      "[epoch 029]  average training loss: 152.2164\n",
      "[epoch 030]  average training loss: 152.1659\n",
      "[epoch 031]  average training loss: 152.0773\n",
      "[epoch 032]  average training loss: 152.0039\n",
      "[epoch 033]  average training loss: 151.9614\n",
      "[epoch 034]  average training loss: 151.8940\n",
      "[epoch 035]  average training loss: 151.8722\n",
      "[epoch 036]  average training loss: 151.8076\n",
      "[epoch 037]  average training loss: 151.7666\n",
      "[epoch 038]  average training loss: 151.7068\n",
      "[epoch 039]  average training loss: 151.6882\n",
      "[epoch 040]  average training loss: 151.6283\n",
      "[epoch 041]  average training loss: 151.6288\n",
      "[epoch 042]  average training loss: 151.5579\n",
      "[epoch 043]  average training loss: 151.5111\n",
      "[epoch 044]  average training loss: 151.5059\n",
      "[epoch 045]  average training loss: 151.4261\n",
      "[epoch 046]  average training loss: 151.3900\n",
      "[epoch 047]  average training loss: 151.3647\n",
      "[epoch 048]  average training loss: 151.3293\n",
      "[epoch 049]  average training loss: 151.2955\n",
      "[epoch 050]  average training loss: 151.2798\n",
      "[epoch 051]  average training loss: 151.2202\n",
      "[epoch 052]  average training loss: 151.1667\n",
      "[epoch 053]  average training loss: 151.1587\n",
      "[epoch 054]  average training loss: 151.0734\n",
      "[epoch 055]  average training loss: 151.0608\n",
      "[epoch 056]  average training loss: 151.0238\n",
      "[epoch 057]  average training loss: 151.0037\n",
      "[epoch 058]  average training loss: 150.9417\n",
      "[epoch 059]  average training loss: 150.9513\n",
      "[epoch 060]  average training loss: 150.9194\n",
      "[epoch 061]  average training loss: 150.8690\n",
      "[epoch 062]  average training loss: 150.8531\n",
      "[epoch 063]  average training loss: 150.8202\n",
      "[epoch 064]  average training loss: 150.8252\n",
      "[epoch 065]  average training loss: 150.7724\n",
      "[epoch 066]  average training loss: 150.7278\n",
      "[epoch 067]  average training loss: 150.7417\n",
      "[epoch 068]  average training loss: 150.7192\n",
      "[epoch 069]  average training loss: 150.6810\n",
      "[epoch 070]  average training loss: 150.6366\n",
      "[epoch 071]  average training loss: 150.6165\n",
      "[epoch 072]  average training loss: 150.5818\n",
      "[epoch 073]  average training loss: 150.5688\n",
      "[epoch 074]  average training loss: 150.5365\n",
      "[epoch 075]  average training loss: 150.5474\n",
      "[epoch 076]  average training loss: 150.5443\n",
      "[epoch 077]  average training loss: 150.4840\n",
      "[epoch 078]  average training loss: 150.4547\n",
      "[epoch 079]  average training loss: 150.4371\n",
      "[epoch 080]  average training loss: 150.4124\n",
      "[epoch 081]  average training loss: 150.3936\n",
      "[epoch 082]  average training loss: 150.3708\n",
      "[epoch 083]  average training loss: 150.3357\n",
      "[epoch 084]  average training loss: 150.3500\n",
      "[epoch 085]  average training loss: 150.3274\n",
      "[epoch 086]  average training loss: 150.3071\n",
      "[epoch 087]  average training loss: 150.2854\n",
      "[epoch 088]  average training loss: 150.2721\n",
      "[epoch 089]  average training loss: 150.2607\n",
      "[epoch 090]  average training loss: 150.2362\n",
      "[epoch 091]  average training loss: 150.2008\n",
      "[epoch 092]  average training loss: 150.1870\n",
      "[epoch 093]  average training loss: 150.1850\n",
      "[epoch 094]  average training loss: 150.1427\n",
      "[epoch 095]  average training loss: 150.1198\n",
      "[epoch 096]  average training loss: 150.0973\n",
      "[epoch 097]  average training loss: 150.0861\n",
      "[epoch 098]  average training loss: 150.0834\n",
      "[epoch 099]  average training loss: 150.0666\n",
      "[epoch 100]  average training loss: 150.0243\n",
      "[epoch 101]  average training loss: 150.0262\n",
      "[epoch 102]  average training loss: 149.9798\n",
      "[epoch 103]  average training loss: 149.9777\n",
      "[epoch 104]  average training loss: 149.9552\n",
      "[epoch 105]  average training loss: 149.8996\n",
      "[epoch 106]  average training loss: 149.8921\n",
      "[epoch 107]  average training loss: 149.8839\n",
      "[epoch 108]  average training loss: 149.8676\n",
      "[epoch 109]  average training loss: 149.8106\n",
      "[epoch 110]  average training loss: 149.8117\n",
      "[epoch 111]  average training loss: 149.7819\n",
      "[epoch 112]  average training loss: 149.7374\n",
      "[epoch 113]  average training loss: 149.7014\n",
      "[epoch 114]  average training loss: 149.6824\n",
      "[epoch 115]  average training loss: 149.6217\n",
      "[epoch 116]  average training loss: 149.6166\n",
      "[epoch 117]  average training loss: 149.5819\n",
      "[epoch 118]  average training loss: 149.5169\n",
      "[epoch 119]  average training loss: 149.4652\n",
      "[epoch 120]  average training loss: 149.3889\n",
      "[epoch 121]  average training loss: 149.3753\n",
      "[epoch 122]  average training loss: 149.2726\n",
      "[epoch 123]  average training loss: 149.2249\n",
      "[epoch 124]  average training loss: 149.1856\n",
      "[epoch 125]  average training loss: 149.1326\n",
      "[epoch 126]  average training loss: 149.0373\n",
      "[epoch 127]  average training loss: 148.9676\n",
      "[epoch 128]  average training loss: 148.8996\n",
      "[epoch 129]  average training loss: 148.8702\n",
      "[epoch 130]  average training loss: 148.7607\n",
      "[epoch 131]  average training loss: 148.6858\n",
      "[epoch 132]  average training loss: 148.6020\n",
      "[epoch 133]  average training loss: 148.5459\n",
      "[epoch 134]  average training loss: 148.5150\n",
      "[epoch 135]  average training loss: 148.4336\n",
      "[epoch 136]  average training loss: 148.3551\n",
      "[epoch 137]  average training loss: 148.2840\n",
      "[epoch 138]  average training loss: 148.2276\n",
      "[epoch 139]  average training loss: 148.1844\n",
      "[epoch 140]  average training loss: 148.0915\n",
      "[epoch 141]  average training loss: 148.0426\n",
      "[epoch 142]  average training loss: 147.9676\n",
      "[epoch 143]  average training loss: 147.8990\n",
      "[epoch 144]  average training loss: 147.8408\n",
      "[epoch 145]  average training loss: 147.7805\n",
      "[epoch 146]  average training loss: 147.7368\n",
      "[epoch 147]  average training loss: 147.6570\n",
      "[epoch 148]  average training loss: 147.5667\n",
      "[epoch 149]  average training loss: 147.5293\n",
      "[epoch 150]  average training loss: 147.4210\n",
      "[epoch 151]  average training loss: 147.3742\n",
      "[epoch 152]  average training loss: 147.3188\n",
      "[epoch 153]  average training loss: 147.2338\n",
      "[epoch 154]  average training loss: 147.1802\n",
      "[epoch 155]  average training loss: 147.1003\n",
      "[epoch 156]  average training loss: 147.0545\n",
      "[epoch 157]  average training loss: 146.9783\n",
      "[epoch 158]  average training loss: 146.9058\n",
      "[epoch 159]  average training loss: 146.8333\n",
      "[epoch 160]  average training loss: 146.7621\n",
      "[epoch 161]  average training loss: 146.7044\n",
      "[epoch 162]  average training loss: 146.6521\n",
      "[epoch 163]  average training loss: 146.5821\n",
      "[epoch 164]  average training loss: 146.5201\n",
      "[epoch 165]  average training loss: 146.4443\n",
      "[epoch 166]  average training loss: 146.3937\n",
      "[epoch 167]  average training loss: 146.3289\n",
      "[epoch 168]  average training loss: 146.2334\n",
      "[epoch 169]  average training loss: 146.1788\n",
      "[epoch 170]  average training loss: 146.0948\n",
      "[epoch 171]  average training loss: 146.0179\n",
      "[epoch 172]  average training loss: 145.9722\n",
      "[epoch 173]  average training loss: 145.9352\n",
      "[epoch 174]  average training loss: 145.8550\n",
      "[epoch 175]  average training loss: 145.8427\n",
      "[epoch 176]  average training loss: 145.7382\n",
      "[epoch 177]  average training loss: 145.6872\n",
      "[epoch 178]  average training loss: 145.6258\n",
      "[epoch 179]  average training loss: 145.5894\n",
      "[epoch 180]  average training loss: 145.5208\n",
      "[epoch 181]  average training loss: 145.4206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 182]  average training loss: 145.4125\n",
      "[epoch 183]  average training loss: 145.3066\n",
      "[epoch 184]  average training loss: 145.2882\n",
      "[epoch 185]  average training loss: 145.2331\n",
      "[epoch 186]  average training loss: 145.1808\n",
      "[epoch 187]  average training loss: 145.1239\n",
      "[epoch 188]  average training loss: 145.0797\n",
      "[epoch 189]  average training loss: 145.0200\n",
      "[epoch 190]  average training loss: 144.9868\n",
      "[epoch 191]  average training loss: 144.9312\n",
      "[epoch 192]  average training loss: 144.9051\n",
      "[epoch 193]  average training loss: 144.8339\n",
      "[epoch 194]  average training loss: 144.7859\n",
      "[epoch 195]  average training loss: 144.7345\n",
      "[epoch 196]  average training loss: 144.6767\n",
      "[epoch 197]  average training loss: 144.6418\n",
      "[epoch 198]  average training loss: 144.6161\n",
      "[epoch 199]  average training loss: 144.5411\n",
      "[epoch 200]  average training loss: 144.5048\n",
      "[epoch 201]  average training loss: 144.4770\n",
      "[epoch 202]  average training loss: 144.4428\n",
      "[epoch 203]  average training loss: 144.3908\n",
      "[epoch 204]  average training loss: 144.3765\n",
      "[epoch 205]  average training loss: 144.3184\n",
      "[epoch 206]  average training loss: 144.2587\n",
      "[epoch 207]  average training loss: 144.2446\n",
      "[epoch 208]  average training loss: 144.1817\n",
      "[epoch 209]  average training loss: 144.1452\n",
      "[epoch 210]  average training loss: 144.1317\n",
      "[epoch 211]  average training loss: 144.0466\n",
      "[epoch 212]  average training loss: 144.0579\n",
      "[epoch 213]  average training loss: 143.9984\n",
      "[epoch 214]  average training loss: 143.9864\n",
      "[epoch 215]  average training loss: 143.9561\n",
      "[epoch 216]  average training loss: 143.8921\n",
      "[epoch 217]  average training loss: 143.8648\n",
      "[epoch 218]  average training loss: 143.8355\n",
      "[epoch 219]  average training loss: 143.7915\n",
      "[epoch 220]  average training loss: 143.7937\n",
      "[epoch 221]  average training loss: 143.7521\n",
      "[epoch 222]  average training loss: 143.6723\n",
      "[epoch 223]  average training loss: 143.6666\n",
      "[epoch 224]  average training loss: 143.6456\n",
      "[epoch 225]  average training loss: 143.6088\n",
      "[epoch 226]  average training loss: 143.6044\n",
      "[epoch 227]  average training loss: 143.5645\n",
      "[epoch 228]  average training loss: 143.5445\n",
      "[epoch 229]  average training loss: 143.5174\n",
      "[epoch 230]  average training loss: 143.4447\n",
      "[epoch 231]  average training loss: 143.4470\n",
      "[epoch 232]  average training loss: 143.4038\n",
      "[epoch 233]  average training loss: 143.3764\n",
      "[epoch 234]  average training loss: 143.3221\n",
      "[epoch 235]  average training loss: 143.3031\n",
      "[epoch 236]  average training loss: 143.2867\n",
      "[epoch 237]  average training loss: 143.2866\n",
      "[epoch 238]  average training loss: 143.2751\n",
      "[epoch 239]  average training loss: 143.2059\n",
      "[epoch 240]  average training loss: 143.2171\n",
      "[epoch 241]  average training loss: 143.1702\n",
      "[epoch 242]  average training loss: 143.1144\n",
      "[epoch 243]  average training loss: 143.1332\n",
      "[epoch 244]  average training loss: 143.0825\n",
      "[epoch 245]  average training loss: 143.0691\n",
      "[epoch 246]  average training loss: 143.0087\n",
      "[epoch 247]  average training loss: 143.0035\n",
      "[epoch 248]  average training loss: 142.9734\n",
      "[epoch 249]  average training loss: 142.9295\n",
      "[epoch 250]  average training loss: 142.9349\n",
      "[epoch 251]  average training loss: 142.9273\n",
      "[epoch 252]  average training loss: 142.8832\n",
      "[epoch 253]  average training loss: 142.9293\n",
      "[epoch 254]  average training loss: 142.8287\n",
      "[epoch 255]  average training loss: 142.8510\n",
      "[epoch 256]  average training loss: 142.8187\n",
      "[epoch 257]  average training loss: 142.7976\n",
      "[epoch 258]  average training loss: 142.7750\n",
      "[epoch 259]  average training loss: 142.7342\n",
      "[epoch 260]  average training loss: 142.7127\n",
      "[epoch 261]  average training loss: 142.6883\n",
      "[epoch 262]  average training loss: 142.6583\n",
      "[epoch 263]  average training loss: 142.6571\n",
      "[epoch 264]  average training loss: 142.6279\n",
      "[epoch 265]  average training loss: 142.6102\n",
      "[epoch 266]  average training loss: 142.5883\n",
      "[epoch 267]  average training loss: 142.5808\n",
      "[epoch 268]  average training loss: 142.5420\n",
      "[epoch 269]  average training loss: 142.5502\n",
      "[epoch 270]  average training loss: 142.4942\n",
      "[epoch 271]  average training loss: 142.4970\n",
      "[epoch 272]  average training loss: 142.4707\n",
      "[epoch 273]  average training loss: 142.4239\n",
      "[epoch 274]  average training loss: 142.4501\n",
      "[epoch 275]  average training loss: 142.4024\n",
      "[epoch 276]  average training loss: 142.3877\n",
      "[epoch 277]  average training loss: 142.3692\n",
      "[epoch 278]  average training loss: 142.3552\n",
      "[epoch 279]  average training loss: 142.3306\n",
      "[epoch 280]  average training loss: 142.2981\n",
      "[epoch 281]  average training loss: 142.2804\n",
      "[epoch 282]  average training loss: 142.2745\n",
      "[epoch 283]  average training loss: 142.2507\n",
      "[epoch 284]  average training loss: 142.2105\n",
      "[epoch 285]  average training loss: 142.2147\n",
      "[epoch 286]  average training loss: 142.1982\n",
      "[epoch 287]  average training loss: 142.1829\n",
      "[epoch 288]  average training loss: 142.1524\n",
      "[epoch 289]  average training loss: 142.1601\n",
      "[epoch 290]  average training loss: 142.1316\n",
      "[epoch 291]  average training loss: 142.0940\n",
      "[epoch 292]  average training loss: 142.1320\n",
      "[epoch 293]  average training loss: 142.1190\n",
      "[epoch 294]  average training loss: 142.0782\n",
      "[epoch 295]  average training loss: 142.0499\n",
      "[epoch 296]  average training loss: 142.0180\n",
      "[epoch 297]  average training loss: 141.9723\n",
      "[epoch 298]  average training loss: 141.9815\n",
      "[epoch 299]  average training loss: 141.9921\n",
      "[epoch 300]  average training loss: 141.9579\n",
      "[epoch 301]  average training loss: 141.9479\n",
      "[epoch 302]  average training loss: 141.8951\n",
      "[epoch 303]  average training loss: 141.9082\n",
      "[epoch 304]  average training loss: 141.9103\n",
      "[epoch 305]  average training loss: 141.8763\n",
      "[epoch 306]  average training loss: 141.8657\n",
      "[epoch 307]  average training loss: 141.8206\n",
      "[epoch 308]  average training loss: 141.8749\n",
      "[epoch 309]  average training loss: 141.7994\n",
      "[epoch 310]  average training loss: 141.7868\n",
      "[epoch 311]  average training loss: 141.8003\n",
      "[epoch 312]  average training loss: 141.7797\n",
      "[epoch 313]  average training loss: 141.7618\n",
      "[epoch 314]  average training loss: 141.7378\n",
      "[epoch 315]  average training loss: 141.7344\n",
      "[epoch 316]  average training loss: 141.7229\n",
      "[epoch 317]  average training loss: 141.7091\n",
      "[epoch 318]  average training loss: 141.7029\n",
      "[epoch 319]  average training loss: 141.6771\n",
      "[epoch 320]  average training loss: 141.6847\n",
      "[epoch 321]  average training loss: 141.6435\n",
      "[epoch 322]  average training loss: 141.6219\n",
      "[epoch 323]  average training loss: 141.6254\n",
      "[epoch 324]  average training loss: 141.6216\n",
      "[epoch 325]  average training loss: 141.5729\n",
      "[epoch 326]  average training loss: 141.5686\n",
      "[epoch 327]  average training loss: 141.5579\n",
      "[epoch 328]  average training loss: 141.5484\n",
      "[epoch 329]  average training loss: 141.5360\n",
      "[epoch 330]  average training loss: 141.5228\n",
      "[epoch 331]  average training loss: 141.5144\n",
      "[epoch 332]  average training loss: 141.4795\n",
      "[epoch 333]  average training loss: 141.4554\n",
      "[epoch 334]  average training loss: 141.4797\n",
      "[epoch 335]  average training loss: 141.4313\n",
      "[epoch 336]  average training loss: 141.4332\n",
      "[epoch 337]  average training loss: 141.4224\n",
      "[epoch 338]  average training loss: 141.3968\n",
      "[epoch 339]  average training loss: 141.3969\n",
      "[epoch 340]  average training loss: 141.4038\n",
      "[epoch 341]  average training loss: 141.3929\n",
      "[epoch 342]  average training loss: 141.3659\n",
      "[epoch 343]  average training loss: 141.3462\n",
      "[epoch 344]  average training loss: 141.3631\n",
      "[epoch 345]  average training loss: 141.3429\n",
      "[epoch 346]  average training loss: 141.2980\n",
      "[epoch 347]  average training loss: 141.2759\n",
      "[epoch 348]  average training loss: 141.2775\n",
      "[epoch 349]  average training loss: 141.2605\n",
      "[epoch 350]  average training loss: 141.2742\n",
      "[epoch 351]  average training loss: 141.2356\n",
      "[epoch 352]  average training loss: 141.2508\n",
      "[epoch 353]  average training loss: 141.2272\n",
      "[epoch 354]  average training loss: 141.2243\n",
      "[epoch 355]  average training loss: 141.2155\n",
      "[epoch 356]  average training loss: 141.1609\n",
      "[epoch 357]  average training loss: 141.1658\n",
      "[epoch 358]  average training loss: 141.1861\n",
      "[epoch 359]  average training loss: 141.1728\n",
      "[epoch 360]  average training loss: 141.1694\n",
      "[epoch 361]  average training loss: 141.1400\n",
      "[epoch 362]  average training loss: 141.1219\n",
      "[epoch 363]  average training loss: 141.0776\n",
      "[epoch 364]  average training loss: 141.0908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 365]  average training loss: 141.0776\n",
      "[epoch 366]  average training loss: 141.0769\n",
      "[epoch 367]  average training loss: 141.0558\n",
      "[epoch 368]  average training loss: 141.0609\n",
      "[epoch 369]  average training loss: 141.0529\n",
      "[epoch 370]  average training loss: 141.0167\n",
      "[epoch 371]  average training loss: 141.0304\n",
      "[epoch 372]  average training loss: 141.0154\n",
      "[epoch 373]  average training loss: 141.0175\n",
      "[epoch 374]  average training loss: 140.9935\n",
      "[epoch 375]  average training loss: 140.9652\n",
      "[epoch 376]  average training loss: 140.9532\n",
      "[epoch 377]  average training loss: 140.9540\n",
      "[epoch 378]  average training loss: 140.9728\n",
      "[epoch 379]  average training loss: 140.9299\n",
      "[epoch 380]  average training loss: 140.9228\n",
      "[epoch 381]  average training loss: 140.9220\n",
      "[epoch 382]  average training loss: 140.8670\n",
      "[epoch 383]  average training loss: 140.8959\n",
      "[epoch 384]  average training loss: 140.8507\n",
      "[epoch 385]  average training loss: 140.8783\n",
      "[epoch 386]  average training loss: 140.8719\n",
      "[epoch 387]  average training loss: 140.8431\n",
      "[epoch 388]  average training loss: 140.8894\n",
      "[epoch 389]  average training loss: 140.8304\n",
      "[epoch 390]  average training loss: 140.8098\n",
      "[epoch 391]  average training loss: 140.8215\n",
      "[epoch 392]  average training loss: 140.7734\n",
      "[epoch 393]  average training loss: 140.7756\n",
      "[epoch 394]  average training loss: 140.7782\n",
      "[epoch 395]  average training loss: 140.7811\n",
      "[epoch 396]  average training loss: 140.7558\n",
      "[epoch 397]  average training loss: 140.7639\n",
      "[epoch 398]  average training loss: 140.7491\n",
      "[epoch 399]  average training loss: 140.7196\n",
      "[epoch 400]  average training loss: 140.7013\n",
      "[epoch 401]  average training loss: 140.6970\n",
      "[epoch 402]  average training loss: 140.6854\n",
      "[epoch 403]  average training loss: 140.7045\n",
      "[epoch 404]  average training loss: 140.6625\n",
      "[epoch 405]  average training loss: 140.6680\n",
      "[epoch 406]  average training loss: 140.6732\n",
      "[epoch 407]  average training loss: 140.6555\n",
      "[epoch 408]  average training loss: 140.6558\n",
      "[epoch 409]  average training loss: 140.6536\n",
      "[epoch 410]  average training loss: 140.6412\n",
      "[epoch 411]  average training loss: 140.6142\n",
      "[epoch 412]  average training loss: 140.5666\n",
      "[epoch 413]  average training loss: 140.5757\n",
      "[epoch 414]  average training loss: 140.5732\n",
      "[epoch 415]  average training loss: 140.6026\n",
      "[epoch 416]  average training loss: 140.6058\n",
      "[epoch 417]  average training loss: 140.5860\n",
      "[epoch 418]  average training loss: 140.5259\n",
      "[epoch 419]  average training loss: 140.5418\n",
      "[epoch 420]  average training loss: 140.5397\n",
      "[epoch 421]  average training loss: 140.5179\n",
      "[epoch 422]  average training loss: 140.5033\n",
      "[epoch 423]  average training loss: 140.4587\n",
      "[epoch 424]  average training loss: 140.4933\n",
      "[epoch 425]  average training loss: 140.4919\n",
      "[epoch 426]  average training loss: 140.4537\n",
      "[epoch 427]  average training loss: 140.4589\n",
      "[epoch 428]  average training loss: 140.4792\n",
      "[epoch 429]  average training loss: 140.4279\n",
      "[epoch 430]  average training loss: 140.4208\n",
      "[epoch 431]  average training loss: 140.4203\n",
      "[epoch 432]  average training loss: 140.4209\n",
      "[epoch 433]  average training loss: 140.4136\n",
      "[epoch 434]  average training loss: 140.4164\n",
      "[epoch 435]  average training loss: 140.4068\n",
      "[epoch 436]  average training loss: 140.4151\n",
      "[epoch 437]  average training loss: 140.4013\n",
      "[epoch 438]  average training loss: 140.3761\n",
      "[epoch 439]  average training loss: 140.3712\n",
      "[epoch 440]  average training loss: 140.3328\n",
      "[epoch 441]  average training loss: 140.3684\n",
      "[epoch 442]  average training loss: 140.3754\n",
      "[epoch 443]  average training loss: 140.3310\n",
      "[epoch 444]  average training loss: 140.3329\n",
      "[epoch 445]  average training loss: 140.2981\n",
      "[epoch 446]  average training loss: 140.3219\n",
      "[epoch 447]  average training loss: 140.2944\n",
      "[epoch 448]  average training loss: 140.2920\n",
      "[epoch 449]  average training loss: 140.2703\n",
      "[epoch 450]  average training loss: 140.2434\n",
      "[epoch 451]  average training loss: 140.2559\n",
      "[epoch 452]  average training loss: 140.2496\n",
      "[epoch 453]  average training loss: 140.2384\n",
      "[epoch 454]  average training loss: 140.2453\n",
      "[epoch 455]  average training loss: 140.2260\n",
      "[epoch 456]  average training loss: 140.2360\n",
      "[epoch 457]  average training loss: 140.2011\n",
      "[epoch 458]  average training loss: 140.2227\n",
      "[epoch 459]  average training loss: 140.2084\n",
      "[epoch 460]  average training loss: 140.1777\n",
      "[epoch 461]  average training loss: 140.1696\n",
      "[epoch 462]  average training loss: 140.1509\n",
      "[epoch 463]  average training loss: 140.1716\n",
      "[epoch 464]  average training loss: 140.1810\n",
      "[epoch 465]  average training loss: 140.1455\n",
      "[epoch 466]  average training loss: 140.1460\n",
      "[epoch 467]  average training loss: 140.1213\n",
      "[epoch 468]  average training loss: 140.1422\n",
      "[epoch 469]  average training loss: 140.1088\n",
      "[epoch 470]  average training loss: 140.0897\n",
      "[epoch 471]  average training loss: 140.0888\n",
      "[epoch 472]  average training loss: 140.1073\n",
      "[epoch 473]  average training loss: 140.0939\n",
      "[epoch 474]  average training loss: 140.0883\n",
      "[epoch 475]  average training loss: 140.0698\n",
      "[epoch 476]  average training loss: 140.0867\n",
      "[epoch 477]  average training loss: 140.0607\n",
      "[epoch 478]  average training loss: 140.0667\n",
      "[epoch 479]  average training loss: 140.0324\n",
      "[epoch 480]  average training loss: 140.0484\n",
      "[epoch 481]  average training loss: 140.0286\n",
      "[epoch 482]  average training loss: 140.0154\n",
      "[epoch 483]  average training loss: 140.0011\n",
      "[epoch 484]  average training loss: 139.9988\n",
      "[epoch 485]  average training loss: 139.9905\n",
      "[epoch 486]  average training loss: 139.9844\n",
      "[epoch 487]  average training loss: 140.0019\n",
      "[epoch 488]  average training loss: 139.9713\n",
      "[epoch 489]  average training loss: 139.9703\n",
      "[epoch 490]  average training loss: 139.9591\n",
      "[epoch 491]  average training loss: 139.9761\n",
      "[epoch 492]  average training loss: 139.9449\n",
      "[epoch 493]  average training loss: 139.9329\n",
      "[epoch 494]  average training loss: 139.9272\n",
      "[epoch 495]  average training loss: 139.9200\n",
      "[epoch 496]  average training loss: 139.9234\n",
      "[epoch 497]  average training loss: 139.8954\n",
      "[epoch 498]  average training loss: 139.9096\n",
      "[epoch 499]  average training loss: 139.9234\n",
      "[epoch 500]  average training loss: 139.8795\n",
      "[epoch 501]  average training loss: 139.8821\n",
      "[epoch 502]  average training loss: 139.8995\n",
      "[epoch 503]  average training loss: 139.8523\n",
      "[epoch 504]  average training loss: 139.8413\n",
      "[epoch 505]  average training loss: 139.8622\n",
      "[epoch 506]  average training loss: 139.8326\n",
      "[epoch 507]  average training loss: 139.8424\n",
      "[epoch 508]  average training loss: 139.8326\n",
      "[epoch 509]  average training loss: 139.8256\n",
      "[epoch 510]  average training loss: 139.8025\n",
      "[epoch 511]  average training loss: 139.7984\n",
      "[epoch 512]  average training loss: 139.7929\n",
      "[epoch 513]  average training loss: 139.8001\n",
      "[epoch 514]  average training loss: 139.8049\n",
      "[epoch 515]  average training loss: 139.7896\n",
      "[epoch 516]  average training loss: 139.7683\n",
      "[epoch 517]  average training loss: 139.7830\n",
      "[epoch 518]  average training loss: 139.7917\n",
      "[epoch 519]  average training loss: 139.7620\n",
      "[epoch 520]  average training loss: 139.7250\n",
      "[epoch 521]  average training loss: 139.7542\n",
      "[epoch 522]  average training loss: 139.7127\n",
      "[epoch 523]  average training loss: 139.7443\n",
      "[epoch 524]  average training loss: 139.7211\n",
      "[epoch 525]  average training loss: 139.7380\n",
      "[epoch 526]  average training loss: 139.7098\n",
      "[epoch 527]  average training loss: 139.6900\n",
      "[epoch 528]  average training loss: 139.7216\n",
      "[epoch 529]  average training loss: 139.6869\n",
      "[epoch 530]  average training loss: 139.6889\n",
      "[epoch 531]  average training loss: 139.6858\n",
      "[epoch 532]  average training loss: 139.6714\n",
      "[epoch 533]  average training loss: 139.6781\n",
      "[epoch 534]  average training loss: 139.6581\n",
      "[epoch 535]  average training loss: 139.6338\n",
      "[epoch 536]  average training loss: 139.6218\n",
      "[epoch 537]  average training loss: 139.6410\n",
      "[epoch 538]  average training loss: 139.6091\n",
      "[epoch 539]  average training loss: 139.6170\n",
      "[epoch 540]  average training loss: 139.6265\n",
      "[epoch 541]  average training loss: 139.6112\n",
      "[epoch 542]  average training loss: 139.6208\n",
      "[epoch 543]  average training loss: 139.5809\n",
      "[epoch 544]  average training loss: 139.5935\n",
      "[epoch 545]  average training loss: 139.6333\n",
      "[epoch 546]  average training loss: 139.5603\n",
      "[epoch 547]  average training loss: 139.5798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 548]  average training loss: 139.5654\n",
      "[epoch 549]  average training loss: 139.5581\n",
      "[epoch 550]  average training loss: 139.5807\n",
      "[epoch 551]  average training loss: 139.5622\n",
      "[epoch 552]  average training loss: 139.5430\n",
      "[epoch 553]  average training loss: 139.5205\n",
      "[epoch 554]  average training loss: 139.5125\n",
      "[epoch 555]  average training loss: 139.5260\n",
      "[epoch 556]  average training loss: 139.5395\n",
      "[epoch 557]  average training loss: 139.4999\n",
      "[epoch 558]  average training loss: 139.5204\n",
      "[epoch 559]  average training loss: 139.4869\n",
      "[epoch 560]  average training loss: 139.4756\n",
      "[epoch 561]  average training loss: 139.4651\n",
      "[epoch 562]  average training loss: 139.4828\n",
      "[epoch 563]  average training loss: 139.4551\n",
      "[epoch 564]  average training loss: 139.4417\n",
      "[epoch 565]  average training loss: 139.4417\n",
      "[epoch 566]  average training loss: 139.4647\n",
      "[epoch 567]  average training loss: 139.4477\n",
      "[epoch 568]  average training loss: 139.4313\n",
      "[epoch 569]  average training loss: 139.4108\n",
      "[epoch 570]  average training loss: 139.4239\n",
      "[epoch 571]  average training loss: 139.4058\n",
      "[epoch 572]  average training loss: 139.4257\n",
      "[epoch 573]  average training loss: 139.3741\n",
      "[epoch 574]  average training loss: 139.4107\n",
      "[epoch 575]  average training loss: 139.4308\n",
      "[epoch 576]  average training loss: 139.4113\n",
      "[epoch 577]  average training loss: 139.3885\n",
      "[epoch 578]  average training loss: 139.3712\n",
      "[epoch 579]  average training loss: 139.3719\n",
      "[epoch 580]  average training loss: 139.3764\n",
      "[epoch 581]  average training loss: 139.3574\n",
      "[epoch 582]  average training loss: 139.3567\n",
      "[epoch 583]  average training loss: 139.3542\n",
      "[epoch 584]  average training loss: 139.3394\n",
      "[epoch 585]  average training loss: 139.3308\n",
      "[epoch 586]  average training loss: 139.3507\n",
      "[epoch 587]  average training loss: 139.3299\n",
      "[epoch 588]  average training loss: 139.3215\n",
      "[epoch 589]  average training loss: 139.3253\n",
      "[epoch 590]  average training loss: 139.3057\n",
      "[epoch 591]  average training loss: 139.3053\n",
      "[epoch 592]  average training loss: 139.3286\n",
      "[epoch 593]  average training loss: 139.2886\n",
      "[epoch 594]  average training loss: 139.2894\n",
      "[epoch 595]  average training loss: 139.3138\n",
      "[epoch 596]  average training loss: 139.2868\n",
      "[epoch 597]  average training loss: 139.2726\n",
      "[epoch 598]  average training loss: 139.2690\n",
      "[epoch 599]  average training loss: 139.2674\n",
      "[epoch 600]  average training loss: 139.2791\n",
      "[epoch 601]  average training loss: 139.2512\n",
      "[epoch 602]  average training loss: 139.2627\n",
      "[epoch 603]  average training loss: 139.2260\n",
      "[epoch 604]  average training loss: 139.2574\n",
      "[epoch 605]  average training loss: 139.2098\n",
      "[epoch 606]  average training loss: 139.2316\n",
      "[epoch 607]  average training loss: 139.2187\n",
      "[epoch 608]  average training loss: 139.2001\n",
      "[epoch 609]  average training loss: 139.2231\n",
      "[epoch 610]  average training loss: 139.1945\n",
      "[epoch 611]  average training loss: 139.1988\n",
      "[epoch 612]  average training loss: 139.1965\n",
      "[epoch 613]  average training loss: 139.1753\n",
      "[epoch 614]  average training loss: 139.1907\n",
      "[epoch 615]  average training loss: 139.1985\n",
      "[epoch 616]  average training loss: 139.1636\n",
      "[epoch 617]  average training loss: 139.1577\n",
      "[epoch 618]  average training loss: 139.1925\n",
      "[epoch 619]  average training loss: 139.1316\n",
      "[epoch 620]  average training loss: 139.1689\n",
      "[epoch 621]  average training loss: 139.1337\n",
      "[epoch 622]  average training loss: 139.1320\n",
      "[epoch 623]  average training loss: 139.1400\n",
      "[epoch 624]  average training loss: 139.1278\n",
      "[epoch 625]  average training loss: 139.1344\n",
      "[epoch 626]  average training loss: 139.1060\n",
      "[epoch 627]  average training loss: 139.1243\n",
      "[epoch 628]  average training loss: 139.0997\n",
      "[epoch 629]  average training loss: 139.1055\n",
      "[epoch 630]  average training loss: 139.1273\n",
      "[epoch 631]  average training loss: 139.1087\n",
      "[epoch 632]  average training loss: 139.1152\n",
      "[epoch 633]  average training loss: 139.0798\n",
      "[epoch 634]  average training loss: 139.0973\n",
      "[epoch 635]  average training loss: 139.0626\n",
      "[epoch 636]  average training loss: 139.0490\n",
      "[epoch 637]  average training loss: 139.0742\n",
      "[epoch 638]  average training loss: 139.0502\n",
      "[epoch 639]  average training loss: 139.0515\n",
      "[epoch 640]  average training loss: 139.0510\n",
      "[epoch 641]  average training loss: 139.0361\n",
      "[epoch 642]  average training loss: 139.0432\n",
      "[epoch 643]  average training loss: 139.0589\n",
      "[epoch 644]  average training loss: 139.0247\n",
      "[epoch 645]  average training loss: 139.0506\n",
      "[epoch 646]  average training loss: 139.0080\n",
      "[epoch 647]  average training loss: 139.0249\n",
      "[epoch 648]  average training loss: 139.0198\n",
      "[epoch 649]  average training loss: 139.0058\n",
      "[epoch 650]  average training loss: 139.0526\n",
      "[epoch 651]  average training loss: 138.9978\n",
      "[epoch 652]  average training loss: 138.9838\n",
      "[epoch 653]  average training loss: 138.9871\n",
      "[epoch 654]  average training loss: 138.9818\n",
      "[epoch 655]  average training loss: 138.9997\n",
      "[epoch 656]  average training loss: 138.9861\n",
      "[epoch 657]  average training loss: 138.9737\n",
      "[epoch 658]  average training loss: 138.9665\n",
      "[epoch 659]  average training loss: 138.9586\n",
      "[epoch 660]  average training loss: 138.9768\n",
      "[epoch 661]  average training loss: 138.9824\n",
      "[epoch 662]  average training loss: 138.9640\n",
      "[epoch 663]  average training loss: 138.9280\n",
      "[epoch 664]  average training loss: 138.9557\n",
      "[epoch 665]  average training loss: 138.9465\n",
      "[epoch 666]  average training loss: 138.9143\n",
      "[epoch 667]  average training loss: 138.9242\n",
      "[epoch 668]  average training loss: 138.9511\n",
      "[epoch 669]  average training loss: 138.9104\n",
      "[epoch 670]  average training loss: 138.9382\n",
      "[epoch 671]  average training loss: 138.9116\n",
      "[epoch 672]  average training loss: 138.9112\n",
      "[epoch 673]  average training loss: 138.8951\n",
      "[epoch 674]  average training loss: 138.9035\n",
      "[epoch 675]  average training loss: 138.9008\n",
      "[epoch 676]  average training loss: 138.8911\n",
      "[epoch 677]  average training loss: 138.8734\n",
      "[epoch 678]  average training loss: 138.8777\n",
      "[epoch 679]  average training loss: 138.8773\n",
      "[epoch 680]  average training loss: 138.8699\n",
      "[epoch 681]  average training loss: 138.8447\n",
      "[epoch 682]  average training loss: 138.8614\n",
      "[epoch 683]  average training loss: 138.8548\n",
      "[epoch 684]  average training loss: 138.8427\n",
      "[epoch 685]  average training loss: 138.8252\n",
      "[epoch 686]  average training loss: 138.8816\n",
      "[epoch 687]  average training loss: 138.8347\n",
      "[epoch 688]  average training loss: 138.8369\n",
      "[epoch 689]  average training loss: 138.8288\n",
      "[epoch 690]  average training loss: 138.8287\n",
      "[epoch 691]  average training loss: 138.8190\n",
      "[epoch 692]  average training loss: 138.8138\n",
      "[epoch 693]  average training loss: 138.8116\n",
      "[epoch 694]  average training loss: 138.7964\n",
      "[epoch 695]  average training loss: 138.7969\n",
      "[epoch 696]  average training loss: 138.8073\n",
      "[epoch 697]  average training loss: 138.7921\n",
      "[epoch 698]  average training loss: 138.8037\n",
      "[epoch 699]  average training loss: 138.8038\n",
      "[epoch 700]  average training loss: 138.7901\n",
      "[epoch 701]  average training loss: 138.7780\n",
      "[epoch 702]  average training loss: 138.7691\n",
      "[epoch 703]  average training loss: 138.7659\n",
      "[epoch 704]  average training loss: 138.7616\n",
      "[epoch 705]  average training loss: 138.7801\n",
      "[epoch 706]  average training loss: 138.7419\n",
      "[epoch 707]  average training loss: 138.7417\n",
      "[epoch 708]  average training loss: 138.7463\n",
      "[epoch 709]  average training loss: 138.7491\n",
      "[epoch 710]  average training loss: 138.7600\n",
      "[epoch 711]  average training loss: 138.7543\n",
      "[epoch 712]  average training loss: 138.7502\n",
      "[epoch 713]  average training loss: 138.7160\n",
      "[epoch 714]  average training loss: 138.7205\n",
      "[epoch 715]  average training loss: 138.7192\n",
      "[epoch 716]  average training loss: 138.7142\n",
      "[epoch 717]  average training loss: 138.7068\n",
      "[epoch 718]  average training loss: 138.7054\n",
      "[epoch 719]  average training loss: 138.7024\n",
      "[epoch 720]  average training loss: 138.6777\n",
      "[epoch 721]  average training loss: 138.7028\n",
      "[epoch 722]  average training loss: 138.7072\n",
      "[epoch 723]  average training loss: 138.6772\n",
      "[epoch 724]  average training loss: 138.6689\n",
      "[epoch 725]  average training loss: 138.6955\n",
      "[epoch 726]  average training loss: 138.6765\n",
      "[epoch 727]  average training loss: 138.6362\n",
      "[epoch 728]  average training loss: 138.6659\n",
      "[epoch 729]  average training loss: 138.6505\n",
      "[epoch 730]  average training loss: 138.6666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 731]  average training loss: 138.6509\n",
      "[epoch 732]  average training loss: 138.6255\n",
      "[epoch 733]  average training loss: 138.6269\n",
      "[epoch 734]  average training loss: 138.6257\n",
      "[epoch 735]  average training loss: 138.6224\n",
      "[epoch 736]  average training loss: 138.6308\n",
      "[epoch 737]  average training loss: 138.5953\n",
      "[epoch 738]  average training loss: 138.6307\n",
      "[epoch 739]  average training loss: 138.6178\n",
      "[epoch 740]  average training loss: 138.6248\n",
      "[epoch 741]  average training loss: 138.5826\n",
      "[epoch 742]  average training loss: 138.6047\n",
      "[epoch 743]  average training loss: 138.6050\n",
      "[epoch 744]  average training loss: 138.5839\n",
      "[epoch 745]  average training loss: 138.5870\n",
      "[epoch 746]  average training loss: 138.5861\n",
      "[epoch 747]  average training loss: 138.5880\n",
      "[epoch 748]  average training loss: 138.5754\n",
      "[epoch 749]  average training loss: 138.5632\n",
      "[epoch 750]  average training loss: 138.5440\n",
      "[epoch 751]  average training loss: 138.5782\n",
      "[epoch 752]  average training loss: 138.5499\n",
      "[epoch 753]  average training loss: 138.5705\n",
      "[epoch 754]  average training loss: 138.5521\n",
      "[epoch 755]  average training loss: 138.5423\n",
      "[epoch 756]  average training loss: 138.5615\n",
      "[epoch 757]  average training loss: 138.5221\n",
      "[epoch 758]  average training loss: 138.5260\n",
      "[epoch 759]  average training loss: 138.5576\n",
      "[epoch 760]  average training loss: 138.5225\n",
      "[epoch 761]  average training loss: 138.5372\n",
      "[epoch 762]  average training loss: 138.5410\n",
      "[epoch 763]  average training loss: 138.5239\n",
      "[epoch 764]  average training loss: 138.5367\n",
      "[epoch 765]  average training loss: 138.5065\n",
      "[epoch 766]  average training loss: 138.4649\n",
      "[epoch 767]  average training loss: 138.4841\n",
      "[epoch 768]  average training loss: 138.4993\n",
      "[epoch 769]  average training loss: 138.4768\n",
      "[epoch 770]  average training loss: 138.4823\n",
      "[epoch 771]  average training loss: 138.4834\n",
      "[epoch 772]  average training loss: 138.4983\n",
      "[epoch 773]  average training loss: 138.4707\n",
      "[epoch 774]  average training loss: 138.4739\n",
      "[epoch 775]  average training loss: 138.4613\n",
      "[epoch 776]  average training loss: 138.4678\n",
      "[epoch 777]  average training loss: 138.4573\n",
      "[epoch 778]  average training loss: 138.4532\n",
      "[epoch 779]  average training loss: 138.4709\n",
      "[epoch 780]  average training loss: 138.4390\n",
      "[epoch 781]  average training loss: 138.4449\n",
      "[epoch 782]  average training loss: 138.4474\n",
      "[epoch 783]  average training loss: 138.4307\n",
      "[epoch 784]  average training loss: 138.4084\n",
      "[epoch 785]  average training loss: 138.4389\n",
      "[epoch 786]  average training loss: 138.4193\n",
      "[epoch 787]  average training loss: 138.3918\n",
      "[epoch 788]  average training loss: 138.4181\n",
      "[epoch 789]  average training loss: 138.4371\n",
      "[epoch 790]  average training loss: 138.3988\n",
      "[epoch 791]  average training loss: 138.3954\n",
      "[epoch 792]  average training loss: 138.4091\n",
      "[epoch 793]  average training loss: 138.3924\n",
      "[epoch 794]  average training loss: 138.3974\n",
      "[epoch 795]  average training loss: 138.3896\n",
      "[epoch 796]  average training loss: 138.3578\n",
      "[epoch 797]  average training loss: 138.3548\n",
      "[epoch 798]  average training loss: 138.3525\n",
      "[epoch 799]  average training loss: 138.3770\n",
      "[epoch 800]  average training loss: 138.3648\n",
      "[epoch 801]  average training loss: 138.3647\n",
      "[epoch 802]  average training loss: 138.3509\n",
      "[epoch 803]  average training loss: 138.3726\n",
      "[epoch 804]  average training loss: 138.3625\n",
      "[epoch 805]  average training loss: 138.3496\n",
      "[epoch 806]  average training loss: 138.3334\n",
      "[epoch 807]  average training loss: 138.3291\n",
      "[epoch 808]  average training loss: 138.3588\n",
      "[epoch 809]  average training loss: 138.3318\n",
      "[epoch 810]  average training loss: 138.3377\n",
      "[epoch 811]  average training loss: 138.3309\n",
      "[epoch 812]  average training loss: 138.3112\n",
      "[epoch 813]  average training loss: 138.3196\n",
      "[epoch 814]  average training loss: 138.3044\n",
      "[epoch 815]  average training loss: 138.2962\n",
      "[epoch 816]  average training loss: 138.2942\n",
      "[epoch 817]  average training loss: 138.3319\n",
      "[epoch 818]  average training loss: 138.2842\n",
      "[epoch 819]  average training loss: 138.2807\n",
      "[epoch 820]  average training loss: 138.2641\n",
      "[epoch 821]  average training loss: 138.2625\n",
      "[epoch 822]  average training loss: 138.2869\n",
      "[epoch 823]  average training loss: 138.2610\n",
      "[epoch 824]  average training loss: 138.2907\n",
      "[epoch 825]  average training loss: 138.3019\n",
      "[epoch 826]  average training loss: 138.2598\n",
      "[epoch 827]  average training loss: 138.2607\n",
      "[epoch 828]  average training loss: 138.2534\n",
      "[epoch 829]  average training loss: 138.2700\n",
      "[epoch 830]  average training loss: 138.2750\n",
      "[epoch 831]  average training loss: 138.2345\n",
      "[epoch 832]  average training loss: 138.2623\n",
      "[epoch 833]  average training loss: 138.2394\n",
      "[epoch 834]  average training loss: 138.2089\n",
      "[epoch 835]  average training loss: 138.2393\n",
      "[epoch 836]  average training loss: 138.2116\n",
      "[epoch 837]  average training loss: 138.2330\n",
      "[epoch 838]  average training loss: 138.2009\n",
      "[epoch 839]  average training loss: 138.2017\n",
      "[epoch 840]  average training loss: 138.2110\n",
      "[epoch 841]  average training loss: 138.2128\n",
      "[epoch 842]  average training loss: 138.1978\n",
      "[epoch 843]  average training loss: 138.1972\n",
      "[epoch 844]  average training loss: 138.1923\n",
      "[epoch 845]  average training loss: 138.1873\n",
      "[epoch 846]  average training loss: 138.1868\n",
      "[epoch 847]  average training loss: 138.1794\n",
      "[epoch 848]  average training loss: 138.1783\n",
      "[epoch 849]  average training loss: 138.1669\n",
      "[epoch 850]  average training loss: 138.1820\n",
      "[epoch 851]  average training loss: 138.1820\n",
      "[epoch 852]  average training loss: 138.1438\n",
      "[epoch 853]  average training loss: 138.1475\n",
      "[epoch 854]  average training loss: 138.1638\n",
      "[epoch 855]  average training loss: 138.1629\n",
      "[epoch 856]  average training loss: 138.1574\n",
      "[epoch 857]  average training loss: 138.1382\n",
      "[epoch 858]  average training loss: 138.1354\n",
      "[epoch 859]  average training loss: 138.1503\n",
      "[epoch 860]  average training loss: 138.1591\n",
      "[epoch 861]  average training loss: 138.1368\n",
      "[epoch 862]  average training loss: 138.1347\n",
      "[epoch 863]  average training loss: 138.1109\n",
      "[epoch 864]  average training loss: 138.1140\n",
      "[epoch 865]  average training loss: 138.1120\n",
      "[epoch 866]  average training loss: 138.1100\n",
      "[epoch 867]  average training loss: 138.1016\n",
      "[epoch 868]  average training loss: 138.1156\n",
      "[epoch 869]  average training loss: 138.1056\n",
      "[epoch 870]  average training loss: 138.1083\n",
      "[epoch 871]  average training loss: 138.0886\n",
      "[epoch 872]  average training loss: 138.0879\n",
      "[epoch 873]  average training loss: 138.0753\n",
      "[epoch 874]  average training loss: 138.0836\n",
      "[epoch 875]  average training loss: 138.0901\n",
      "[epoch 876]  average training loss: 138.0814\n",
      "[epoch 877]  average training loss: 138.0772\n",
      "[epoch 878]  average training loss: 138.0803\n",
      "[epoch 879]  average training loss: 138.0740\n",
      "[epoch 880]  average training loss: 138.0700\n",
      "[epoch 881]  average training loss: 138.0625\n",
      "[epoch 882]  average training loss: 138.0590\n",
      "[epoch 883]  average training loss: 138.0479\n",
      "[epoch 884]  average training loss: 138.0544\n",
      "[epoch 885]  average training loss: 138.0356\n",
      "[epoch 886]  average training loss: 138.0043\n",
      "[epoch 887]  average training loss: 138.0468\n",
      "[epoch 888]  average training loss: 138.0311\n",
      "[epoch 889]  average training loss: 138.0718\n",
      "[epoch 890]  average training loss: 138.0375\n",
      "[epoch 891]  average training loss: 138.0118\n",
      "[epoch 892]  average training loss: 138.0165\n",
      "[epoch 893]  average training loss: 138.0160\n",
      "[epoch 894]  average training loss: 137.9831\n",
      "[epoch 895]  average training loss: 138.0247\n",
      "[epoch 896]  average training loss: 138.0111\n",
      "[epoch 897]  average training loss: 138.0041\n",
      "[epoch 898]  average training loss: 137.9734\n",
      "[epoch 899]  average training loss: 138.0191\n",
      "[epoch 900]  average training loss: 137.9907\n",
      "[epoch 901]  average training loss: 137.9810\n",
      "[epoch 902]  average training loss: 137.9960\n",
      "[epoch 903]  average training loss: 138.0087\n",
      "[epoch 904]  average training loss: 137.9961\n",
      "[epoch 905]  average training loss: 137.9735\n",
      "[epoch 906]  average training loss: 137.9883\n",
      "[epoch 907]  average training loss: 137.9662\n",
      "[epoch 908]  average training loss: 137.9729\n",
      "[epoch 909]  average training loss: 137.9506\n",
      "[epoch 910]  average training loss: 137.9673\n",
      "[epoch 911]  average training loss: 137.9557\n",
      "[epoch 912]  average training loss: 137.9461\n",
      "[epoch 913]  average training loss: 137.9577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 914]  average training loss: 137.9701\n",
      "[epoch 915]  average training loss: 137.9485\n",
      "[epoch 916]  average training loss: 137.9309\n",
      "[epoch 917]  average training loss: 137.9163\n",
      "[epoch 918]  average training loss: 137.9103\n",
      "[epoch 919]  average training loss: 137.9295\n",
      "[epoch 920]  average training loss: 137.9226\n",
      "[epoch 921]  average training loss: 137.9305\n",
      "[epoch 922]  average training loss: 137.9145\n",
      "[epoch 923]  average training loss: 137.9177\n",
      "[epoch 924]  average training loss: 137.9048\n",
      "[epoch 925]  average training loss: 137.9036\n",
      "[epoch 926]  average training loss: 137.9362\n",
      "[epoch 927]  average training loss: 137.8965\n",
      "[epoch 928]  average training loss: 137.9042\n",
      "[epoch 929]  average training loss: 137.8991\n",
      "[epoch 930]  average training loss: 137.8877\n",
      "[epoch 931]  average training loss: 137.8842\n",
      "[epoch 932]  average training loss: 137.8785\n",
      "[epoch 933]  average training loss: 137.8777\n",
      "[epoch 934]  average training loss: 137.8634\n",
      "[epoch 935]  average training loss: 137.8779\n",
      "[epoch 936]  average training loss: 137.8709\n",
      "[epoch 937]  average training loss: 137.8745\n",
      "[epoch 938]  average training loss: 137.8743\n",
      "[epoch 939]  average training loss: 137.8471\n",
      "[epoch 940]  average training loss: 137.8410\n",
      "[epoch 941]  average training loss: 137.8726\n",
      "[epoch 942]  average training loss: 137.8372\n",
      "[epoch 943]  average training loss: 137.8631\n",
      "[epoch 944]  average training loss: 137.8605\n",
      "[epoch 945]  average training loss: 137.8372\n",
      "[epoch 946]  average training loss: 137.8511\n",
      "[epoch 947]  average training loss: 137.8603\n",
      "[epoch 948]  average training loss: 137.8296\n",
      "[epoch 949]  average training loss: 137.8219\n",
      "[epoch 950]  average training loss: 137.8329\n",
      "[epoch 951]  average training loss: 137.8354\n",
      "[epoch 952]  average training loss: 137.8327\n",
      "[epoch 953]  average training loss: 137.8426\n",
      "[epoch 954]  average training loss: 137.8092\n",
      "[epoch 955]  average training loss: 137.7987\n",
      "[epoch 956]  average training loss: 137.8175\n",
      "[epoch 957]  average training loss: 137.8040\n",
      "[epoch 958]  average training loss: 137.7986\n",
      "[epoch 959]  average training loss: 137.7952\n",
      "[epoch 960]  average training loss: 137.7841\n",
      "[epoch 961]  average training loss: 137.8147\n",
      "[epoch 962]  average training loss: 137.7828\n",
      "[epoch 963]  average training loss: 137.7731\n",
      "[epoch 964]  average training loss: 137.8008\n",
      "[epoch 965]  average training loss: 137.7809\n",
      "[epoch 966]  average training loss: 137.7704\n",
      "[epoch 967]  average training loss: 137.7531\n",
      "[epoch 968]  average training loss: 137.7738\n",
      "[epoch 969]  average training loss: 137.7861\n",
      "[epoch 970]  average training loss: 137.7629\n",
      "[epoch 971]  average training loss: 137.7680\n",
      "[epoch 972]  average training loss: 137.7497\n",
      "[epoch 973]  average training loss: 137.7803\n",
      "[epoch 974]  average training loss: 137.7542\n",
      "[epoch 975]  average training loss: 137.7723\n",
      "[epoch 976]  average training loss: 137.7328\n",
      "[epoch 977]  average training loss: 137.7307\n",
      "[epoch 978]  average training loss: 137.7519\n",
      "[epoch 979]  average training loss: 137.7432\n",
      "[epoch 980]  average training loss: 137.7183\n",
      "[epoch 981]  average training loss: 137.7387\n",
      "[epoch 982]  average training loss: 137.7139\n",
      "[epoch 983]  average training loss: 137.7279\n",
      "[epoch 984]  average training loss: 137.7070\n",
      "[epoch 985]  average training loss: 137.7085\n",
      "[epoch 986]  average training loss: 137.7064\n",
      "[epoch 987]  average training loss: 137.7108\n",
      "[epoch 988]  average training loss: 137.7150\n",
      "[epoch 989]  average training loss: 137.7073\n",
      "[epoch 990]  average training loss: 137.7132\n",
      "[epoch 991]  average training loss: 137.6679\n",
      "[epoch 992]  average training loss: 137.6859\n",
      "[epoch 993]  average training loss: 137.6832\n",
      "[epoch 994]  average training loss: 137.6993\n",
      "[epoch 995]  average training loss: 137.6908\n",
      "[epoch 996]  average training loss: 137.6823\n",
      "[epoch 997]  average training loss: 137.6783\n",
      "[epoch 998]  average training loss: 137.6634\n",
      "[epoch 999]  average training loss: 137.6633\n",
      "[epoch 1000]  average training loss: 137.6648\n",
      "[epoch 1001]  average training loss: 137.6604\n",
      "[epoch 1002]  average training loss: 137.6693\n",
      "[epoch 1003]  average training loss: 137.6577\n",
      "[epoch 1004]  average training loss: 137.6505\n",
      "[epoch 1005]  average training loss: 137.6468\n",
      "[epoch 1006]  average training loss: 137.6420\n",
      "[epoch 1007]  average training loss: 137.6588\n",
      "[epoch 1008]  average training loss: 137.6745\n",
      "[epoch 1009]  average training loss: 137.6583\n",
      "[epoch 1010]  average training loss: 137.6432\n",
      "[epoch 1011]  average training loss: 137.6384\n",
      "[epoch 1012]  average training loss: 137.6181\n",
      "[epoch 1013]  average training loss: 137.6400\n",
      "[epoch 1014]  average training loss: 137.6435\n",
      "[epoch 1015]  average training loss: 137.6364\n",
      "[epoch 1016]  average training loss: 137.6120\n",
      "[epoch 1017]  average training loss: 137.6022\n",
      "[epoch 1018]  average training loss: 137.6263\n",
      "[epoch 1019]  average training loss: 137.6048\n",
      "[epoch 1020]  average training loss: 137.6330\n",
      "[epoch 1021]  average training loss: 137.6112\n",
      "[epoch 1022]  average training loss: 137.5855\n",
      "[epoch 1023]  average training loss: 137.5873\n",
      "[epoch 1024]  average training loss: 137.5915\n",
      "[epoch 1025]  average training loss: 137.5963\n",
      "[epoch 1026]  average training loss: 137.5734\n",
      "[epoch 1027]  average training loss: 137.5906\n",
      "[epoch 1028]  average training loss: 137.5995\n",
      "[epoch 1029]  average training loss: 137.5870\n",
      "[epoch 1030]  average training loss: 137.5597\n",
      "[epoch 1031]  average training loss: 137.5551\n",
      "[epoch 1032]  average training loss: 137.5698\n",
      "[epoch 1033]  average training loss: 137.5777\n",
      "[epoch 1034]  average training loss: 137.5742\n",
      "[epoch 1035]  average training loss: 137.5509\n",
      "[epoch 1036]  average training loss: 137.5545\n",
      "[epoch 1037]  average training loss: 137.5572\n",
      "[epoch 1038]  average training loss: 137.5435\n",
      "[epoch 1039]  average training loss: 137.5370\n",
      "[epoch 1040]  average training loss: 137.5528\n",
      "[epoch 1041]  average training loss: 137.5764\n",
      "[epoch 1042]  average training loss: 137.5593\n",
      "[epoch 1043]  average training loss: 137.5380\n",
      "[epoch 1044]  average training loss: 137.5245\n",
      "[epoch 1045]  average training loss: 137.5441\n",
      "[epoch 1046]  average training loss: 137.5416\n",
      "[epoch 1047]  average training loss: 137.5236\n",
      "[epoch 1048]  average training loss: 137.5376\n",
      "[epoch 1049]  average training loss: 137.5369\n",
      "[epoch 1050]  average training loss: 137.5163\n",
      "[epoch 1051]  average training loss: 137.5179\n",
      "[epoch 1052]  average training loss: 137.5056\n",
      "[epoch 1053]  average training loss: 137.5123\n",
      "[epoch 1054]  average training loss: 137.5018\n",
      "[epoch 1055]  average training loss: 137.5019\n",
      "[epoch 1056]  average training loss: 137.4958\n",
      "[epoch 1057]  average training loss: 137.4945\n",
      "[epoch 1058]  average training loss: 137.4987\n",
      "[epoch 1059]  average training loss: 137.4807\n",
      "[epoch 1060]  average training loss: 137.4924\n",
      "[epoch 1061]  average training loss: 137.5117\n",
      "[epoch 1062]  average training loss: 137.4747\n",
      "[epoch 1063]  average training loss: 137.4876\n",
      "[epoch 1064]  average training loss: 137.4722\n",
      "[epoch 1065]  average training loss: 137.4799\n",
      "[epoch 1066]  average training loss: 137.4615\n",
      "[epoch 1067]  average training loss: 137.4644\n",
      "[epoch 1068]  average training loss: 137.4771\n",
      "[epoch 1069]  average training loss: 137.4754\n",
      "[epoch 1070]  average training loss: 137.4479\n",
      "[epoch 1071]  average training loss: 137.4678\n",
      "[epoch 1072]  average training loss: 137.4840\n",
      "[epoch 1073]  average training loss: 137.4336\n",
      "[epoch 1074]  average training loss: 137.4553\n",
      "[epoch 1075]  average training loss: 137.4449\n",
      "[epoch 1076]  average training loss: 137.4429\n",
      "[epoch 1077]  average training loss: 137.4412\n",
      "[epoch 1078]  average training loss: 137.4289\n",
      "[epoch 1079]  average training loss: 137.4443\n",
      "[epoch 1080]  average training loss: 137.4406\n",
      "[epoch 1081]  average training loss: 137.4399\n",
      "[epoch 1082]  average training loss: 137.4476\n",
      "[epoch 1083]  average training loss: 137.4412\n",
      "[epoch 1084]  average training loss: 137.4272\n",
      "[epoch 1085]  average training loss: 137.4169\n",
      "[epoch 1086]  average training loss: 137.4085\n",
      "[epoch 1087]  average training loss: 137.4462\n",
      "[epoch 1088]  average training loss: 137.4012\n",
      "[epoch 1089]  average training loss: 137.4163\n",
      "[epoch 1090]  average training loss: 137.3997\n",
      "[epoch 1091]  average training loss: 137.4050\n",
      "[epoch 1092]  average training loss: 137.3720\n",
      "[epoch 1093]  average training loss: 137.3896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1094]  average training loss: 137.3943\n",
      "[epoch 1095]  average training loss: 137.4163\n",
      "[epoch 1096]  average training loss: 137.3962\n",
      "[epoch 1097]  average training loss: 137.3785\n",
      "[epoch 1098]  average training loss: 137.3761\n",
      "[epoch 1099]  average training loss: 137.3848\n",
      "[epoch 1100]  average training loss: 137.4000\n",
      "[epoch 1101]  average training loss: 137.3834\n",
      "[epoch 1102]  average training loss: 137.3878\n",
      "[epoch 1103]  average training loss: 137.3802\n",
      "[epoch 1104]  average training loss: 137.3586\n",
      "[epoch 1105]  average training loss: 137.3720\n",
      "[epoch 1106]  average training loss: 137.3557\n",
      "[epoch 1107]  average training loss: 137.3576\n",
      "[epoch 1108]  average training loss: 137.3527\n",
      "[epoch 1109]  average training loss: 137.3520\n",
      "[epoch 1110]  average training loss: 137.3697\n",
      "[epoch 1111]  average training loss: 137.3610\n",
      "[epoch 1112]  average training loss: 137.3594\n",
      "[epoch 1113]  average training loss: 137.3472\n",
      "[epoch 1114]  average training loss: 137.3249\n",
      "[epoch 1115]  average training loss: 137.3408\n",
      "[epoch 1116]  average training loss: 137.3392\n",
      "[epoch 1117]  average training loss: 137.3347\n",
      "[epoch 1118]  average training loss: 137.3467\n",
      "[epoch 1119]  average training loss: 137.3483\n",
      "[epoch 1120]  average training loss: 137.2925\n",
      "[epoch 1121]  average training loss: 137.3172\n",
      "[epoch 1122]  average training loss: 137.2963\n",
      "[epoch 1123]  average training loss: 137.3286\n",
      "[epoch 1124]  average training loss: 137.3353\n",
      "[epoch 1125]  average training loss: 137.3100\n",
      "[epoch 1126]  average training loss: 137.3119\n",
      "[epoch 1127]  average training loss: 137.3092\n",
      "[epoch 1128]  average training loss: 137.3129\n",
      "[epoch 1129]  average training loss: 137.2942\n",
      "[epoch 1130]  average training loss: 137.2820\n",
      "[epoch 1131]  average training loss: 137.3173\n",
      "[epoch 1132]  average training loss: 137.3041\n",
      "[epoch 1133]  average training loss: 137.3089\n",
      "[epoch 1134]  average training loss: 137.2778\n",
      "[epoch 1135]  average training loss: 137.2899\n",
      "[epoch 1136]  average training loss: 137.2907\n",
      "[epoch 1137]  average training loss: 137.2923\n",
      "[epoch 1138]  average training loss: 137.2825\n",
      "[epoch 1139]  average training loss: 137.2879\n",
      "[epoch 1140]  average training loss: 137.2811\n",
      "[epoch 1141]  average training loss: 137.2766\n",
      "[epoch 1142]  average training loss: 137.2664\n",
      "[epoch 1143]  average training loss: 137.2677\n",
      "[epoch 1144]  average training loss: 137.2746\n",
      "[epoch 1145]  average training loss: 137.2455\n",
      "[epoch 1146]  average training loss: 137.2628\n",
      "[epoch 1147]  average training loss: 137.2868\n",
      "[epoch 1148]  average training loss: 137.2505\n",
      "[epoch 1149]  average training loss: 137.2424\n",
      "[epoch 1150]  average training loss: 137.2475\n",
      "[epoch 1151]  average training loss: 137.2308\n",
      "[epoch 1152]  average training loss: 137.2580\n",
      "[epoch 1153]  average training loss: 137.2657\n",
      "[epoch 1154]  average training loss: 137.2431\n",
      "[epoch 1155]  average training loss: 137.2353\n",
      "[epoch 1156]  average training loss: 137.2360\n",
      "[epoch 1157]  average training loss: 137.2171\n",
      "[epoch 1158]  average training loss: 137.2666\n",
      "[epoch 1159]  average training loss: 137.2230\n",
      "[epoch 1160]  average training loss: 137.2364\n",
      "[epoch 1161]  average training loss: 137.2212\n",
      "[epoch 1162]  average training loss: 137.2062\n",
      "[epoch 1163]  average training loss: 137.2233\n",
      "[epoch 1164]  average training loss: 137.2159\n",
      "[epoch 1165]  average training loss: 137.1946\n",
      "[epoch 1166]  average training loss: 137.2364\n",
      "[epoch 1167]  average training loss: 137.2032\n",
      "[epoch 1168]  average training loss: 137.1955\n",
      "[epoch 1169]  average training loss: 137.2244\n",
      "[epoch 1170]  average training loss: 137.2235\n",
      "[epoch 1171]  average training loss: 137.2161\n",
      "[epoch 1172]  average training loss: 137.2178\n",
      "[epoch 1173]  average training loss: 137.2170\n",
      "[epoch 1174]  average training loss: 137.1782\n",
      "[epoch 1175]  average training loss: 137.1742\n",
      "[epoch 1176]  average training loss: 137.2086\n",
      "[epoch 1177]  average training loss: 137.1908\n",
      "[epoch 1178]  average training loss: 137.1873\n",
      "[epoch 1179]  average training loss: 137.1899\n",
      "[epoch 1180]  average training loss: 137.1496\n",
      "[epoch 1181]  average training loss: 137.1943\n",
      "[epoch 1182]  average training loss: 137.1502\n",
      "[epoch 1183]  average training loss: 137.1648\n",
      "[epoch 1184]  average training loss: 137.1704\n",
      "[epoch 1185]  average training loss: 137.1407\n",
      "[epoch 1186]  average training loss: 137.1734\n",
      "[epoch 1187]  average training loss: 137.1468\n",
      "[epoch 1188]  average training loss: 137.1679\n",
      "[epoch 1189]  average training loss: 137.1648\n",
      "[epoch 1190]  average training loss: 137.1696\n",
      "[epoch 1191]  average training loss: 137.1521\n",
      "[epoch 1192]  average training loss: 137.1677\n",
      "[epoch 1193]  average training loss: 137.1413\n",
      "[epoch 1194]  average training loss: 137.1366\n",
      "[epoch 1195]  average training loss: 137.1244\n",
      "[epoch 1196]  average training loss: 137.1604\n",
      "[epoch 1197]  average training loss: 137.1262\n",
      "[epoch 1198]  average training loss: 137.1364\n",
      "[epoch 1199]  average training loss: 137.1392\n",
      "[epoch 1200]  average training loss: 137.1438\n",
      "[epoch 1201]  average training loss: 137.1068\n",
      "[epoch 1202]  average training loss: 137.1224\n",
      "[epoch 1203]  average training loss: 137.1264\n",
      "[epoch 1204]  average training loss: 137.1053\n",
      "[epoch 1205]  average training loss: 137.1235\n",
      "[epoch 1206]  average training loss: 137.1113\n",
      "[epoch 1207]  average training loss: 137.1180\n",
      "[epoch 1208]  average training loss: 137.0996\n",
      "[epoch 1209]  average training loss: 137.1235\n",
      "[epoch 1210]  average training loss: 137.0854\n",
      "[epoch 1211]  average training loss: 137.1065\n",
      "[epoch 1212]  average training loss: 137.0968\n",
      "[epoch 1213]  average training loss: 137.1074\n",
      "[epoch 1214]  average training loss: 137.0941\n",
      "[epoch 1215]  average training loss: 137.1104\n",
      "[epoch 1216]  average training loss: 137.0842\n",
      "[epoch 1217]  average training loss: 137.0759\n",
      "[epoch 1218]  average training loss: 137.1042\n",
      "[epoch 1219]  average training loss: 137.0842\n",
      "[epoch 1220]  average training loss: 137.0575\n",
      "[epoch 1221]  average training loss: 137.0817\n",
      "[epoch 1222]  average training loss: 137.0776\n",
      "[epoch 1223]  average training loss: 137.0610\n",
      "[epoch 1224]  average training loss: 137.0767\n",
      "[epoch 1225]  average training loss: 137.0760\n",
      "[epoch 1226]  average training loss: 137.0854\n",
      "[epoch 1227]  average training loss: 137.0628\n",
      "[epoch 1228]  average training loss: 137.0559\n",
      "[epoch 1229]  average training loss: 137.0607\n",
      "[epoch 1230]  average training loss: 137.0530\n",
      "[epoch 1231]  average training loss: 137.0598\n",
      "[epoch 1232]  average training loss: 137.0575\n",
      "[epoch 1233]  average training loss: 137.0425\n",
      "[epoch 1234]  average training loss: 137.0604\n",
      "[epoch 1235]  average training loss: 137.0371\n",
      "[epoch 1236]  average training loss: 137.0452\n",
      "[epoch 1237]  average training loss: 137.0504\n",
      "[epoch 1238]  average training loss: 137.0570\n",
      "[epoch 1239]  average training loss: 137.0301\n",
      "[epoch 1240]  average training loss: 137.0335\n",
      "[epoch 1241]  average training loss: 137.0195\n",
      "[epoch 1242]  average training loss: 137.0256\n",
      "[epoch 1243]  average training loss: 137.0302\n",
      "[epoch 1244]  average training loss: 137.0238\n",
      "[epoch 1245]  average training loss: 137.0152\n",
      "[epoch 1246]  average training loss: 137.0107\n",
      "[epoch 1247]  average training loss: 137.0120\n",
      "[epoch 1248]  average training loss: 136.9997\n",
      "[epoch 1249]  average training loss: 136.9870\n",
      "[epoch 1250]  average training loss: 137.0247\n",
      "[epoch 1251]  average training loss: 137.0173\n",
      "[epoch 1252]  average training loss: 137.0233\n",
      "[epoch 1253]  average training loss: 136.9972\n",
      "[epoch 1254]  average training loss: 137.0215\n",
      "[epoch 1255]  average training loss: 137.0053\n",
      "[epoch 1256]  average training loss: 136.9923\n",
      "[epoch 1257]  average training loss: 137.0069\n",
      "[epoch 1258]  average training loss: 136.9673\n",
      "[epoch 1259]  average training loss: 137.0046\n",
      "[epoch 1260]  average training loss: 137.0139\n",
      "[epoch 1261]  average training loss: 137.0078\n",
      "[epoch 1262]  average training loss: 136.9710\n",
      "[epoch 1263]  average training loss: 136.9895\n",
      "[epoch 1264]  average training loss: 136.9916\n",
      "[epoch 1265]  average training loss: 136.9746\n",
      "[epoch 1266]  average training loss: 136.9607\n",
      "[epoch 1267]  average training loss: 136.9829\n",
      "[epoch 1268]  average training loss: 136.9805\n",
      "[epoch 1269]  average training loss: 136.9752\n",
      "[epoch 1270]  average training loss: 136.9751\n",
      "[epoch 1271]  average training loss: 136.9609\n",
      "[epoch 1272]  average training loss: 136.9664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1273]  average training loss: 136.9590\n",
      "[epoch 1274]  average training loss: 136.9733\n",
      "[epoch 1275]  average training loss: 136.9544\n",
      "[epoch 1276]  average training loss: 136.9484\n",
      "[epoch 1277]  average training loss: 136.9544\n",
      "[epoch 1278]  average training loss: 136.9281\n",
      "[epoch 1279]  average training loss: 136.9534\n",
      "[epoch 1280]  average training loss: 136.9405\n",
      "[epoch 1281]  average training loss: 136.9513\n",
      "[epoch 1282]  average training loss: 136.9276\n",
      "[epoch 1283]  average training loss: 136.9230\n",
      "[epoch 1284]  average training loss: 136.9376\n",
      "[epoch 1285]  average training loss: 136.9388\n",
      "[epoch 1286]  average training loss: 136.9644\n",
      "[epoch 1287]  average training loss: 136.9275\n",
      "[epoch 1288]  average training loss: 136.9263\n",
      "[epoch 1289]  average training loss: 136.9292\n",
      "[epoch 1290]  average training loss: 136.9248\n",
      "[epoch 1291]  average training loss: 136.9366\n",
      "[epoch 1292]  average training loss: 136.9112\n",
      "[epoch 1293]  average training loss: 136.9196\n",
      "[epoch 1294]  average training loss: 136.9174\n",
      "[epoch 1295]  average training loss: 136.9137\n",
      "[epoch 1296]  average training loss: 136.9185\n",
      "[epoch 1297]  average training loss: 136.9008\n",
      "[epoch 1298]  average training loss: 136.9116\n",
      "[epoch 1299]  average training loss: 136.9286\n",
      "[epoch 1300]  average training loss: 136.8943\n",
      "[epoch 1301]  average training loss: 136.9055\n",
      "[epoch 1302]  average training loss: 136.8959\n",
      "[epoch 1303]  average training loss: 136.9122\n",
      "[epoch 1304]  average training loss: 136.8924\n",
      "[epoch 1305]  average training loss: 136.9110\n",
      "[epoch 1306]  average training loss: 136.8931\n",
      "[epoch 1307]  average training loss: 136.8810\n",
      "[epoch 1308]  average training loss: 136.8745\n",
      "[epoch 1309]  average training loss: 136.8703\n",
      "[epoch 1310]  average training loss: 136.8855\n",
      "[epoch 1311]  average training loss: 136.8773\n",
      "[epoch 1312]  average training loss: 136.8677\n",
      "[epoch 1313]  average training loss: 136.8837\n",
      "[epoch 1314]  average training loss: 136.8940\n",
      "[epoch 1315]  average training loss: 136.8809\n",
      "[epoch 1316]  average training loss: 136.8683\n",
      "[epoch 1317]  average training loss: 136.8609\n",
      "[epoch 1318]  average training loss: 136.8677\n",
      "[epoch 1319]  average training loss: 136.8764\n",
      "[epoch 1320]  average training loss: 136.8709\n",
      "[epoch 1321]  average training loss: 136.8554\n",
      "[epoch 1322]  average training loss: 136.8608\n",
      "[epoch 1323]  average training loss: 136.8270\n",
      "[epoch 1324]  average training loss: 136.8517\n",
      "[epoch 1325]  average training loss: 136.8361\n",
      "[epoch 1326]  average training loss: 136.8631\n",
      "[epoch 1327]  average training loss: 136.8350\n",
      "[epoch 1328]  average training loss: 136.8243\n",
      "[epoch 1329]  average training loss: 136.8666\n",
      "[epoch 1330]  average training loss: 136.8486\n",
      "[epoch 1331]  average training loss: 136.8357\n",
      "[epoch 1332]  average training loss: 136.8377\n",
      "[epoch 1333]  average training loss: 136.8258\n",
      "[epoch 1334]  average training loss: 136.8406\n",
      "[epoch 1335]  average training loss: 136.8357\n",
      "[epoch 1336]  average training loss: 136.8141\n",
      "[epoch 1337]  average training loss: 136.8189\n",
      "[epoch 1338]  average training loss: 136.8175\n",
      "[epoch 1339]  average training loss: 136.8421\n",
      "[epoch 1340]  average training loss: 136.8359\n",
      "[epoch 1341]  average training loss: 136.8371\n",
      "[epoch 1342]  average training loss: 136.8177\n",
      "[epoch 1343]  average training loss: 136.8006\n",
      "[epoch 1344]  average training loss: 136.8164\n",
      "[epoch 1345]  average training loss: 136.7818\n",
      "[epoch 1346]  average training loss: 136.7954\n",
      "[epoch 1347]  average training loss: 136.7911\n",
      "[epoch 1348]  average training loss: 136.8213\n",
      "[epoch 1349]  average training loss: 136.8222\n",
      "[epoch 1350]  average training loss: 136.7896\n",
      "[epoch 1351]  average training loss: 136.8015\n",
      "[epoch 1352]  average training loss: 136.7951\n",
      "[epoch 1353]  average training loss: 136.7962\n",
      "[epoch 1354]  average training loss: 136.7805\n",
      "[epoch 1355]  average training loss: 136.8025\n",
      "[epoch 1356]  average training loss: 136.7928\n",
      "[epoch 1357]  average training loss: 136.8088\n",
      "[epoch 1358]  average training loss: 136.7871\n",
      "[epoch 1359]  average training loss: 136.7577\n",
      "[epoch 1360]  average training loss: 136.7746\n",
      "[epoch 1361]  average training loss: 136.7784\n",
      "[epoch 1362]  average training loss: 136.7642\n",
      "[epoch 1363]  average training loss: 136.7624\n",
      "[epoch 1364]  average training loss: 136.7660\n",
      "[epoch 1365]  average training loss: 136.7693\n",
      "[epoch 1366]  average training loss: 136.7605\n",
      "[epoch 1367]  average training loss: 136.7749\n",
      "[epoch 1368]  average training loss: 136.7691\n",
      "[epoch 1369]  average training loss: 136.7615\n",
      "[epoch 1370]  average training loss: 136.7583\n",
      "[epoch 1371]  average training loss: 136.7632\n",
      "[epoch 1372]  average training loss: 136.7729\n",
      "[epoch 1373]  average training loss: 136.7494\n",
      "[epoch 1374]  average training loss: 136.7730\n",
      "[epoch 1375]  average training loss: 136.7706\n",
      "[epoch 1376]  average training loss: 136.7380\n",
      "[epoch 1377]  average training loss: 136.7346\n",
      "[epoch 1378]  average training loss: 136.7506\n",
      "[epoch 1379]  average training loss: 136.7381\n",
      "[epoch 1380]  average training loss: 136.7262\n",
      "[epoch 1381]  average training loss: 136.7356\n",
      "[epoch 1382]  average training loss: 136.7323\n",
      "[epoch 1383]  average training loss: 136.7436\n",
      "[epoch 1384]  average training loss: 136.7247\n",
      "[epoch 1385]  average training loss: 136.7330\n",
      "[epoch 1386]  average training loss: 136.7177\n",
      "[epoch 1387]  average training loss: 136.7306\n",
      "[epoch 1388]  average training loss: 136.7156\n",
      "[epoch 1389]  average training loss: 136.7090\n",
      "[epoch 1390]  average training loss: 136.7311\n",
      "[epoch 1391]  average training loss: 136.7215\n",
      "[epoch 1392]  average training loss: 136.7347\n",
      "[epoch 1393]  average training loss: 136.7147\n",
      "[epoch 1394]  average training loss: 136.7131\n",
      "[epoch 1395]  average training loss: 136.7104\n",
      "[epoch 1396]  average training loss: 136.6998\n",
      "[epoch 1397]  average training loss: 136.6722\n",
      "[epoch 1398]  average training loss: 136.7051\n",
      "[epoch 1399]  average training loss: 136.7145\n",
      "[epoch 1400]  average training loss: 136.7172\n",
      "[epoch 1401]  average training loss: 136.6814\n",
      "[epoch 1402]  average training loss: 136.7081\n",
      "[epoch 1403]  average training loss: 136.7106\n",
      "[epoch 1404]  average training loss: 136.6801\n",
      "[epoch 1405]  average training loss: 136.6950\n",
      "[epoch 1406]  average training loss: 136.6975\n",
      "[epoch 1407]  average training loss: 136.7051\n",
      "[epoch 1408]  average training loss: 136.6873\n",
      "[epoch 1409]  average training loss: 136.6786\n",
      "[epoch 1410]  average training loss: 136.6709\n",
      "[epoch 1411]  average training loss: 136.6995\n",
      "[epoch 1412]  average training loss: 136.6704\n",
      "[epoch 1413]  average training loss: 136.6642\n",
      "[epoch 1414]  average training loss: 136.6772\n",
      "[epoch 1415]  average training loss: 136.6652\n",
      "[epoch 1416]  average training loss: 136.6686\n",
      "[epoch 1417]  average training loss: 136.6730\n",
      "[epoch 1418]  average training loss: 136.6605\n",
      "[epoch 1419]  average training loss: 136.6647\n",
      "[epoch 1420]  average training loss: 136.6670\n",
      "[epoch 1421]  average training loss: 136.6740\n",
      "[epoch 1422]  average training loss: 136.6402\n",
      "[epoch 1423]  average training loss: 136.6402\n",
      "[epoch 1424]  average training loss: 136.6389\n",
      "[epoch 1425]  average training loss: 136.6451\n",
      "[epoch 1426]  average training loss: 136.6547\n",
      "[epoch 1427]  average training loss: 136.6460\n",
      "[epoch 1428]  average training loss: 136.6364\n",
      "[epoch 1429]  average training loss: 136.6543\n",
      "[epoch 1430]  average training loss: 136.6585\n",
      "[epoch 1431]  average training loss: 136.6539\n",
      "[epoch 1432]  average training loss: 136.6328\n",
      "[epoch 1433]  average training loss: 136.6449\n",
      "[epoch 1434]  average training loss: 136.6365\n",
      "[epoch 1435]  average training loss: 136.6387\n",
      "[epoch 1436]  average training loss: 136.6267\n",
      "[epoch 1437]  average training loss: 136.6194\n",
      "[epoch 1438]  average training loss: 136.6147\n",
      "[epoch 1439]  average training loss: 136.6165\n",
      "[epoch 1440]  average training loss: 136.6181\n",
      "[epoch 1441]  average training loss: 136.6166\n",
      "[epoch 1442]  average training loss: 136.6226\n",
      "[epoch 1443]  average training loss: 136.6255\n",
      "[epoch 1444]  average training loss: 136.6209\n",
      "[epoch 1445]  average training loss: 136.6129\n",
      "[epoch 1446]  average training loss: 136.6270\n",
      "[epoch 1447]  average training loss: 136.6029\n",
      "[epoch 1448]  average training loss: 136.5846\n",
      "[epoch 1449]  average training loss: 136.6083\n",
      "[epoch 1450]  average training loss: 136.5983\n",
      "[epoch 1451]  average training loss: 136.6117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1452]  average training loss: 136.5925\n",
      "[epoch 1453]  average training loss: 136.6067\n",
      "[epoch 1454]  average training loss: 136.5910\n",
      "[epoch 1455]  average training loss: 136.6097\n",
      "[epoch 1456]  average training loss: 136.5999\n",
      "[epoch 1457]  average training loss: 136.5865\n",
      "[epoch 1458]  average training loss: 136.6010\n",
      "[epoch 1459]  average training loss: 136.5846\n",
      "[epoch 1460]  average training loss: 136.5970\n",
      "[epoch 1461]  average training loss: 136.5881\n",
      "[epoch 1462]  average training loss: 136.5941\n",
      "[epoch 1463]  average training loss: 136.5607\n",
      "[epoch 1464]  average training loss: 136.5527\n",
      "[epoch 1465]  average training loss: 136.5639\n",
      "[epoch 1466]  average training loss: 136.5616\n",
      "[epoch 1467]  average training loss: 136.5836\n",
      "[epoch 1468]  average training loss: 136.5526\n",
      "[epoch 1469]  average training loss: 136.5737\n",
      "[epoch 1470]  average training loss: 136.5973\n",
      "[epoch 1471]  average training loss: 136.5760\n",
      "[epoch 1472]  average training loss: 136.5730\n",
      "[epoch 1473]  average training loss: 136.5695\n",
      "[epoch 1474]  average training loss: 136.5484\n",
      "[epoch 1475]  average training loss: 136.5350\n",
      "[epoch 1476]  average training loss: 136.5561\n",
      "[epoch 1477]  average training loss: 136.5371\n",
      "[epoch 1478]  average training loss: 136.5640\n",
      "[epoch 1479]  average training loss: 136.5470\n",
      "[epoch 1480]  average training loss: 136.5281\n",
      "[epoch 1481]  average training loss: 136.5534\n",
      "[epoch 1482]  average training loss: 136.5616\n",
      "[epoch 1483]  average training loss: 136.5493\n",
      "[epoch 1484]  average training loss: 136.5351\n",
      "[epoch 1485]  average training loss: 136.5332\n",
      "[epoch 1486]  average training loss: 136.5362\n",
      "[epoch 1487]  average training loss: 136.5273\n",
      "[epoch 1488]  average training loss: 136.5133\n",
      "[epoch 1489]  average training loss: 136.5396\n",
      "[epoch 1490]  average training loss: 136.5220\n",
      "[epoch 1491]  average training loss: 136.5331\n",
      "[epoch 1492]  average training loss: 136.5102\n",
      "[epoch 1493]  average training loss: 136.5118\n",
      "[epoch 1494]  average training loss: 136.5101\n",
      "[epoch 1495]  average training loss: 136.5358\n",
      "[epoch 1496]  average training loss: 136.5009\n",
      "[epoch 1497]  average training loss: 136.5225\n",
      "[epoch 1498]  average training loss: 136.4977\n",
      "[epoch 1499]  average training loss: 136.5144\n",
      "[epoch 1500]  average training loss: 136.5015\n",
      "[epoch 1501]  average training loss: 136.5421\n",
      "[epoch 1502]  average training loss: 136.5097\n",
      "[epoch 1503]  average training loss: 136.5148\n",
      "[epoch 1504]  average training loss: 136.5148\n",
      "[epoch 1505]  average training loss: 136.5041\n",
      "[epoch 1506]  average training loss: 136.4925\n",
      "[epoch 1507]  average training loss: 136.5113\n",
      "[epoch 1508]  average training loss: 136.4811\n",
      "[epoch 1509]  average training loss: 136.4938\n",
      "[epoch 1510]  average training loss: 136.4898\n",
      "[epoch 1511]  average training loss: 136.4940\n",
      "[epoch 1512]  average training loss: 136.5207\n",
      "[epoch 1513]  average training loss: 136.4748\n",
      "[epoch 1514]  average training loss: 136.4752\n",
      "[epoch 1515]  average training loss: 136.4761\n",
      "[epoch 1516]  average training loss: 136.4786\n",
      "[epoch 1517]  average training loss: 136.4961\n",
      "[epoch 1518]  average training loss: 136.4749\n",
      "[epoch 1519]  average training loss: 136.4928\n",
      "[epoch 1520]  average training loss: 136.4669\n",
      "[epoch 1521]  average training loss: 136.4828\n",
      "[epoch 1522]  average training loss: 136.4625\n",
      "[epoch 1523]  average training loss: 136.4636\n",
      "[epoch 1524]  average training loss: 136.4559\n",
      "[epoch 1525]  average training loss: 136.4600\n",
      "[epoch 1526]  average training loss: 136.4776\n",
      "[epoch 1527]  average training loss: 136.4390\n",
      "[epoch 1528]  average training loss: 136.4722\n",
      "[epoch 1529]  average training loss: 136.4449\n",
      "[epoch 1530]  average training loss: 136.4381\n",
      "[epoch 1531]  average training loss: 136.4550\n",
      "[epoch 1532]  average training loss: 136.4194\n",
      "[epoch 1533]  average training loss: 136.4495\n",
      "[epoch 1534]  average training loss: 136.4561\n",
      "[epoch 1535]  average training loss: 136.4617\n",
      "[epoch 1536]  average training loss: 136.4414\n",
      "[epoch 1537]  average training loss: 136.4390\n",
      "[epoch 1538]  average training loss: 136.4374\n",
      "[epoch 1539]  average training loss: 136.4460\n",
      "[epoch 1540]  average training loss: 136.4328\n",
      "[epoch 1541]  average training loss: 136.4543\n",
      "[epoch 1542]  average training loss: 136.4175\n",
      "[epoch 1543]  average training loss: 136.4042\n",
      "[epoch 1544]  average training loss: 136.4378\n",
      "[epoch 1545]  average training loss: 136.4256\n",
      "[epoch 1546]  average training loss: 136.4335\n",
      "[epoch 1547]  average training loss: 136.4065\n",
      "[epoch 1548]  average training loss: 136.4170\n",
      "[epoch 1549]  average training loss: 136.4165\n",
      "[epoch 1550]  average training loss: 136.4103\n",
      "[epoch 1551]  average training loss: 136.4016\n",
      "[epoch 1552]  average training loss: 136.4283\n",
      "[epoch 1553]  average training loss: 136.4252\n",
      "[epoch 1554]  average training loss: 136.3946\n",
      "[epoch 1555]  average training loss: 136.4041\n",
      "[epoch 1556]  average training loss: 136.4146\n",
      "[epoch 1557]  average training loss: 136.4084\n",
      "[epoch 1558]  average training loss: 136.3928\n",
      "[epoch 1559]  average training loss: 136.3970\n",
      "[epoch 1560]  average training loss: 136.4029\n",
      "[epoch 1561]  average training loss: 136.3939\n",
      "[epoch 1562]  average training loss: 136.3843\n",
      "[epoch 1563]  average training loss: 136.3959\n",
      "[epoch 1564]  average training loss: 136.3733\n",
      "[epoch 1565]  average training loss: 136.3882\n",
      "[epoch 1566]  average training loss: 136.3651\n",
      "[epoch 1567]  average training loss: 136.3810\n",
      "[epoch 1568]  average training loss: 136.3900\n",
      "[epoch 1569]  average training loss: 136.3858\n",
      "[epoch 1570]  average training loss: 136.3837\n",
      "[epoch 1571]  average training loss: 136.3739\n",
      "[epoch 1572]  average training loss: 136.3735\n",
      "[epoch 1573]  average training loss: 136.3685\n",
      "[epoch 1574]  average training loss: 136.3702\n",
      "[epoch 1575]  average training loss: 136.3868\n",
      "[epoch 1576]  average training loss: 136.3913\n",
      "[epoch 1577]  average training loss: 136.3643\n",
      "[epoch 1578]  average training loss: 136.3702\n",
      "[epoch 1579]  average training loss: 136.3627\n",
      "[epoch 1580]  average training loss: 136.3363\n",
      "[epoch 1581]  average training loss: 136.3805\n",
      "[epoch 1582]  average training loss: 136.3575\n",
      "[epoch 1583]  average training loss: 136.3578\n",
      "[epoch 1584]  average training loss: 136.3535\n",
      "[epoch 1585]  average training loss: 136.3674\n",
      "[epoch 1586]  average training loss: 136.3379\n",
      "[epoch 1587]  average training loss: 136.3522\n",
      "[epoch 1588]  average training loss: 136.3567\n",
      "[epoch 1589]  average training loss: 136.3614\n",
      "[epoch 1590]  average training loss: 136.3572\n",
      "[epoch 1591]  average training loss: 136.3424\n",
      "[epoch 1592]  average training loss: 136.3504\n",
      "[epoch 1593]  average training loss: 136.3357\n",
      "[epoch 1594]  average training loss: 136.3550\n",
      "[epoch 1595]  average training loss: 136.3372\n",
      "[epoch 1596]  average training loss: 136.3246\n",
      "[epoch 1597]  average training loss: 136.3266\n",
      "[epoch 1598]  average training loss: 136.3340\n",
      "[epoch 1599]  average training loss: 136.3416\n",
      "[epoch 1600]  average training loss: 136.3180\n",
      "[epoch 1601]  average training loss: 136.3436\n",
      "[epoch 1602]  average training loss: 136.3237\n",
      "[epoch 1603]  average training loss: 136.3271\n",
      "[epoch 1604]  average training loss: 136.3181\n",
      "[epoch 1605]  average training loss: 136.3382\n",
      "[epoch 1606]  average training loss: 136.3161\n",
      "[epoch 1607]  average training loss: 136.3144\n",
      "[epoch 1608]  average training loss: 136.3173\n",
      "[epoch 1609]  average training loss: 136.3101\n",
      "[epoch 1610]  average training loss: 136.3058\n",
      "[epoch 1611]  average training loss: 136.2988\n",
      "[epoch 1612]  average training loss: 136.3176\n",
      "[epoch 1613]  average training loss: 136.3148\n",
      "[epoch 1614]  average training loss: 136.2875\n",
      "[epoch 1615]  average training loss: 136.2994\n",
      "[epoch 1616]  average training loss: 136.2764\n",
      "[epoch 1617]  average training loss: 136.2770\n",
      "[epoch 1618]  average training loss: 136.3228\n",
      "[epoch 1619]  average training loss: 136.2867\n",
      "[epoch 1620]  average training loss: 136.2833\n",
      "[epoch 1621]  average training loss: 136.2817\n",
      "[epoch 1622]  average training loss: 136.3066\n",
      "[epoch 1623]  average training loss: 136.2951\n",
      "[epoch 1624]  average training loss: 136.2841\n",
      "[epoch 1625]  average training loss: 136.2841\n",
      "[epoch 1626]  average training loss: 136.2835\n",
      "[epoch 1627]  average training loss: 136.2917\n",
      "[epoch 1628]  average training loss: 136.2972\n",
      "[epoch 1629]  average training loss: 136.2715\n",
      "[epoch 1630]  average training loss: 136.2896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1631]  average training loss: 136.2822\n",
      "[epoch 1632]  average training loss: 136.2807\n",
      "[epoch 1633]  average training loss: 136.2416\n",
      "[epoch 1634]  average training loss: 136.2657\n",
      "[epoch 1635]  average training loss: 136.2756\n",
      "[epoch 1636]  average training loss: 136.2523\n",
      "[epoch 1637]  average training loss: 136.2516\n",
      "[epoch 1638]  average training loss: 136.2554\n",
      "[epoch 1639]  average training loss: 136.2496\n",
      "[epoch 1640]  average training loss: 136.2615\n",
      "[epoch 1641]  average training loss: 136.2688\n",
      "[epoch 1642]  average training loss: 136.2400\n",
      "[epoch 1643]  average training loss: 136.2822\n",
      "[epoch 1644]  average training loss: 136.2307\n",
      "[epoch 1645]  average training loss: 136.2581\n",
      "[epoch 1646]  average training loss: 136.2236\n",
      "[epoch 1647]  average training loss: 136.2445\n",
      "[epoch 1648]  average training loss: 136.2418\n",
      "[epoch 1649]  average training loss: 136.2473\n",
      "[epoch 1650]  average training loss: 136.2431\n",
      "[epoch 1651]  average training loss: 136.2427\n",
      "[epoch 1652]  average training loss: 136.2421\n",
      "[epoch 1653]  average training loss: 136.2182\n",
      "[epoch 1654]  average training loss: 136.2324\n",
      "[epoch 1655]  average training loss: 136.2182\n",
      "[epoch 1656]  average training loss: 136.2066\n",
      "[epoch 1657]  average training loss: 136.2071\n",
      "[epoch 1658]  average training loss: 136.2122\n",
      "[epoch 1659]  average training loss: 136.2116\n",
      "[epoch 1660]  average training loss: 136.2354\n",
      "[epoch 1661]  average training loss: 136.2014\n",
      "[epoch 1662]  average training loss: 136.2178\n",
      "[epoch 1663]  average training loss: 136.1933\n",
      "[epoch 1664]  average training loss: 136.2069\n",
      "[epoch 1665]  average training loss: 136.2124\n",
      "[epoch 1666]  average training loss: 136.2043\n",
      "[epoch 1667]  average training loss: 136.1897\n",
      "[epoch 1668]  average training loss: 136.1956\n",
      "[epoch 1669]  average training loss: 136.1980\n",
      "[epoch 1670]  average training loss: 136.1890\n",
      "[epoch 1671]  average training loss: 136.2149\n",
      "[epoch 1672]  average training loss: 136.2117\n",
      "[epoch 1673]  average training loss: 136.2152\n",
      "[epoch 1674]  average training loss: 136.1915\n",
      "[epoch 1675]  average training loss: 136.1988\n",
      "[epoch 1676]  average training loss: 136.1936\n",
      "[epoch 1677]  average training loss: 136.1827\n",
      "[epoch 1678]  average training loss: 136.1747\n",
      "[epoch 1679]  average training loss: 136.1663\n",
      "[epoch 1680]  average training loss: 136.1798\n",
      "[epoch 1681]  average training loss: 136.1695\n",
      "[epoch 1682]  average training loss: 136.1798\n",
      "[epoch 1683]  average training loss: 136.1835\n",
      "[epoch 1684]  average training loss: 136.1685\n",
      "[epoch 1685]  average training loss: 136.1726\n",
      "[epoch 1686]  average training loss: 136.1481\n",
      "[epoch 1687]  average training loss: 136.1603\n",
      "[epoch 1688]  average training loss: 136.1812\n",
      "[epoch 1689]  average training loss: 136.1756\n",
      "[epoch 1690]  average training loss: 136.1513\n",
      "[epoch 1691]  average training loss: 136.1707\n",
      "[epoch 1692]  average training loss: 136.1568\n",
      "[epoch 1693]  average training loss: 136.1700\n",
      "[epoch 1694]  average training loss: 136.1386\n",
      "[epoch 1695]  average training loss: 136.1585\n",
      "[epoch 1696]  average training loss: 136.1562\n",
      "[epoch 1697]  average training loss: 136.1495\n",
      "[epoch 1698]  average training loss: 136.1467\n",
      "[epoch 1699]  average training loss: 136.1195\n",
      "[epoch 1700]  average training loss: 136.1302\n",
      "[epoch 1701]  average training loss: 136.1398\n",
      "[epoch 1702]  average training loss: 136.1325\n",
      "[epoch 1703]  average training loss: 136.1512\n",
      "[epoch 1704]  average training loss: 136.1388\n",
      "[epoch 1705]  average training loss: 136.1425\n",
      "[epoch 1706]  average training loss: 136.1323\n",
      "[epoch 1707]  average training loss: 136.0911\n",
      "[epoch 1708]  average training loss: 136.1268\n",
      "[epoch 1709]  average training loss: 136.1376\n",
      "[epoch 1710]  average training loss: 136.1227\n",
      "[epoch 1711]  average training loss: 136.1202\n",
      "[epoch 1712]  average training loss: 136.1311\n",
      "[epoch 1713]  average training loss: 136.1144\n",
      "[epoch 1714]  average training loss: 136.1039\n",
      "[epoch 1715]  average training loss: 136.1199\n",
      "[epoch 1716]  average training loss: 136.1200\n",
      "[epoch 1717]  average training loss: 136.1251\n",
      "[epoch 1718]  average training loss: 136.1109\n",
      "[epoch 1719]  average training loss: 136.1183\n",
      "[epoch 1720]  average training loss: 136.1117\n",
      "[epoch 1721]  average training loss: 136.1127\n",
      "[epoch 1722]  average training loss: 136.1261\n",
      "[epoch 1723]  average training loss: 136.1034\n",
      "[epoch 1724]  average training loss: 136.1068\n",
      "[epoch 1725]  average training loss: 136.0913\n",
      "[epoch 1726]  average training loss: 136.0760\n",
      "[epoch 1727]  average training loss: 136.1154\n",
      "[epoch 1728]  average training loss: 136.1011\n",
      "[epoch 1729]  average training loss: 136.0960\n",
      "[epoch 1730]  average training loss: 136.0905\n",
      "[epoch 1731]  average training loss: 136.1022\n",
      "[epoch 1732]  average training loss: 136.0826\n",
      "[epoch 1733]  average training loss: 136.0750\n",
      "[epoch 1734]  average training loss: 136.0774\n",
      "[epoch 1735]  average training loss: 136.0903\n",
      "[epoch 1736]  average training loss: 136.0776\n",
      "[epoch 1737]  average training loss: 136.0512\n",
      "[epoch 1738]  average training loss: 136.0773\n",
      "[epoch 1739]  average training loss: 136.0939\n",
      "[epoch 1740]  average training loss: 136.0544\n",
      "[epoch 1741]  average training loss: 136.0692\n",
      "[epoch 1742]  average training loss: 136.0613\n",
      "[epoch 1743]  average training loss: 136.0709\n",
      "[epoch 1744]  average training loss: 136.0551\n",
      "[epoch 1745]  average training loss: 136.0682\n",
      "[epoch 1746]  average training loss: 136.0637\n",
      "[epoch 1747]  average training loss: 136.0366\n",
      "[epoch 1748]  average training loss: 136.0500\n",
      "[epoch 1749]  average training loss: 136.0410\n",
      "[epoch 1750]  average training loss: 136.0405\n",
      "[epoch 1751]  average training loss: 136.0437\n",
      "[epoch 1752]  average training loss: 136.0424\n",
      "[epoch 1753]  average training loss: 136.0741\n",
      "[epoch 1754]  average training loss: 136.0340\n",
      "[epoch 1755]  average training loss: 136.0499\n",
      "[epoch 1756]  average training loss: 136.0547\n",
      "[epoch 1757]  average training loss: 136.0301\n",
      "[epoch 1758]  average training loss: 136.0352\n",
      "[epoch 1759]  average training loss: 136.0372\n",
      "[epoch 1760]  average training loss: 136.0502\n",
      "[epoch 1761]  average training loss: 136.0230\n",
      "[epoch 1762]  average training loss: 136.0394\n",
      "[epoch 1763]  average training loss: 136.0339\n",
      "[epoch 1764]  average training loss: 136.0290\n",
      "[epoch 1765]  average training loss: 136.0101\n",
      "[epoch 1766]  average training loss: 136.0171\n",
      "[epoch 1767]  average training loss: 136.0317\n",
      "[epoch 1768]  average training loss: 136.0272\n",
      "[epoch 1769]  average training loss: 136.0035\n",
      "[epoch 1770]  average training loss: 136.0338\n",
      "[epoch 1771]  average training loss: 136.0053\n",
      "[epoch 1772]  average training loss: 136.0143\n",
      "[epoch 1773]  average training loss: 136.0154\n",
      "[epoch 1774]  average training loss: 136.0028\n",
      "[epoch 1775]  average training loss: 135.9894\n",
      "[epoch 1776]  average training loss: 136.0181\n",
      "[epoch 1777]  average training loss: 135.9996\n",
      "[epoch 1778]  average training loss: 135.9893\n",
      "[epoch 1779]  average training loss: 135.9813\n",
      "[epoch 1780]  average training loss: 136.0010\n",
      "[epoch 1781]  average training loss: 135.9929\n",
      "[epoch 1782]  average training loss: 135.9964\n",
      "[epoch 1783]  average training loss: 135.9867\n",
      "[epoch 1784]  average training loss: 136.0125\n",
      "[epoch 1785]  average training loss: 136.0080\n",
      "[epoch 1786]  average training loss: 135.9894\n",
      "[epoch 1787]  average training loss: 135.9810\n",
      "[epoch 1788]  average training loss: 135.9681\n",
      "[epoch 1789]  average training loss: 135.9786\n",
      "[epoch 1790]  average training loss: 135.9978\n",
      "[epoch 1791]  average training loss: 135.9528\n",
      "[epoch 1792]  average training loss: 135.9870\n",
      "[epoch 1793]  average training loss: 135.9751\n",
      "[epoch 1794]  average training loss: 135.9739\n",
      "[epoch 1795]  average training loss: 135.9596\n",
      "[epoch 1796]  average training loss: 135.9835\n",
      "[epoch 1797]  average training loss: 135.9626\n",
      "[epoch 1798]  average training loss: 135.9410\n",
      "[epoch 1799]  average training loss: 135.9763\n",
      "[epoch 1800]  average training loss: 135.9596\n",
      "[epoch 1801]  average training loss: 135.9559\n",
      "[epoch 1802]  average training loss: 135.9625\n",
      "[epoch 1803]  average training loss: 135.9650\n",
      "[epoch 1804]  average training loss: 135.9393\n",
      "[epoch 1805]  average training loss: 135.9482\n",
      "[epoch 1806]  average training loss: 135.9595\n",
      "[epoch 1807]  average training loss: 135.9537\n",
      "[epoch 1808]  average training loss: 135.9632\n",
      "[epoch 1809]  average training loss: 135.9477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1810]  average training loss: 135.9599\n",
      "[epoch 1811]  average training loss: 135.9441\n",
      "[epoch 1812]  average training loss: 135.9311\n",
      "[epoch 1813]  average training loss: 135.9348\n",
      "[epoch 1814]  average training loss: 135.9262\n",
      "[epoch 1815]  average training loss: 135.9295\n",
      "[epoch 1816]  average training loss: 135.9314\n",
      "[epoch 1817]  average training loss: 135.9478\n",
      "[epoch 1818]  average training loss: 135.9345\n",
      "[epoch 1819]  average training loss: 135.9334\n",
      "[epoch 1820]  average training loss: 135.9297\n",
      "[epoch 1821]  average training loss: 135.9156\n",
      "[epoch 1822]  average training loss: 135.9288\n",
      "[epoch 1823]  average training loss: 135.9202\n",
      "[epoch 1824]  average training loss: 135.9090\n",
      "[epoch 1825]  average training loss: 135.9142\n",
      "[epoch 1826]  average training loss: 135.9250\n",
      "[epoch 1827]  average training loss: 135.9069\n",
      "[epoch 1828]  average training loss: 135.9202\n",
      "[epoch 1829]  average training loss: 135.9164\n",
      "[epoch 1830]  average training loss: 135.9094\n",
      "[epoch 1831]  average training loss: 135.9011\n",
      "[epoch 1832]  average training loss: 135.9021\n",
      "[epoch 1833]  average training loss: 135.8884\n",
      "[epoch 1834]  average training loss: 135.9127\n",
      "[epoch 1835]  average training loss: 135.9069\n",
      "[epoch 1836]  average training loss: 135.8957\n",
      "[epoch 1837]  average training loss: 135.8858\n",
      "[epoch 1838]  average training loss: 135.9020\n",
      "[epoch 1839]  average training loss: 135.9079\n",
      "[epoch 1840]  average training loss: 135.8799\n",
      "[epoch 1841]  average training loss: 135.8814\n",
      "[epoch 1842]  average training loss: 135.8884\n",
      "[epoch 1843]  average training loss: 135.8875\n",
      "[epoch 1844]  average training loss: 135.8781\n",
      "[epoch 1845]  average training loss: 135.8991\n",
      "[epoch 1846]  average training loss: 135.8734\n",
      "[epoch 1847]  average training loss: 135.8615\n",
      "[epoch 1848]  average training loss: 135.9001\n",
      "[epoch 1849]  average training loss: 135.8572\n",
      "[epoch 1850]  average training loss: 135.8593\n",
      "[epoch 1851]  average training loss: 135.8636\n",
      "[epoch 1852]  average training loss: 135.8674\n",
      "[epoch 1853]  average training loss: 135.8737\n",
      "[epoch 1854]  average training loss: 135.8617\n",
      "[epoch 1855]  average training loss: 135.8630\n",
      "[epoch 1856]  average training loss: 135.8474\n",
      "[epoch 1857]  average training loss: 135.8551\n",
      "[epoch 1858]  average training loss: 135.8755\n",
      "[epoch 1859]  average training loss: 135.8371\n",
      "[epoch 1860]  average training loss: 135.8745\n",
      "[epoch 1861]  average training loss: 135.8466\n",
      "[epoch 1862]  average training loss: 135.8593\n",
      "[epoch 1863]  average training loss: 135.8484\n",
      "[epoch 1864]  average training loss: 135.8259\n",
      "[epoch 1865]  average training loss: 135.8489\n",
      "[epoch 1866]  average training loss: 135.8690\n",
      "[epoch 1867]  average training loss: 135.8516\n",
      "[epoch 1868]  average training loss: 135.8420\n",
      "[epoch 1869]  average training loss: 135.8124\n",
      "[epoch 1870]  average training loss: 135.8135\n",
      "[epoch 1871]  average training loss: 135.8365\n",
      "[epoch 1872]  average training loss: 135.8284\n",
      "[epoch 1873]  average training loss: 135.8425\n",
      "[epoch 1874]  average training loss: 135.7997\n",
      "[epoch 1875]  average training loss: 135.8164\n",
      "[epoch 1876]  average training loss: 135.8274\n",
      "[epoch 1877]  average training loss: 135.8212\n",
      "[epoch 1878]  average training loss: 135.8267\n",
      "[epoch 1879]  average training loss: 135.8276\n",
      "[epoch 1880]  average training loss: 135.8186\n",
      "[epoch 1881]  average training loss: 135.8026\n",
      "[epoch 1882]  average training loss: 135.8108\n",
      "[epoch 1883]  average training loss: 135.8266\n",
      "[epoch 1884]  average training loss: 135.8173\n",
      "[epoch 1885]  average training loss: 135.7948\n",
      "[epoch 1886]  average training loss: 135.8258\n",
      "[epoch 1887]  average training loss: 135.8066\n",
      "[epoch 1888]  average training loss: 135.8017\n",
      "[epoch 1889]  average training loss: 135.8205\n",
      "[epoch 1890]  average training loss: 135.7898\n",
      "[epoch 1891]  average training loss: 135.8031\n",
      "[epoch 1892]  average training loss: 135.7868\n",
      "[epoch 1893]  average training loss: 135.7625\n",
      "[epoch 1894]  average training loss: 135.7899\n",
      "[epoch 1895]  average training loss: 135.7954\n",
      "[epoch 1896]  average training loss: 135.7884\n",
      "[epoch 1897]  average training loss: 135.7920\n",
      "[epoch 1898]  average training loss: 135.7882\n",
      "[epoch 1899]  average training loss: 135.7896\n",
      "[epoch 1900]  average training loss: 135.7785\n",
      "[epoch 1901]  average training loss: 135.7677\n",
      "[epoch 1902]  average training loss: 135.7726\n",
      "[epoch 1903]  average training loss: 135.7689\n",
      "[epoch 1904]  average training loss: 135.7782\n",
      "[epoch 1905]  average training loss: 135.7560\n",
      "[epoch 1906]  average training loss: 135.7848\n",
      "[epoch 1907]  average training loss: 135.7693\n",
      "[epoch 1908]  average training loss: 135.7580\n",
      "[epoch 1909]  average training loss: 135.7550\n",
      "[epoch 1910]  average training loss: 135.7572\n",
      "[epoch 1911]  average training loss: 135.7773\n",
      "[epoch 1912]  average training loss: 135.7468\n",
      "[epoch 1913]  average training loss: 135.7709\n",
      "[epoch 1914]  average training loss: 135.7442\n",
      "[epoch 1915]  average training loss: 135.7474\n",
      "[epoch 1916]  average training loss: 135.7653\n",
      "[epoch 1917]  average training loss: 135.7485\n",
      "[epoch 1918]  average training loss: 135.7349\n",
      "[epoch 1919]  average training loss: 135.7399\n",
      "[epoch 1920]  average training loss: 135.7472\n",
      "[epoch 1921]  average training loss: 135.7620\n",
      "[epoch 1922]  average training loss: 135.7378\n",
      "[epoch 1923]  average training loss: 135.7454\n",
      "[epoch 1924]  average training loss: 135.7368\n",
      "[epoch 1925]  average training loss: 135.7464\n",
      "[epoch 1926]  average training loss: 135.7330\n",
      "[epoch 1927]  average training loss: 135.6953\n",
      "[epoch 1928]  average training loss: 135.7275\n",
      "[epoch 1929]  average training loss: 135.7306\n",
      "[epoch 1930]  average training loss: 135.7367\n",
      "[epoch 1931]  average training loss: 135.7169\n",
      "[epoch 1932]  average training loss: 135.7390\n",
      "[epoch 1933]  average training loss: 135.7425\n",
      "[epoch 1934]  average training loss: 135.7001\n",
      "[epoch 1935]  average training loss: 135.7212\n",
      "[epoch 1936]  average training loss: 135.7061\n",
      "[epoch 1937]  average training loss: 135.7181\n",
      "[epoch 1938]  average training loss: 135.7182\n",
      "[epoch 1939]  average training loss: 135.7249\n",
      "[epoch 1940]  average training loss: 135.7130\n",
      "[epoch 1941]  average training loss: 135.6857\n",
      "[epoch 1942]  average training loss: 135.6964\n",
      "[epoch 1943]  average training loss: 135.7076\n",
      "[epoch 1944]  average training loss: 135.7145\n",
      "[epoch 1945]  average training loss: 135.7175\n",
      "[epoch 1946]  average training loss: 135.6919\n",
      "[epoch 1947]  average training loss: 135.7197\n",
      "[epoch 1948]  average training loss: 135.7144\n",
      "[epoch 1949]  average training loss: 135.6871\n",
      "[epoch 1950]  average training loss: 135.6963\n",
      "[epoch 1951]  average training loss: 135.7121\n",
      "[epoch 1952]  average training loss: 135.6980\n",
      "[epoch 1953]  average training loss: 135.6782\n",
      "[epoch 1954]  average training loss: 135.6880\n",
      "[epoch 1955]  average training loss: 135.6729\n",
      "[epoch 1956]  average training loss: 135.6997\n",
      "[epoch 1957]  average training loss: 135.7025\n",
      "[epoch 1958]  average training loss: 135.6658\n",
      "[epoch 1959]  average training loss: 135.6901\n",
      "[epoch 1960]  average training loss: 135.6936\n",
      "[epoch 1961]  average training loss: 135.6575\n",
      "[epoch 1962]  average training loss: 135.6816\n",
      "[epoch 1963]  average training loss: 135.6747\n",
      "[epoch 1964]  average training loss: 135.6715\n",
      "[epoch 1965]  average training loss: 135.6818\n",
      "[epoch 1966]  average training loss: 135.6337\n",
      "[epoch 1967]  average training loss: 135.6495\n",
      "[epoch 1968]  average training loss: 135.6805\n",
      "[epoch 1969]  average training loss: 135.6868\n",
      "[epoch 1970]  average training loss: 135.6538\n",
      "[epoch 1971]  average training loss: 135.6580\n",
      "[epoch 1972]  average training loss: 135.6558\n",
      "[epoch 1973]  average training loss: 135.6614\n",
      "[epoch 1974]  average training loss: 135.6444\n",
      "[epoch 1975]  average training loss: 135.6583\n",
      "[epoch 1976]  average training loss: 135.6848\n",
      "[epoch 1977]  average training loss: 135.6407\n",
      "[epoch 1978]  average training loss: 135.6487\n",
      "[epoch 1979]  average training loss: 135.6533\n",
      "[epoch 1980]  average training loss: 135.6378\n",
      "[epoch 1981]  average training loss: 135.6467\n",
      "[epoch 1982]  average training loss: 135.6429\n",
      "[epoch 1983]  average training loss: 135.6269\n",
      "[epoch 1984]  average training loss: 135.6153\n",
      "[epoch 1985]  average training loss: 135.6357\n",
      "[epoch 1986]  average training loss: 135.6506\n",
      "[epoch 1987]  average training loss: 135.6270\n",
      "[epoch 1988]  average training loss: 135.6282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1989]  average training loss: 135.6154\n",
      "[epoch 1990]  average training loss: 135.6142\n",
      "[epoch 1991]  average training loss: 135.6189\n",
      "[epoch 1992]  average training loss: 135.5952\n",
      "[epoch 1993]  average training loss: 135.6216\n",
      "[epoch 1994]  average training loss: 135.6221\n",
      "[epoch 1995]  average training loss: 135.6386\n",
      "[epoch 1996]  average training loss: 135.6170\n",
      "[epoch 1997]  average training loss: 135.6383\n",
      "[epoch 1998]  average training loss: 135.6262\n",
      "[epoch 1999]  average training loss: 135.6188\n",
      "[epoch 2000]  average training loss: 135.6028\n",
      "[epoch 2001]  average training loss: 135.6199\n",
      "[epoch 2002]  average training loss: 135.6184\n",
      "[epoch 2003]  average training loss: 135.6172\n",
      "[epoch 2004]  average training loss: 135.6077\n",
      "[epoch 2005]  average training loss: 135.6072\n",
      "[epoch 2006]  average training loss: 135.6035\n",
      "[epoch 2007]  average training loss: 135.5908\n",
      "[epoch 2008]  average training loss: 135.5833\n",
      "[epoch 2009]  average training loss: 135.5939\n",
      "[epoch 2010]  average training loss: 135.5880\n",
      "[epoch 2011]  average training loss: 135.5909\n",
      "[epoch 2012]  average training loss: 135.5942\n",
      "[epoch 2013]  average training loss: 135.6142\n",
      "[epoch 2014]  average training loss: 135.5902\n",
      "[epoch 2015]  average training loss: 135.5800\n",
      "[epoch 2016]  average training loss: 135.5829\n",
      "[epoch 2017]  average training loss: 135.5733\n",
      "[epoch 2018]  average training loss: 135.5769\n",
      "[epoch 2019]  average training loss: 135.5886\n",
      "[epoch 2020]  average training loss: 135.5573\n",
      "[epoch 2021]  average training loss: 135.5891\n",
      "[epoch 2022]  average training loss: 135.5746\n",
      "[epoch 2023]  average training loss: 135.5759\n",
      "[epoch 2024]  average training loss: 135.5554\n",
      "[epoch 2025]  average training loss: 135.5583\n",
      "[epoch 2026]  average training loss: 135.5611\n",
      "[epoch 2027]  average training loss: 135.5657\n",
      "[epoch 2028]  average training loss: 135.5792\n",
      "[epoch 2029]  average training loss: 135.5723\n",
      "[epoch 2030]  average training loss: 135.5557\n",
      "[epoch 2031]  average training loss: 135.5677\n",
      "[epoch 2032]  average training loss: 135.5717\n",
      "[epoch 2033]  average training loss: 135.5655\n",
      "[epoch 2034]  average training loss: 135.5444\n",
      "[epoch 2035]  average training loss: 135.5557\n",
      "[epoch 2036]  average training loss: 135.5513\n",
      "[epoch 2037]  average training loss: 135.5422\n",
      "[epoch 2038]  average training loss: 135.5415\n",
      "[epoch 2039]  average training loss: 135.5409\n",
      "[epoch 2040]  average training loss: 135.5311\n",
      "[epoch 2041]  average training loss: 135.5450\n",
      "[epoch 2042]  average training loss: 135.5582\n",
      "[epoch 2043]  average training loss: 135.5495\n",
      "[epoch 2044]  average training loss: 135.5289\n",
      "[epoch 2045]  average training loss: 135.5376\n",
      "[epoch 2046]  average training loss: 135.5364\n",
      "[epoch 2047]  average training loss: 135.5237\n",
      "[epoch 2048]  average training loss: 135.5192\n",
      "[epoch 2049]  average training loss: 135.5406\n",
      "[epoch 2050]  average training loss: 135.5213\n",
      "[epoch 2051]  average training loss: 135.5210\n",
      "[epoch 2052]  average training loss: 135.5243\n",
      "[epoch 2053]  average training loss: 135.5483\n",
      "[epoch 2054]  average training loss: 135.5198\n",
      "[epoch 2055]  average training loss: 135.5124\n",
      "[epoch 2056]  average training loss: 135.5208\n",
      "[epoch 2057]  average training loss: 135.5244\n",
      "[epoch 2058]  average training loss: 135.5252\n",
      "[epoch 2059]  average training loss: 135.5230\n",
      "[epoch 2060]  average training loss: 135.5171\n",
      "[epoch 2061]  average training loss: 135.5087\n",
      "[epoch 2062]  average training loss: 135.5080\n",
      "[epoch 2063]  average training loss: 135.5134\n",
      "[epoch 2064]  average training loss: 135.5043\n",
      "[epoch 2065]  average training loss: 135.5052\n",
      "[epoch 2066]  average training loss: 135.4874\n",
      "[epoch 2067]  average training loss: 135.4922\n",
      "[epoch 2068]  average training loss: 135.4961\n",
      "[epoch 2069]  average training loss: 135.5046\n",
      "[epoch 2070]  average training loss: 135.4922\n",
      "[epoch 2071]  average training loss: 135.5250\n",
      "[epoch 2072]  average training loss: 135.5256\n",
      "[epoch 2073]  average training loss: 135.4851\n",
      "[epoch 2074]  average training loss: 135.4751\n",
      "[epoch 2075]  average training loss: 135.4747\n",
      "[epoch 2076]  average training loss: 135.4790\n",
      "[epoch 2077]  average training loss: 135.4711\n",
      "[epoch 2078]  average training loss: 135.4668\n",
      "[epoch 2079]  average training loss: 135.4843\n",
      "[epoch 2080]  average training loss: 135.4643\n",
      "[epoch 2081]  average training loss: 135.4652\n",
      "[epoch 2082]  average training loss: 135.4875\n",
      "[epoch 2083]  average training loss: 135.4835\n",
      "[epoch 2084]  average training loss: 135.4764\n",
      "[epoch 2085]  average training loss: 135.4900\n",
      "[epoch 2086]  average training loss: 135.4799\n",
      "[epoch 2087]  average training loss: 135.4790\n",
      "[epoch 2088]  average training loss: 135.4837\n",
      "[epoch 2089]  average training loss: 135.4678\n",
      "[epoch 2090]  average training loss: 135.4703\n",
      "[epoch 2091]  average training loss: 135.4440\n",
      "[epoch 2092]  average training loss: 135.4566\n",
      "[epoch 2093]  average training loss: 135.4513\n",
      "[epoch 2094]  average training loss: 135.4552\n",
      "[epoch 2095]  average training loss: 135.4568\n",
      "[epoch 2096]  average training loss: 135.4602\n",
      "[epoch 2097]  average training loss: 135.4620\n",
      "[epoch 2098]  average training loss: 135.4518\n",
      "[epoch 2099]  average training loss: 135.4683\n",
      "[epoch 2100]  average training loss: 135.4455\n",
      "[epoch 2101]  average training loss: 135.4574\n",
      "[epoch 2102]  average training loss: 135.4354\n",
      "[epoch 2103]  average training loss: 135.4271\n",
      "[epoch 2104]  average training loss: 135.4317\n",
      "[epoch 2105]  average training loss: 135.4381\n",
      "[epoch 2106]  average training loss: 135.4342\n",
      "[epoch 2107]  average training loss: 135.4576\n",
      "[epoch 2108]  average training loss: 135.4126\n",
      "[epoch 2109]  average training loss: 135.4529\n",
      "[epoch 2110]  average training loss: 135.4347\n",
      "[epoch 2111]  average training loss: 135.4455\n",
      "[epoch 2112]  average training loss: 135.4161\n",
      "[epoch 2113]  average training loss: 135.4441\n",
      "[epoch 2114]  average training loss: 135.4018\n",
      "[epoch 2115]  average training loss: 135.4422\n",
      "[epoch 2116]  average training loss: 135.4309\n",
      "[epoch 2117]  average training loss: 135.4234\n",
      "[epoch 2118]  average training loss: 135.4260\n",
      "[epoch 2119]  average training loss: 135.4123\n",
      "[epoch 2120]  average training loss: 135.4216\n",
      "[epoch 2121]  average training loss: 135.4052\n",
      "[epoch 2122]  average training loss: 135.3984\n",
      "[epoch 2123]  average training loss: 135.4126\n",
      "[epoch 2124]  average training loss: 135.3961\n",
      "[epoch 2125]  average training loss: 135.4180\n",
      "[epoch 2126]  average training loss: 135.3939\n",
      "[epoch 2127]  average training loss: 135.4257\n",
      "[epoch 2128]  average training loss: 135.4077\n",
      "[epoch 2129]  average training loss: 135.4134\n",
      "[epoch 2130]  average training loss: 135.4050\n",
      "[epoch 2131]  average training loss: 135.3969\n",
      "[epoch 2132]  average training loss: 135.3949\n",
      "[epoch 2133]  average training loss: 135.4057\n",
      "[epoch 2134]  average training loss: 135.4193\n",
      "[epoch 2135]  average training loss: 135.3949\n",
      "[epoch 2136]  average training loss: 135.3892\n",
      "[epoch 2137]  average training loss: 135.3863\n",
      "[epoch 2138]  average training loss: 135.3864\n",
      "[epoch 2139]  average training loss: 135.3877\n",
      "[epoch 2140]  average training loss: 135.3971\n",
      "[epoch 2141]  average training loss: 135.3912\n",
      "[epoch 2142]  average training loss: 135.3822\n",
      "[epoch 2143]  average training loss: 135.3790\n",
      "[epoch 2144]  average training loss: 135.3934\n",
      "[epoch 2145]  average training loss: 135.3761\n",
      "[epoch 2146]  average training loss: 135.3768\n",
      "[epoch 2147]  average training loss: 135.3774\n",
      "[epoch 2148]  average training loss: 135.3831\n",
      "[epoch 2149]  average training loss: 135.3718\n",
      "[epoch 2150]  average training loss: 135.3615\n",
      "[epoch 2151]  average training loss: 135.3600\n",
      "[epoch 2152]  average training loss: 135.3705\n",
      "[epoch 2153]  average training loss: 135.3543\n",
      "[epoch 2154]  average training loss: 135.3733\n",
      "[epoch 2155]  average training loss: 135.3683\n",
      "[epoch 2156]  average training loss: 135.3551\n",
      "[epoch 2157]  average training loss: 135.3690\n",
      "[epoch 2158]  average training loss: 135.3658\n",
      "[epoch 2159]  average training loss: 135.3688\n",
      "[epoch 2160]  average training loss: 135.3458\n",
      "[epoch 2161]  average training loss: 135.3468\n",
      "[epoch 2162]  average training loss: 135.3404\n",
      "[epoch 2163]  average training loss: 135.3366\n",
      "[epoch 2164]  average training loss: 135.3277\n",
      "[epoch 2165]  average training loss: 135.3466\n",
      "[epoch 2166]  average training loss: 135.3695\n",
      "[epoch 2167]  average training loss: 135.3391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 2168]  average training loss: 135.3576\n",
      "[epoch 2169]  average training loss: 135.3245\n",
      "[epoch 2170]  average training loss: 135.3560\n",
      "[epoch 2171]  average training loss: 135.3387\n",
      "[epoch 2172]  average training loss: 135.3426\n",
      "[epoch 2173]  average training loss: 135.3259\n",
      "[epoch 2174]  average training loss: 135.3246\n",
      "[epoch 2175]  average training loss: 135.3456\n",
      "[epoch 2176]  average training loss: 135.3105\n",
      "[epoch 2177]  average training loss: 135.3098\n",
      "[epoch 2178]  average training loss: 135.3284\n",
      "[epoch 2179]  average training loss: 135.3367\n",
      "[epoch 2180]  average training loss: 135.3338\n",
      "[epoch 2181]  average training loss: 135.3131\n",
      "[epoch 2182]  average training loss: 135.3217\n",
      "[epoch 2183]  average training loss: 135.3332\n",
      "[epoch 2184]  average training loss: 135.3313\n",
      "[epoch 2185]  average training loss: 135.3218\n",
      "[epoch 2186]  average training loss: 135.3002\n",
      "[epoch 2187]  average training loss: 135.3109\n",
      "[epoch 2188]  average training loss: 135.3089\n",
      "[epoch 2189]  average training loss: 135.3032\n",
      "[epoch 2190]  average training loss: 135.3040\n",
      "[epoch 2191]  average training loss: 135.3020\n",
      "[epoch 2192]  average training loss: 135.3051\n",
      "[epoch 2193]  average training loss: 135.3028\n",
      "[epoch 2194]  average training loss: 135.2852\n",
      "[epoch 2195]  average training loss: 135.3088\n",
      "[epoch 2196]  average training loss: 135.2863\n",
      "[epoch 2197]  average training loss: 135.3077\n",
      "[epoch 2198]  average training loss: 135.3051\n",
      "[epoch 2199]  average training loss: 135.2847\n",
      "[epoch 2200]  average training loss: 135.2966\n",
      "[epoch 2201]  average training loss: 135.2969\n",
      "[epoch 2202]  average training loss: 135.2855\n",
      "[epoch 2203]  average training loss: 135.2731\n",
      "[epoch 2204]  average training loss: 135.2857\n",
      "[epoch 2205]  average training loss: 135.2956\n",
      "[epoch 2206]  average training loss: 135.2772\n",
      "[epoch 2207]  average training loss: 135.3110\n",
      "[epoch 2208]  average training loss: 135.2822\n",
      "[epoch 2209]  average training loss: 135.2634\n",
      "[epoch 2210]  average training loss: 135.2692\n",
      "[epoch 2211]  average training loss: 135.2812\n",
      "[epoch 2212]  average training loss: 135.2584\n",
      "[epoch 2213]  average training loss: 135.2632\n",
      "[epoch 2214]  average training loss: 135.2970\n",
      "[epoch 2215]  average training loss: 135.2714\n",
      "[epoch 2216]  average training loss: 135.2865\n",
      "[epoch 2217]  average training loss: 135.2867\n",
      "[epoch 2218]  average training loss: 135.2498\n",
      "[epoch 2219]  average training loss: 135.2793\n",
      "[epoch 2220]  average training loss: 135.2527\n",
      "[epoch 2221]  average training loss: 135.2778\n",
      "[epoch 2222]  average training loss: 135.2709\n",
      "[epoch 2223]  average training loss: 135.2551\n",
      "[epoch 2224]  average training loss: 135.2738\n",
      "[epoch 2225]  average training loss: 135.2596\n",
      "[epoch 2226]  average training loss: 135.2600\n",
      "[epoch 2227]  average training loss: 135.2535\n",
      "[epoch 2228]  average training loss: 135.2538\n",
      "[epoch 2229]  average training loss: 135.2442\n",
      "[epoch 2230]  average training loss: 135.2540\n",
      "[epoch 2231]  average training loss: 135.2586\n",
      "[epoch 2232]  average training loss: 135.2568\n",
      "[epoch 2233]  average training loss: 135.2419\n",
      "[epoch 2234]  average training loss: 135.2730\n",
      "[epoch 2235]  average training loss: 135.2310\n",
      "[epoch 2236]  average training loss: 135.2453\n",
      "[epoch 2237]  average training loss: 135.2383\n",
      "[epoch 2238]  average training loss: 135.2274\n",
      "[epoch 2239]  average training loss: 135.2367\n",
      "[epoch 2240]  average training loss: 135.2212\n",
      "[epoch 2241]  average training loss: 135.2581\n",
      "[epoch 2242]  average training loss: 135.2328\n",
      "[epoch 2243]  average training loss: 135.2542\n",
      "[epoch 2244]  average training loss: 135.2466\n",
      "[epoch 2245]  average training loss: 135.2163\n",
      "[epoch 2246]  average training loss: 135.2355\n",
      "[epoch 2247]  average training loss: 135.2401\n",
      "[epoch 2248]  average training loss: 135.2232\n",
      "[epoch 2249]  average training loss: 135.2226\n",
      "[epoch 2250]  average training loss: 135.2337\n",
      "[epoch 2251]  average training loss: 135.2170\n",
      "[epoch 2252]  average training loss: 135.2503\n",
      "[epoch 2253]  average training loss: 135.2144\n",
      "[epoch 2254]  average training loss: 135.1946\n",
      "[epoch 2255]  average training loss: 135.2207\n",
      "[epoch 2256]  average training loss: 135.2092\n",
      "[epoch 2257]  average training loss: 135.2093\n",
      "[epoch 2258]  average training loss: 135.2145\n",
      "[epoch 2259]  average training loss: 135.2167\n",
      "[epoch 2260]  average training loss: 135.2233\n",
      "[epoch 2261]  average training loss: 135.2066\n",
      "[epoch 2262]  average training loss: 135.2200\n",
      "[epoch 2263]  average training loss: 135.2021\n",
      "[epoch 2264]  average training loss: 135.1969\n",
      "[epoch 2265]  average training loss: 135.1946\n",
      "[epoch 2266]  average training loss: 135.2010\n",
      "[epoch 2267]  average training loss: 135.2091\n",
      "[epoch 2268]  average training loss: 135.2129\n",
      "[epoch 2269]  average training loss: 135.2061\n",
      "[epoch 2270]  average training loss: 135.1934\n",
      "[epoch 2271]  average training loss: 135.1953\n",
      "[epoch 2272]  average training loss: 135.2063\n",
      "[epoch 2273]  average training loss: 135.1936\n",
      "[epoch 2274]  average training loss: 135.1719\n",
      "[epoch 2275]  average training loss: 135.1957\n",
      "[epoch 2276]  average training loss: 135.1851\n",
      "[epoch 2277]  average training loss: 135.1752\n",
      "[epoch 2278]  average training loss: 135.1758\n",
      "[epoch 2279]  average training loss: 135.1713\n",
      "[epoch 2280]  average training loss: 135.2014\n",
      "[epoch 2281]  average training loss: 135.1883\n",
      "[epoch 2282]  average training loss: 135.1860\n",
      "[epoch 2283]  average training loss: 135.1908\n",
      "[epoch 2284]  average training loss: 135.1637\n",
      "[epoch 2285]  average training loss: 135.1850\n",
      "[epoch 2286]  average training loss: 135.1693\n",
      "[epoch 2287]  average training loss: 135.1628\n",
      "[epoch 2288]  average training loss: 135.1812\n",
      "[epoch 2289]  average training loss: 135.1573\n",
      "[epoch 2290]  average training loss: 135.1522\n",
      "[epoch 2291]  average training loss: 135.1424\n",
      "[epoch 2292]  average training loss: 135.1585\n",
      "[epoch 2293]  average training loss: 135.1504\n",
      "[epoch 2294]  average training loss: 135.1643\n",
      "[epoch 2295]  average training loss: 135.1703\n",
      "[epoch 2296]  average training loss: 135.1570\n",
      "[epoch 2297]  average training loss: 135.1600\n",
      "[epoch 2298]  average training loss: 135.1311\n",
      "[epoch 2299]  average training loss: 135.1418\n",
      "[epoch 2300]  average training loss: 135.1551\n",
      "[epoch 2301]  average training loss: 135.1389\n",
      "[epoch 2302]  average training loss: 135.1573\n",
      "[epoch 2303]  average training loss: 135.1496\n",
      "[epoch 2304]  average training loss: 135.1589\n",
      "[epoch 2305]  average training loss: 135.1229\n",
      "[epoch 2306]  average training loss: 135.1456\n",
      "[epoch 2307]  average training loss: 135.1463\n",
      "[epoch 2308]  average training loss: 135.1362\n",
      "[epoch 2309]  average training loss: 135.1430\n",
      "[epoch 2310]  average training loss: 135.1484\n",
      "[epoch 2311]  average training loss: 135.1476\n",
      "[epoch 2312]  average training loss: 135.1243\n",
      "[epoch 2313]  average training loss: 135.1441\n",
      "[epoch 2314]  average training loss: 135.1458\n",
      "[epoch 2315]  average training loss: 135.1361\n",
      "[epoch 2316]  average training loss: 135.1208\n",
      "[epoch 2317]  average training loss: 135.1362\n",
      "[epoch 2318]  average training loss: 135.1204\n",
      "[epoch 2319]  average training loss: 135.1433\n",
      "[epoch 2320]  average training loss: 135.1156\n",
      "[epoch 2321]  average training loss: 135.1235\n",
      "[epoch 2322]  average training loss: 135.1235\n",
      "[epoch 2323]  average training loss: 135.1029\n",
      "[epoch 2324]  average training loss: 135.1153\n",
      "[epoch 2325]  average training loss: 135.0975\n",
      "[epoch 2326]  average training loss: 135.0940\n",
      "[epoch 2327]  average training loss: 135.1219\n",
      "[epoch 2328]  average training loss: 135.1344\n",
      "[epoch 2329]  average training loss: 135.1135\n",
      "[epoch 2330]  average training loss: 135.0999\n",
      "[epoch 2331]  average training loss: 135.1240\n",
      "[epoch 2332]  average training loss: 135.1059\n",
      "[epoch 2333]  average training loss: 135.0860\n",
      "[epoch 2334]  average training loss: 135.1220\n",
      "[epoch 2335]  average training loss: 135.1224\n",
      "[epoch 2336]  average training loss: 135.0926\n",
      "[epoch 2337]  average training loss: 135.1166\n",
      "[epoch 2338]  average training loss: 135.0955\n",
      "[epoch 2339]  average training loss: 135.0989\n",
      "[epoch 2340]  average training loss: 135.1036\n",
      "[epoch 2341]  average training loss: 135.0870\n",
      "[epoch 2342]  average training loss: 135.1000\n",
      "[epoch 2343]  average training loss: 135.0947\n",
      "[epoch 2344]  average training loss: 135.0862\n",
      "[epoch 2345]  average training loss: 135.0963\n",
      "[epoch 2346]  average training loss: 135.0855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 2347]  average training loss: 135.0748\n",
      "[epoch 2348]  average training loss: 135.0878\n",
      "[epoch 2349]  average training loss: 135.0779\n",
      "[epoch 2350]  average training loss: 135.0852\n",
      "[epoch 2351]  average training loss: 135.0870\n",
      "[epoch 2352]  average training loss: 135.0791\n",
      "[epoch 2353]  average training loss: 135.0858\n",
      "[epoch 2354]  average training loss: 135.0814\n",
      "[epoch 2355]  average training loss: 135.1067\n",
      "[epoch 2356]  average training loss: 135.0744\n",
      "[epoch 2357]  average training loss: 135.0709\n",
      "[epoch 2358]  average training loss: 135.0721\n",
      "[epoch 2359]  average training loss: 135.0868\n",
      "[epoch 2360]  average training loss: 135.0597\n",
      "[epoch 2361]  average training loss: 135.0672\n",
      "[epoch 2362]  average training loss: 135.0679\n",
      "[epoch 2363]  average training loss: 135.0564\n",
      "[epoch 2364]  average training loss: 135.0776\n",
      "[epoch 2365]  average training loss: 135.0678\n",
      "[epoch 2366]  average training loss: 135.0461\n",
      "[epoch 2367]  average training loss: 135.0599\n",
      "[epoch 2368]  average training loss: 135.0458\n",
      "[epoch 2369]  average training loss: 135.0507\n",
      "[epoch 2370]  average training loss: 135.0758\n",
      "[epoch 2371]  average training loss: 135.0778\n",
      "[epoch 2372]  average training loss: 135.0439\n",
      "[epoch 2373]  average training loss: 135.0506\n",
      "[epoch 2374]  average training loss: 135.0705\n",
      "[epoch 2375]  average training loss: 135.0431\n",
      "[epoch 2376]  average training loss: 135.0348\n",
      "[epoch 2377]  average training loss: 135.0490\n",
      "[epoch 2378]  average training loss: 135.0556\n",
      "[epoch 2379]  average training loss: 135.0378\n",
      "[epoch 2380]  average training loss: 135.0181\n",
      "[epoch 2381]  average training loss: 135.0317\n",
      "[epoch 2382]  average training loss: 135.0655\n",
      "[epoch 2383]  average training loss: 135.0346\n",
      "[epoch 2384]  average training loss: 135.0405\n",
      "[epoch 2385]  average training loss: 135.0294\n",
      "[epoch 2386]  average training loss: 135.0318\n",
      "[epoch 2387]  average training loss: 135.0353\n",
      "[epoch 2388]  average training loss: 135.0443\n",
      "[epoch 2389]  average training loss: 135.0304\n",
      "[epoch 2390]  average training loss: 135.0224\n",
      "[epoch 2391]  average training loss: 135.0388\n",
      "[epoch 2392]  average training loss: 135.0575\n",
      "[epoch 2393]  average training loss: 135.0233\n",
      "[epoch 2394]  average training loss: 135.0176\n",
      "[epoch 2395]  average training loss: 135.0087\n",
      "[epoch 2396]  average training loss: 135.0197\n",
      "[epoch 2397]  average training loss: 135.0237\n",
      "[epoch 2398]  average training loss: 135.0329\n",
      "[epoch 2399]  average training loss: 135.0276\n",
      "[epoch 2400]  average training loss: 135.0148\n",
      "[epoch 2401]  average training loss: 135.0080\n",
      "[epoch 2402]  average training loss: 134.9951\n",
      "[epoch 2403]  average training loss: 135.0255\n",
      "[epoch 2404]  average training loss: 135.0041\n",
      "[epoch 2405]  average training loss: 135.0085\n",
      "[epoch 2406]  average training loss: 135.0070\n",
      "[epoch 2407]  average training loss: 135.0216\n",
      "[epoch 2408]  average training loss: 135.0196\n",
      "[epoch 2409]  average training loss: 134.9862\n",
      "[epoch 2410]  average training loss: 135.0090\n",
      "[epoch 2411]  average training loss: 134.9993\n",
      "[epoch 2412]  average training loss: 135.0036\n",
      "[epoch 2413]  average training loss: 134.9910\n",
      "[epoch 2414]  average training loss: 135.0129\n",
      "[epoch 2415]  average training loss: 134.9978\n",
      "[epoch 2416]  average training loss: 134.9876\n",
      "[epoch 2417]  average training loss: 134.9959\n",
      "[epoch 2418]  average training loss: 134.9994\n",
      "[epoch 2419]  average training loss: 134.9884\n",
      "[epoch 2420]  average training loss: 134.9859\n",
      "[epoch 2421]  average training loss: 135.0160\n",
      "[epoch 2422]  average training loss: 134.9990\n",
      "[epoch 2423]  average training loss: 134.9989\n",
      "[epoch 2424]  average training loss: 134.9931\n",
      "[epoch 2425]  average training loss: 134.9869\n",
      "[epoch 2426]  average training loss: 134.9630\n",
      "[epoch 2427]  average training loss: 134.9952\n",
      "[epoch 2428]  average training loss: 134.9649\n",
      "[epoch 2429]  average training loss: 134.9815\n",
      "[epoch 2430]  average training loss: 134.9636\n",
      "[epoch 2431]  average training loss: 134.9681\n",
      "[epoch 2432]  average training loss: 134.9969\n",
      "[epoch 2433]  average training loss: 134.9913\n",
      "[epoch 2434]  average training loss: 134.9601\n",
      "[epoch 2435]  average training loss: 134.9673\n",
      "[epoch 2436]  average training loss: 134.9708\n",
      "[epoch 2437]  average training loss: 134.9843\n",
      "[epoch 2438]  average training loss: 134.9795\n",
      "[epoch 2439]  average training loss: 134.9731\n",
      "[epoch 2440]  average training loss: 134.9766\n",
      "[epoch 2441]  average training loss: 134.9690\n",
      "[epoch 2442]  average training loss: 134.9837\n",
      "[epoch 2443]  average training loss: 134.9887\n",
      "[epoch 2444]  average training loss: 134.9913\n",
      "[epoch 2445]  average training loss: 134.9390\n",
      "[epoch 2446]  average training loss: 134.9649\n",
      "[epoch 2447]  average training loss: 134.9729\n",
      "[epoch 2448]  average training loss: 134.9645\n",
      "[epoch 2449]  average training loss: 134.9342\n",
      "[epoch 2450]  average training loss: 134.9170\n",
      "[epoch 2451]  average training loss: 134.9469\n",
      "[epoch 2452]  average training loss: 134.9609\n",
      "[epoch 2453]  average training loss: 134.9426\n",
      "[epoch 2454]  average training loss: 134.9385\n",
      "[epoch 2455]  average training loss: 134.9553\n",
      "[epoch 2456]  average training loss: 134.9447\n",
      "[epoch 2457]  average training loss: 134.9542\n",
      "[epoch 2458]  average training loss: 134.9478\n",
      "[epoch 2459]  average training loss: 134.9284\n",
      "[epoch 2460]  average training loss: 134.9636\n",
      "[epoch 2461]  average training loss: 134.9351\n",
      "[epoch 2462]  average training loss: 134.9301\n",
      "[epoch 2463]  average training loss: 134.9546\n",
      "[epoch 2464]  average training loss: 134.9382\n",
      "[epoch 2465]  average training loss: 134.9183\n",
      "[epoch 2466]  average training loss: 134.9734\n",
      "[epoch 2467]  average training loss: 134.9479\n",
      "[epoch 2468]  average training loss: 134.9148\n",
      "[epoch 2469]  average training loss: 134.9163\n",
      "[epoch 2470]  average training loss: 134.9128\n",
      "[epoch 2471]  average training loss: 134.9285\n",
      "[epoch 2472]  average training loss: 134.9371\n",
      "[epoch 2473]  average training loss: 134.9130\n",
      "[epoch 2474]  average training loss: 134.9203\n",
      "[epoch 2475]  average training loss: 134.9200\n",
      "[epoch 2476]  average training loss: 134.9224\n",
      "[epoch 2477]  average training loss: 134.9248\n",
      "[epoch 2478]  average training loss: 134.9164\n",
      "[epoch 2479]  average training loss: 134.9085\n",
      "[epoch 2480]  average training loss: 134.9183\n",
      "[epoch 2481]  average training loss: 134.9165\n",
      "[epoch 2482]  average training loss: 134.9144\n",
      "[epoch 2483]  average training loss: 134.8764\n",
      "[epoch 2484]  average training loss: 134.9143\n",
      "[epoch 2485]  average training loss: 134.9069\n",
      "[epoch 2486]  average training loss: 134.9174\n",
      "[epoch 2487]  average training loss: 134.9010\n",
      "[epoch 2488]  average training loss: 134.8953\n",
      "[epoch 2489]  average training loss: 134.8964\n",
      "[epoch 2490]  average training loss: 134.9030\n",
      "[epoch 2491]  average training loss: 134.9124\n",
      "[epoch 2492]  average training loss: 134.9009\n",
      "[epoch 2493]  average training loss: 134.9175\n",
      "[epoch 2494]  average training loss: 134.8858\n",
      "[epoch 2495]  average training loss: 134.8897\n",
      "[epoch 2496]  average training loss: 134.8998\n",
      "[epoch 2497]  average training loss: 134.8955\n",
      "[epoch 2498]  average training loss: 134.9177\n",
      "[epoch 2499]  average training loss: 134.8829\n",
      "[epoch 2500]  average training loss: 134.9203\n",
      "[epoch 2501]  average training loss: 134.8932\n",
      "[epoch 2502]  average training loss: 134.8953\n",
      "[epoch 2503]  average training loss: 134.8670\n",
      "[epoch 2504]  average training loss: 134.8727\n",
      "[epoch 2505]  average training loss: 134.8876\n",
      "[epoch 2506]  average training loss: 134.8871\n",
      "[epoch 2507]  average training loss: 134.8869\n",
      "[epoch 2508]  average training loss: 134.8726\n",
      "[epoch 2509]  average training loss: 134.8666\n",
      "[epoch 2510]  average training loss: 134.8908\n",
      "[epoch 2511]  average training loss: 134.8765\n",
      "[epoch 2512]  average training loss: 134.8836\n",
      "[epoch 2513]  average training loss: 134.8865\n",
      "[epoch 2514]  average training loss: 134.8716\n",
      "[epoch 2515]  average training loss: 134.8881\n",
      "[epoch 2516]  average training loss: 134.8851\n",
      "[epoch 2517]  average training loss: 134.8691\n",
      "[epoch 2518]  average training loss: 134.8498\n",
      "[epoch 2519]  average training loss: 134.8625\n",
      "[epoch 2520]  average training loss: 134.8785\n",
      "[epoch 2521]  average training loss: 134.8615\n",
      "[epoch 2522]  average training loss: 134.8616\n",
      "[epoch 2523]  average training loss: 134.8848\n",
      "[epoch 2524]  average training loss: 134.8845\n",
      "[epoch 2525]  average training loss: 134.8680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 2526]  average training loss: 134.8471\n",
      "[epoch 2527]  average training loss: 134.8451\n",
      "[epoch 2528]  average training loss: 134.8657\n",
      "[epoch 2529]  average training loss: 134.8563\n",
      "[epoch 2530]  average training loss: 134.8682\n",
      "[epoch 2531]  average training loss: 134.8354\n",
      "[epoch 2532]  average training loss: 134.8388\n",
      "[epoch 2533]  average training loss: 134.8579\n",
      "[epoch 2534]  average training loss: 134.8764\n",
      "[epoch 2535]  average training loss: 134.8233\n",
      "[epoch 2536]  average training loss: 134.8316\n",
      "[epoch 2537]  average training loss: 134.8553\n",
      "[epoch 2538]  average training loss: 134.8402\n",
      "[epoch 2539]  average training loss: 134.8298\n",
      "[epoch 2540]  average training loss: 134.8308\n",
      "[epoch 2541]  average training loss: 134.8589\n",
      "[epoch 2542]  average training loss: 134.8319\n",
      "[epoch 2543]  average training loss: 134.8312\n",
      "[epoch 2544]  average training loss: 134.8542\n",
      "[epoch 2545]  average training loss: 134.8283\n",
      "[epoch 2546]  average training loss: 134.8329\n",
      "[epoch 2547]  average training loss: 134.8427\n",
      "[epoch 2548]  average training loss: 134.8412\n",
      "[epoch 2549]  average training loss: 134.8193\n",
      "[epoch 2550]  average training loss: 134.8260\n",
      "[epoch 2551]  average training loss: 134.8192\n",
      "[epoch 2552]  average training loss: 134.8160\n",
      "[epoch 2553]  average training loss: 134.8180\n",
      "[epoch 2554]  average training loss: 134.8368\n",
      "[epoch 2555]  average training loss: 134.8456\n",
      "[epoch 2556]  average training loss: 134.8168\n",
      "[epoch 2557]  average training loss: 134.8287\n",
      "[epoch 2558]  average training loss: 134.8120\n",
      "[epoch 2559]  average training loss: 134.8142\n",
      "[epoch 2560]  average training loss: 134.7957\n",
      "[epoch 2561]  average training loss: 134.8376\n",
      "[epoch 2562]  average training loss: 134.8189\n",
      "[epoch 2563]  average training loss: 134.8167\n",
      "[epoch 2564]  average training loss: 134.8136\n",
      "[epoch 2565]  average training loss: 134.7969\n",
      "[epoch 2566]  average training loss: 134.7758\n",
      "[epoch 2567]  average training loss: 134.8295\n",
      "[epoch 2568]  average training loss: 134.8221\n",
      "[epoch 2569]  average training loss: 134.8120\n",
      "[epoch 2570]  average training loss: 134.8072\n",
      "[epoch 2571]  average training loss: 134.8056\n",
      "[epoch 2572]  average training loss: 134.8024\n",
      "[epoch 2573]  average training loss: 134.7904\n",
      "[epoch 2574]  average training loss: 134.8130\n",
      "[epoch 2575]  average training loss: 134.8015\n",
      "[epoch 2576]  average training loss: 134.7983\n",
      "[epoch 2577]  average training loss: 134.7717\n",
      "[epoch 2578]  average training loss: 134.8104\n",
      "[epoch 2579]  average training loss: 134.7990\n",
      "[epoch 2580]  average training loss: 134.7725\n",
      "[epoch 2581]  average training loss: 134.7925\n",
      "[epoch 2582]  average training loss: 134.8162\n",
      "[epoch 2583]  average training loss: 134.8320\n",
      "[epoch 2584]  average training loss: 134.7716\n",
      "[epoch 2585]  average training loss: 134.7858\n",
      "[epoch 2586]  average training loss: 134.7831\n",
      "[epoch 2587]  average training loss: 134.8046\n",
      "[epoch 2588]  average training loss: 134.7890\n",
      "[epoch 2589]  average training loss: 134.7802\n",
      "[epoch 2590]  average training loss: 134.7763\n",
      "[epoch 2591]  average training loss: 134.7759\n",
      "[epoch 2592]  average training loss: 134.7751\n",
      "[epoch 2593]  average training loss: 134.7646\n",
      "[epoch 2594]  average training loss: 134.7526\n",
      "[epoch 2595]  average training loss: 134.7742\n",
      "[epoch 2596]  average training loss: 134.7640\n",
      "[epoch 2597]  average training loss: 134.7801\n",
      "[epoch 2598]  average training loss: 134.7802\n",
      "[epoch 2599]  average training loss: 134.7674\n",
      "[epoch 2600]  average training loss: 134.7695\n",
      "[epoch 2601]  average training loss: 134.7977\n",
      "[epoch 2602]  average training loss: 134.7789\n",
      "[epoch 2603]  average training loss: 134.7512\n",
      "[epoch 2604]  average training loss: 134.7661\n",
      "[epoch 2605]  average training loss: 134.7410\n",
      "[epoch 2606]  average training loss: 134.7529\n",
      "[epoch 2607]  average training loss: 134.7639\n",
      "[epoch 2608]  average training loss: 134.7467\n",
      "[epoch 2609]  average training loss: 134.7601\n",
      "[epoch 2610]  average training loss: 134.7433\n",
      "[epoch 2611]  average training loss: 134.7369\n",
      "[epoch 2612]  average training loss: 134.7579\n",
      "[epoch 2613]  average training loss: 134.7514\n",
      "[epoch 2614]  average training loss: 134.7624\n",
      "[epoch 2615]  average training loss: 134.7688\n",
      "[epoch 2616]  average training loss: 134.7424\n",
      "[epoch 2617]  average training loss: 134.7346\n",
      "[epoch 2618]  average training loss: 134.7568\n",
      "[epoch 2619]  average training loss: 134.7420\n",
      "[epoch 2620]  average training loss: 134.7469\n",
      "[epoch 2621]  average training loss: 134.7493\n",
      "[epoch 2622]  average training loss: 134.7552\n",
      "[epoch 2623]  average training loss: 134.7404\n",
      "[epoch 2624]  average training loss: 134.7456\n",
      "[epoch 2625]  average training loss: 134.7191\n",
      "[epoch 2626]  average training loss: 134.7473\n",
      "[epoch 2627]  average training loss: 134.7320\n",
      "[epoch 2628]  average training loss: 134.7519\n",
      "[epoch 2629]  average training loss: 134.7434\n",
      "[epoch 2630]  average training loss: 134.7453\n",
      "[epoch 2631]  average training loss: 134.7349\n",
      "[epoch 2632]  average training loss: 134.7240\n",
      "[epoch 2633]  average training loss: 134.7319\n",
      "[epoch 2634]  average training loss: 134.7479\n",
      "[epoch 2635]  average training loss: 134.7175\n",
      "[epoch 2636]  average training loss: 134.7152\n",
      "[epoch 2637]  average training loss: 134.7352\n",
      "[epoch 2638]  average training loss: 134.7329\n",
      "[epoch 2639]  average training loss: 134.7251\n",
      "[epoch 2640]  average training loss: 134.7347\n",
      "[epoch 2641]  average training loss: 134.7152\n",
      "[epoch 2642]  average training loss: 134.7160\n",
      "[epoch 2643]  average training loss: 134.7090\n",
      "[epoch 2644]  average training loss: 134.7239\n",
      "[epoch 2645]  average training loss: 134.7189\n",
      "[epoch 2646]  average training loss: 134.7103\n",
      "[epoch 2647]  average training loss: 134.7138\n",
      "[epoch 2648]  average training loss: 134.7152\n",
      "[epoch 2649]  average training loss: 134.7196\n",
      "[epoch 2650]  average training loss: 134.7085\n",
      "[epoch 2651]  average training loss: 134.7037\n",
      "[epoch 2652]  average training loss: 134.7179\n",
      "[epoch 2653]  average training loss: 134.6982\n",
      "[epoch 2654]  average training loss: 134.7075\n",
      "[epoch 2655]  average training loss: 134.7402\n",
      "[epoch 2656]  average training loss: 134.7233\n",
      "[epoch 2657]  average training loss: 134.7097\n",
      "[epoch 2658]  average training loss: 134.6828\n",
      "[epoch 2659]  average training loss: 134.7179\n",
      "[epoch 2660]  average training loss: 134.7002\n",
      "[epoch 2661]  average training loss: 134.7009\n",
      "[epoch 2662]  average training loss: 134.7069\n",
      "[epoch 2663]  average training loss: 134.6987\n",
      "[epoch 2664]  average training loss: 134.6812\n",
      "[epoch 2665]  average training loss: 134.6867\n",
      "[epoch 2666]  average training loss: 134.6980\n",
      "[epoch 2667]  average training loss: 134.7042\n",
      "[epoch 2668]  average training loss: 134.7070\n",
      "[epoch 2669]  average training loss: 134.6952\n",
      "[epoch 2670]  average training loss: 134.6708\n",
      "[epoch 2671]  average training loss: 134.6803\n",
      "[epoch 2672]  average training loss: 134.6797\n",
      "[epoch 2673]  average training loss: 134.6867\n",
      "[epoch 2674]  average training loss: 134.6782\n",
      "[epoch 2675]  average training loss: 134.6905\n",
      "[epoch 2676]  average training loss: 134.6937\n",
      "[epoch 2677]  average training loss: 134.7007\n",
      "[epoch 2678]  average training loss: 134.6844\n",
      "[epoch 2679]  average training loss: 134.6783\n",
      "[epoch 2680]  average training loss: 134.6794\n",
      "[epoch 2681]  average training loss: 134.6822\n",
      "[epoch 2682]  average training loss: 134.6753\n",
      "[epoch 2683]  average training loss: 134.6660\n",
      "[epoch 2684]  average training loss: 134.6548\n",
      "[epoch 2685]  average training loss: 134.6909\n",
      "[epoch 2686]  average training loss: 134.6653\n",
      "[epoch 2687]  average training loss: 134.6888\n",
      "[epoch 2688]  average training loss: 134.6790\n",
      "[epoch 2689]  average training loss: 134.6707\n",
      "[epoch 2690]  average training loss: 134.6709\n",
      "[epoch 2691]  average training loss: 134.6776\n",
      "[epoch 2692]  average training loss: 134.6633\n",
      "[epoch 2693]  average training loss: 134.6545\n",
      "[epoch 2694]  average training loss: 134.6540\n",
      "[epoch 2695]  average training loss: 134.6604\n",
      "[epoch 2696]  average training loss: 134.6568\n",
      "[epoch 2697]  average training loss: 134.6780\n",
      "[epoch 2698]  average training loss: 134.6621\n",
      "[epoch 2699]  average training loss: 134.6459\n",
      "[epoch 2700]  average training loss: 134.6462\n",
      "[epoch 2701]  average training loss: 134.6383\n",
      "[epoch 2702]  average training loss: 134.6493\n",
      "[epoch 2703]  average training loss: 134.6658\n",
      "[epoch 2704]  average training loss: 134.6634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 2705]  average training loss: 134.6534\n",
      "[epoch 2706]  average training loss: 134.6235\n",
      "[epoch 2707]  average training loss: 134.6413\n",
      "[epoch 2708]  average training loss: 134.6497\n",
      "[epoch 2709]  average training loss: 134.6622\n",
      "[epoch 2710]  average training loss: 134.6346\n",
      "[epoch 2711]  average training loss: 134.6260\n",
      "[epoch 2712]  average training loss: 134.6644\n",
      "[epoch 2713]  average training loss: 134.6362\n",
      "[epoch 2714]  average training loss: 134.6161\n",
      "[epoch 2715]  average training loss: 134.6372\n",
      "[epoch 2716]  average training loss: 134.6512\n",
      "[epoch 2717]  average training loss: 134.6401\n",
      "[epoch 2718]  average training loss: 134.6317\n",
      "[epoch 2719]  average training loss: 134.6173\n",
      "[epoch 2720]  average training loss: 134.6267\n",
      "[epoch 2721]  average training loss: 134.6230\n",
      "[epoch 2722]  average training loss: 134.6264\n",
      "[epoch 2723]  average training loss: 134.6359\n",
      "[epoch 2724]  average training loss: 134.6142\n",
      "[epoch 2725]  average training loss: 134.6197\n",
      "[epoch 2726]  average training loss: 134.6373\n",
      "[epoch 2727]  average training loss: 134.6351\n",
      "[epoch 2728]  average training loss: 134.6393\n",
      "[epoch 2729]  average training loss: 134.6252\n",
      "[epoch 2730]  average training loss: 134.6298\n",
      "[epoch 2731]  average training loss: 134.6395\n",
      "[epoch 2732]  average training loss: 134.6096\n",
      "[epoch 2733]  average training loss: 134.6233\n",
      "[epoch 2734]  average training loss: 134.6391\n",
      "[epoch 2735]  average training loss: 134.6240\n",
      "[epoch 2736]  average training loss: 134.6075\n",
      "[epoch 2737]  average training loss: 134.6118\n",
      "[epoch 2738]  average training loss: 134.6238\n",
      "[epoch 2739]  average training loss: 134.6225\n",
      "[epoch 2740]  average training loss: 134.6048\n",
      "[epoch 2741]  average training loss: 134.6182\n",
      "[epoch 2742]  average training loss: 134.6122\n",
      "[epoch 2743]  average training loss: 134.6095\n",
      "[epoch 2744]  average training loss: 134.6098\n",
      "[epoch 2745]  average training loss: 134.5985\n",
      "[epoch 2746]  average training loss: 134.6001\n",
      "[epoch 2747]  average training loss: 134.6065\n",
      "[epoch 2748]  average training loss: 134.6020\n",
      "[epoch 2749]  average training loss: 134.6181\n",
      "[epoch 2750]  average training loss: 134.5923\n",
      "[epoch 2751]  average training loss: 134.6026\n",
      "[epoch 2752]  average training loss: 134.6024\n",
      "[epoch 2753]  average training loss: 134.5860\n",
      "[epoch 2754]  average training loss: 134.6115\n",
      "[epoch 2755]  average training loss: 134.5864\n",
      "[epoch 2756]  average training loss: 134.5967\n",
      "[epoch 2757]  average training loss: 134.5984\n",
      "[epoch 2758]  average training loss: 134.5872\n",
      "[epoch 2759]  average training loss: 134.5757\n",
      "[epoch 2760]  average training loss: 134.5897\n",
      "[epoch 2761]  average training loss: 134.6065\n",
      "[epoch 2762]  average training loss: 134.5898\n",
      "[epoch 2763]  average training loss: 134.5860\n",
      "[epoch 2764]  average training loss: 134.6041\n",
      "[epoch 2765]  average training loss: 134.5919\n",
      "[epoch 2766]  average training loss: 134.5761\n",
      "[epoch 2767]  average training loss: 134.5842\n",
      "[epoch 2768]  average training loss: 134.5920\n",
      "[epoch 2769]  average training loss: 134.5709\n",
      "[epoch 2770]  average training loss: 134.5682\n",
      "[epoch 2771]  average training loss: 134.5976\n",
      "[epoch 2772]  average training loss: 134.5774\n",
      "[epoch 2773]  average training loss: 134.6017\n",
      "[epoch 2774]  average training loss: 134.5800\n",
      "[epoch 2775]  average training loss: 134.5557\n",
      "[epoch 2776]  average training loss: 134.5855\n",
      "[epoch 2777]  average training loss: 134.5723\n",
      "[epoch 2778]  average training loss: 134.5596\n",
      "[epoch 2779]  average training loss: 134.5603\n",
      "[epoch 2780]  average training loss: 134.5868\n",
      "[epoch 2781]  average training loss: 134.5605\n",
      "[epoch 2782]  average training loss: 134.5854\n",
      "[epoch 2783]  average training loss: 134.5735\n",
      "[epoch 2784]  average training loss: 134.5609\n",
      "[epoch 2785]  average training loss: 134.5637\n",
      "[epoch 2786]  average training loss: 134.5661\n",
      "[epoch 2787]  average training loss: 134.5832\n",
      "[epoch 2788]  average training loss: 134.5366\n",
      "[epoch 2789]  average training loss: 134.5589\n",
      "[epoch 2790]  average training loss: 134.5385\n",
      "[epoch 2791]  average training loss: 134.5675\n",
      "[epoch 2792]  average training loss: 134.5648\n",
      "[epoch 2793]  average training loss: 134.5392\n",
      "[epoch 2794]  average training loss: 134.5749\n",
      "[epoch 2795]  average training loss: 134.5564\n",
      "[epoch 2796]  average training loss: 134.5348\n",
      "[epoch 2797]  average training loss: 134.5497\n",
      "[epoch 2798]  average training loss: 134.5460\n",
      "[epoch 2799]  average training loss: 134.5595\n",
      "[epoch 2800]  average training loss: 134.5349\n",
      "[epoch 2801]  average training loss: 134.5601\n",
      "[epoch 2802]  average training loss: 134.5509\n",
      "[epoch 2803]  average training loss: 134.5649\n",
      "[epoch 2804]  average training loss: 134.5513\n",
      "[epoch 2805]  average training loss: 134.5484\n",
      "[epoch 2806]  average training loss: 134.5165\n",
      "[epoch 2807]  average training loss: 134.5246\n",
      "[epoch 2808]  average training loss: 134.5571\n",
      "[epoch 2809]  average training loss: 134.5376\n",
      "[epoch 2810]  average training loss: 134.5268\n",
      "[epoch 2811]  average training loss: 134.5288\n",
      "[epoch 2812]  average training loss: 134.5449\n",
      "[epoch 2813]  average training loss: 134.5319\n",
      "[epoch 2814]  average training loss: 134.5452\n",
      "[epoch 2815]  average training loss: 134.5325\n",
      "[epoch 2816]  average training loss: 134.5314\n",
      "[epoch 2817]  average training loss: 134.5306\n",
      "[epoch 2818]  average training loss: 134.5529\n",
      "[epoch 2819]  average training loss: 134.5367\n",
      "[epoch 2820]  average training loss: 134.5160\n",
      "[epoch 2821]  average training loss: 134.5356\n",
      "[epoch 2822]  average training loss: 134.5574\n",
      "[epoch 2823]  average training loss: 134.5167\n",
      "[epoch 2824]  average training loss: 134.5126\n",
      "[epoch 2825]  average training loss: 134.5229\n",
      "[epoch 2826]  average training loss: 134.5180\n",
      "[epoch 2827]  average training loss: 134.5147\n",
      "[epoch 2828]  average training loss: 134.5112\n",
      "[epoch 2829]  average training loss: 134.5013\n",
      "[epoch 2830]  average training loss: 134.5201\n",
      "[epoch 2831]  average training loss: 134.5215\n",
      "[epoch 2832]  average training loss: 134.5057\n",
      "[epoch 2833]  average training loss: 134.5069\n",
      "[epoch 2834]  average training loss: 134.5087\n",
      "[epoch 2835]  average training loss: 134.5274\n",
      "[epoch 2836]  average training loss: 134.5180\n",
      "[epoch 2837]  average training loss: 134.5007\n",
      "[epoch 2838]  average training loss: 134.4927\n",
      "[epoch 2839]  average training loss: 134.5079\n",
      "[epoch 2840]  average training loss: 134.5042\n",
      "[epoch 2841]  average training loss: 134.5008\n",
      "[epoch 2842]  average training loss: 134.5177\n",
      "[epoch 2843]  average training loss: 134.5034\n",
      "[epoch 2844]  average training loss: 134.4969\n",
      "[epoch 2845]  average training loss: 134.5001\n",
      "[epoch 2846]  average training loss: 134.4940\n",
      "[epoch 2847]  average training loss: 134.4942\n",
      "[epoch 2848]  average training loss: 134.4850\n",
      "[epoch 2849]  average training loss: 134.5071\n",
      "[epoch 2850]  average training loss: 134.4975\n",
      "[epoch 2851]  average training loss: 134.4939\n",
      "[epoch 2852]  average training loss: 134.5105\n",
      "[epoch 2853]  average training loss: 134.4613\n",
      "[epoch 2854]  average training loss: 134.5015\n",
      "[epoch 2855]  average training loss: 134.4926\n",
      "[epoch 2856]  average training loss: 134.4941\n",
      "[epoch 2857]  average training loss: 134.4926\n",
      "[epoch 2858]  average training loss: 134.4916\n",
      "[epoch 2859]  average training loss: 134.4904\n",
      "[epoch 2860]  average training loss: 134.4839\n",
      "[epoch 2861]  average training loss: 134.4961\n",
      "[epoch 2862]  average training loss: 134.4750\n",
      "[epoch 2863]  average training loss: 134.4787\n",
      "[epoch 2864]  average training loss: 134.4658\n",
      "[epoch 2865]  average training loss: 134.4853\n",
      "[epoch 2866]  average training loss: 134.4799\n",
      "[epoch 2867]  average training loss: 134.4692\n",
      "[epoch 2868]  average training loss: 134.4790\n",
      "[epoch 2869]  average training loss: 134.4734\n",
      "[epoch 2870]  average training loss: 134.4698\n",
      "[epoch 2871]  average training loss: 134.4878\n",
      "[epoch 2872]  average training loss: 134.4712\n",
      "[epoch 2873]  average training loss: 134.4637\n",
      "[epoch 2874]  average training loss: 134.4635\n",
      "[epoch 2875]  average training loss: 134.4648\n",
      "[epoch 2876]  average training loss: 134.4799\n",
      "[epoch 2877]  average training loss: 134.4715\n",
      "[epoch 2878]  average training loss: 134.4737\n",
      "[epoch 2879]  average training loss: 134.4842\n",
      "[epoch 2880]  average training loss: 134.4425\n",
      "[epoch 2881]  average training loss: 134.4547\n",
      "[epoch 2882]  average training loss: 134.4523\n",
      "[epoch 2883]  average training loss: 134.4651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 2884]  average training loss: 134.4454\n",
      "[epoch 2885]  average training loss: 134.4532\n",
      "[epoch 2886]  average training loss: 134.4533\n",
      "[epoch 2887]  average training loss: 134.4490\n",
      "[epoch 2888]  average training loss: 134.4566\n",
      "[epoch 2889]  average training loss: 134.4766\n",
      "[epoch 2890]  average training loss: 134.4710\n",
      "[epoch 2891]  average training loss: 134.4696\n",
      "[epoch 2892]  average training loss: 134.4434\n",
      "[epoch 2893]  average training loss: 134.4239\n",
      "[epoch 2894]  average training loss: 134.4325\n",
      "[epoch 2895]  average training loss: 134.4419\n",
      "[epoch 2896]  average training loss: 134.4553\n",
      "[epoch 2897]  average training loss: 134.4523\n",
      "[epoch 2898]  average training loss: 134.4708\n",
      "[epoch 2899]  average training loss: 134.4338\n",
      "[epoch 2900]  average training loss: 134.4541\n",
      "[epoch 2901]  average training loss: 134.4428\n",
      "[epoch 2902]  average training loss: 134.4437\n",
      "[epoch 2903]  average training loss: 134.4438\n",
      "[epoch 2904]  average training loss: 134.4385\n",
      "[epoch 2905]  average training loss: 134.4453\n",
      "[epoch 2906]  average training loss: 134.4502\n",
      "[epoch 2907]  average training loss: 134.4253\n",
      "[epoch 2908]  average training loss: 134.4472\n",
      "[epoch 2909]  average training loss: 134.4398\n",
      "[epoch 2910]  average training loss: 134.4023\n",
      "[epoch 2911]  average training loss: 134.4142\n",
      "[epoch 2912]  average training loss: 134.4238\n",
      "[epoch 2913]  average training loss: 134.4474\n",
      "[epoch 2914]  average training loss: 134.4313\n",
      "[epoch 2915]  average training loss: 134.4507\n",
      "[epoch 2916]  average training loss: 134.4276\n",
      "[epoch 2917]  average training loss: 134.4330\n",
      "[epoch 2918]  average training loss: 134.4109\n",
      "[epoch 2919]  average training loss: 134.4404\n",
      "[epoch 2920]  average training loss: 134.4215\n",
      "[epoch 2921]  average training loss: 134.4398\n",
      "[epoch 2922]  average training loss: 134.4192\n",
      "[epoch 2923]  average training loss: 134.4286\n",
      "[epoch 2924]  average training loss: 134.4049\n",
      "[epoch 2925]  average training loss: 134.4189\n",
      "[epoch 2926]  average training loss: 134.4216\n",
      "[epoch 2927]  average training loss: 134.4100\n",
      "[epoch 2928]  average training loss: 134.4224\n",
      "[epoch 2929]  average training loss: 134.4228\n",
      "[epoch 2930]  average training loss: 134.4212\n",
      "[epoch 2931]  average training loss: 134.4206\n",
      "[epoch 2932]  average training loss: 134.4075\n",
      "[epoch 2933]  average training loss: 134.4099\n",
      "[epoch 2934]  average training loss: 134.4132\n",
      "[epoch 2935]  average training loss: 134.3989\n",
      "[epoch 2936]  average training loss: 134.3947\n",
      "[epoch 2937]  average training loss: 134.4147\n",
      "[epoch 2938]  average training loss: 134.4082\n",
      "[epoch 2939]  average training loss: 134.3912\n",
      "[epoch 2940]  average training loss: 134.4002\n",
      "[epoch 2941]  average training loss: 134.3976\n",
      "[epoch 2942]  average training loss: 134.3975\n",
      "[epoch 2943]  average training loss: 134.4056\n",
      "[epoch 2944]  average training loss: 134.3943\n",
      "[epoch 2945]  average training loss: 134.3958\n",
      "[epoch 2946]  average training loss: 134.3882\n",
      "[epoch 2947]  average training loss: 134.4067\n",
      "[epoch 2948]  average training loss: 134.3853\n",
      "[epoch 2949]  average training loss: 134.3938\n",
      "[epoch 2950]  average training loss: 134.3931\n",
      "[epoch 2951]  average training loss: 134.3863\n",
      "[epoch 2952]  average training loss: 134.3934\n",
      "[epoch 2953]  average training loss: 134.3765\n",
      "[epoch 2954]  average training loss: 134.3759\n",
      "[epoch 2955]  average training loss: 134.4104\n",
      "[epoch 2956]  average training loss: 134.3823\n",
      "[epoch 2957]  average training loss: 134.3713\n",
      "[epoch 2958]  average training loss: 134.3893\n",
      "[epoch 2959]  average training loss: 134.3911\n",
      "[epoch 2960]  average training loss: 134.3922\n",
      "[epoch 2961]  average training loss: 134.3693\n",
      "[epoch 2962]  average training loss: 134.3880\n",
      "[epoch 2963]  average training loss: 134.3730\n",
      "[epoch 2964]  average training loss: 134.3885\n",
      "[epoch 2965]  average training loss: 134.3858\n",
      "[epoch 2966]  average training loss: 134.3882\n",
      "[epoch 2967]  average training loss: 134.3712\n",
      "[epoch 2968]  average training loss: 134.3723\n",
      "[epoch 2969]  average training loss: 134.3630\n",
      "[epoch 2970]  average training loss: 134.3634\n",
      "[epoch 2971]  average training loss: 134.3588\n",
      "[epoch 2972]  average training loss: 134.3693\n",
      "[epoch 2973]  average training loss: 134.3764\n",
      "[epoch 2974]  average training loss: 134.3665\n",
      "[epoch 2975]  average training loss: 134.3633\n",
      "[epoch 2976]  average training loss: 134.3865\n",
      "[epoch 2977]  average training loss: 134.3457\n",
      "[epoch 2978]  average training loss: 134.3744\n",
      "[epoch 2979]  average training loss: 134.3747\n",
      "[epoch 2980]  average training loss: 134.3677\n",
      "[epoch 2981]  average training loss: 134.3577\n",
      "[epoch 2982]  average training loss: 134.3629\n",
      "[epoch 2983]  average training loss: 134.3880\n",
      "[epoch 2984]  average training loss: 134.3642\n",
      "[epoch 2985]  average training loss: 134.3851\n",
      "[epoch 2986]  average training loss: 134.3548\n",
      "[epoch 2987]  average training loss: 134.3540\n",
      "[epoch 2988]  average training loss: 134.3452\n",
      "[epoch 2989]  average training loss: 134.3607\n",
      "[epoch 2990]  average training loss: 134.3765\n",
      "[epoch 2991]  average training loss: 134.3412\n",
      "[epoch 2992]  average training loss: 134.3567\n",
      "[epoch 2993]  average training loss: 134.3392\n",
      "[epoch 2994]  average training loss: 134.3405\n",
      "[epoch 2995]  average training loss: 134.3338\n",
      "[epoch 2996]  average training loss: 134.3303\n",
      "[epoch 2997]  average training loss: 134.3429\n",
      "[epoch 2998]  average training loss: 134.3415\n",
      "[epoch 2999]  average training loss: 134.3501\n"
     ]
    }
   ],
   "source": [
    "train_loader = dataloader_first()\n",
    "# clear param store\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# setup the VAE\n",
    "vae = VAE(use_cuda=USE_CUDA)\n",
    "\n",
    "# setup the optimizer\n",
    "adam_params = {\"lr\": 0.0003}\n",
    "optimizer = Adam(adam_params)\n",
    "\n",
    "\n",
    "svi = SVI(vae.model, vae.guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "\n",
    "train_elbo = []\n",
    "test_elbo = []\n",
    "# training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_epoch_loss_train = train(svi, train_loader, use_cuda=USE_CUDA)\n",
    "    \n",
    "    train_elbo.append(-total_epoch_loss_train)\n",
    "    print(\"[epoch %03d]  average training loss: %.4f\" % (epoch, total_epoch_loss_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint():\n",
    "    log(\"loading model from ...\")\n",
    "    dmm.load_state_dict(torch.load('/raid/scratch/davidb/1_DEVELOPMENT/VAE_FIRST/models/file_conf2_supervised_vae_epoch_'+str(epoch)))\n",
    "    log(\"loading optimizer states from ...\")\n",
    "    optimizer.load('/raid/scratch/davidb/1_DEVELOPMENT/VAE_FIRST/models/file_conf2_supervised_vae_epoch_'+str(epoch)+'_opt')\n",
    "    log(\"done loading model and optimizer states.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch):\n",
    "    print(\"saving model to ...\")\n",
    "    torch.save(vae.state_dict(), '/raid/scratch/davidb/1_DEVELOPMENT/VAE_FIRST/models/file_conf2_supervised_vae_epoch_'+str(epoch))\n",
    "    print(\"saving optimizer states...\")\n",
    "    optimizer.save('/raid/scratch/davidb/1_DEVELOPMENT/VAE_FIRST/models/file_conf2_supervised_vae_epoch_'+str(epoch)+'_opt')\n",
    "    print(\"done saving model and optimizer checkpoints to disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_image_sampler(z0=0.0,z1=0.0, z2=0.0, z3=0.0, z4=0.0, z5=0.0, z6=0.0, z7=0.0, z8=0.0, z9=0.0):\n",
    "    z = torch.rand(1,10)\n",
    "    z[0,0]=z0\n",
    "    z[0,1]=z1\n",
    "    z[0,2]=z2\n",
    "    z[0,3]=z3\n",
    "    z[0,4]=z4\n",
    "    z[0,5]=z5   \n",
    "    z[0,6]=z6\n",
    "    z[0,7]=z7    \n",
    "    z[0,8]=z8\n",
    "    z[0,9]=z9\n",
    "    labels_y = torch.tensor(np.zeros((10)))\n",
    "    labels_y[0] = 1\n",
    "    single_sample_image = vae.decoder([z,labels_y.float()])\n",
    "    image_array_single =single_sample_image.reshape(28,28).cpu().detach().numpy()\n",
    "    temp_array=image_array_single\n",
    "  #  plt.figure(figsize = (10,10))\n",
    "  #  plt.imshow(image_array_single)\n",
    " #   plt.colorbar()\n",
    " #   plt.show() \n",
    "    return image_array_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rand_0 = np.random.uniform(-2,2)\n",
    "rand_1 = np.random.uniform(-2,2)\n",
    "rand_2 = np.random.uniform(-0.5,0.5)\n",
    "rand_3 = np.random.uniform(-0.2,0.4)\n",
    "rand_4 = np.random.uniform(-1,1)\n",
    "rand_5 = np.random.uniform(-0.1,0.1)\n",
    "rand_6 = np.random.uniform(-0.1,0.2)\n",
    "rand_7 = np.random.uniform(-2.3,2.5)\n",
    "rand_8 = np.random.uniform(-0.2,0.2)\n",
    "rand_9 = np.random.uniform(-0.2,0.2)\n",
    "image = single_image_sampler(rand_0,rand_1,rand_2,rand_3,rand_4,rand_5,rand_6,rand_7,rand_8,rand_9)\n",
    "img_max = image.max() \n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "with torch.no_grad():\n",
    "    count = 0\n",
    "    z_fr1 = z_fr2 = torch.randn(100, 2)\n",
    "    labels_y1 = torch.tensor(np.zeros((100,2)))\n",
    "    labels_y2 = torch.tensor(np.zeros((100,2)))\n",
    "    for i in range (0,10):\n",
    "        for j in range (0,10):\n",
    "            z_fr1[count,0] = z_fr2[count,0] = np.random.uniform(-1,1)\n",
    "            z_fr1[count,1] = z_fr2[count,1] = np.random.uniform(-1,1)\n",
    "            labels_y1[count,0] = 1\n",
    "            labels_y2[count,1] = 1\n",
    "            count = count +1 \n",
    "        \n",
    "    sample1 = vae.decoder([z_fr1,labels_y1.float()])\n",
    "    \n",
    "    save_image(sample1.view(100, 1, 100, 100), 'fr1_sample_z_space_' +str(epoch)+'.png',nrow=10)\n",
    "    \n",
    "    sample2 = vae.decoder([z_fr2,labels_y2.float()])\n",
    "\n",
    "    save_image(sample2.view(100, 1, 100, 100), 'fr2_sample_z_space_' +str(epoch)+'.png',nrow=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00018433, 0.00033273, 0.00033441, ..., 0.00035796, 0.00019329,\n",
       "        0.00016873],\n",
       "       [0.00021016, 0.00027572, 0.00041435, ..., 0.00036804, 0.00031961,\n",
       "        0.0002838 ],\n",
       "       [0.00024903, 0.00040582, 0.0006807 , ..., 0.00037013, 0.00034296,\n",
       "        0.00029304],\n",
       "       ...,\n",
       "       [0.00030603, 0.00032814, 0.00037756, ..., 0.00041224, 0.00034986,\n",
       "        0.00026457],\n",
       "       [0.00028421, 0.00025892, 0.00030835, ..., 0.00039391, 0.00030643,\n",
       "        0.00013053],\n",
       "       [0.00014841, 0.00015673, 0.00026209, ..., 0.00039841, 0.00035061,\n",
       "        0.0001815 ]], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.reshape(100,100).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = torch.randn(1, 2)\n",
    "labels_y = torch.tensor(np.zeros((1,2)))\n",
    "z[0,0]=0.5\n",
    "z[0,1]=1.7\n",
    "labels_y[0,1] = 0\n",
    "labels_y[0,0] = 0\n",
    "labels_y[0,1] = 1\n",
    "sample = vae.decoder([z,labels_y.float()])\n",
    "\n",
    "temp_array=sample.reshape(100,100).cpu().detach().numpy()\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.imshow(temp_array)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = torch.randn(1, 2)\n",
    "labels_y = torch.tensor(np.zeros((1,2)))\n",
    "z[0,0]=0.5\n",
    "z[0,1]=1.7\n",
    "labels_y[0,1] = 0\n",
    "labels_y[0,0] = 0\n",
    "labels_y[0,0] = 1\n",
    "sample = vae.decoder([z,labels_y.float()])\n",
    "\n",
    "temp_array=sample.reshape(100,100).cpu().detach().numpy()\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.imshow(temp_array)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
