{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Auto Encoder Notebook\n",
    "Generative Algorithm that has been trained using FIRST Radio Sources. Makes use of the FRDEEP dataset of FIRST Radio Sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "from MiraBest import MiraBest\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports the images from the FRDEEP, we make use of the FIRST radio sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FRDEEP import FRDEEPF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader_first():\n",
    "    transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize([0.5],[0.5])])\n",
    "    \n",
    "    trainset = FRDEEPF(root='./FIRST_data', train=True, download=True, transform=transform)  \n",
    "    trainloader = torch.utils.data.DataLoader(trainset, shuffle=True, num_workers=2, batch_size=len(trainset))\n",
    "    \n",
    "    testset = FRDEEPF(root='./FIRST_data', train=False, download=True, transform=transform) \n",
    "    testloader = torch.utils.data.DataLoader(testset, shuffle=True, num_workers=2,batch_size = len(trainset))\n",
    "    \n",
    "    classes = ('FRI', 'FRII')\n",
    "    \n",
    "    # ---------------------------------------Train Data Array-------------------------------------------\n",
    "    \n",
    "    array_train= next(iter(trainloader))[0].numpy() # Training Datasets is loaded in numpy array\n",
    "    \n",
    "    array_label= next(iter(trainloader))[1].numpy() # Training Datasets labels is loaded in seperate numpy array\n",
    "    \n",
    "    augmented_data=np.zeros((19800,1,100,100))\n",
    "    \n",
    "    count=0\n",
    "    \n",
    "    for j in range(0,550):\n",
    "        image_object=Image.fromarray(array_train[j,0,:,:])\n",
    "        for i in range(0,36):\n",
    "            rotated=image_object.rotate(i*10)\n",
    "            imgarr = np.array(rotated)\n",
    "            temp_img_array=imgarr[25:125,25:125]\n",
    "            augmented_data[count,0,:,:]=temp_img_array\n",
    "            count+=1\n",
    "           \n",
    "    augmented_data=(augmented_data-np.min(augmented_data))/(np.max(augmented_data)-np.min(augmented_data))\n",
    "    \n",
    "    X=augmented_data\n",
    "    \n",
    "    X_random_mix=np.take(X,np.random.permutation(X.shape[0]),axis=0,out=X);\n",
    "    \n",
    "    \n",
    "    # ---------------------------------------Test Data Array-------------------------------------------\n",
    "    array_test = next(iter(testloader))[0].numpy()\n",
    "    \n",
    "    augmented_data_test=np.zeros((50,1,100,100))\n",
    "    \n",
    "    count=0\n",
    " #   \n",
    "    for j in range(0,50):\n",
    "        image_object_test=Image.fromarray(array_test[j,0,:,:])\n",
    "        for i in range(0,1):\n",
    "            rotated_test=image_object_test.rotate(i*10)\n",
    "            imgarr_test = np.array(rotated_test)\n",
    "            temp_img_array_test=imgarr_test[25:125,25:125]\n",
    "            augmented_data_test[count,0,:,:]=temp_img_array_test\n",
    "            count+=1\n",
    "           \n",
    "    augmented_data_test=(augmented_data_test-np.min(augmented_data_test))/(np.max(augmented_data_test)-np.min(augmented_data_test))\n",
    "    \n",
    "    X_test=augmented_data_test\n",
    "    \n",
    "    X_random_mix_test=np.take(X_test,np.random.permutation(X_test.shape[0]),axis=0,out=X_test);\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return X_random_mix,X_random_mix_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Auto Encoder Neural Network \n",
    "The Variational Auto Encoder consists of two neural network: i. An encoder and ii.a decoder.  \n",
    "- The Encoder\\\n",
    "The enncoder consists of 5 layers and an input layer. The 100 by 100 pizels are reshaped to 10000 input features that are then reduced to 4096 in the first connected layer, then to 2048 features, then to 1024 features, to 512 features, 256 features and finally to the final 2 dimensional latent space z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim1, h_dim2, h_dim3, h_dim4, h_dim5, z_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # encoder part\n",
    "        self.fc1 = nn.Linear(x_dim, h_dim1) #x_dim=10000 to h_dim1=4096 \n",
    "        self.fc2 = nn.Linear(h_dim1, h_dim2) #h_dim1=4096 to h_dim2=2048\n",
    "        self.fc3 = nn.Linear(h_dim2, h_dim3) #h_dim2=2048 to h_dim3=1024\n",
    "        self.fc4 = nn.Linear(h_dim3, h_dim4) #h_dim3=1024 to h_dim4=512\n",
    "        self.fc5 = nn.Linear(h_dim4, h_dim5) #h_dim4=512 to h_dim5=256\n",
    "        self.fc61 = nn.Linear(h_dim5, z_dim) #h_dim5=256 to z_dim=2\n",
    "        self.fc62 = nn.Linear(h_dim5, z_dim) #h_dim5=256 to z_dim=2\n",
    "        self.softplus = nn.Softplus()\n",
    "        \n",
    "        # decoder part\n",
    "        self.fc7 = nn.Linear(z_dim, h_dim5) #z_dim=2 to h_dim5=256\n",
    "        self.fc8 = nn.Linear(h_dim5, h_dim4) #h_dim5=256 to h_dim4=512\n",
    "        self.fc9 = nn.Linear(h_dim4, h_dim3) #h_dim4=512 to h_dim3=1024\n",
    "        self.fc10 = nn.Linear(h_dim3, h_dim2) #h_dim3=1024 to h_dim2=2048\n",
    "        self.fc11 = nn.Linear(h_dim2, h_dim1) #h_dim2=2048 to h_dim1=4096\n",
    "        self.fc12 = nn.Linear(h_dim1, x_dim)  #h_dim1=4096 to x_dim=10000\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        \n",
    "        slope_param = 0.0001\n",
    "        \n",
    "        h = self.softplus(self.fc1(x))\n",
    "        h = F.leaky_relu(self.fc2(h),slope_param)\n",
    "        h = F.leaky_relu(self.fc3(h),slope_param)\n",
    "        h = F.leaky_relu(self.fc4(h),slope_param)\n",
    "        h = F.leaky_relu(self.fc5(h),slope_param)\n",
    "        return self.fc61(h), self.fc62(h) # mu, log_var\n",
    "    \n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu) # return z sample\n",
    "        \n",
    "    def decoder(self, z):\n",
    "        slope_param = 0.0001\n",
    "        h = F.leaky_relu(self.fc7(z),slope_param)\n",
    "        h = F.leaky_relu(self.fc8(h),slope_param)\n",
    "        h = F.leaky_relu(self.fc9(h),slope_param)\n",
    "        h = F.leaky_relu(self.fc10(h),slope_param)\n",
    "        h = F.leaky_relu(self.fc11(h),slope_param)\n",
    "        return F.sigmoid(self.fc12(h)) # Try to see what happens when we put a Softplus Here\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encoder(x.view(-1, 10000))\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z), mu, log_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "vae = VAE(x_dim=10000, h_dim1= 4096, h_dim2=2048, h_dim3=1024, h_dim4=512, h_dim5=256, z_dim=2)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    vae.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adagrad(vae.parameters(), lr=0.3e-3)\n",
    "\n",
    "# return reconstruction error + KL divergence losses\n",
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 10000), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + KLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "X,X_test= dataloader_first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    vae.train()\n",
    "    train_loss = 0   \n",
    "    data=torch.zeros(100, 1, 100, 100).cpu\n",
    "    epoch_loss = 0\n",
    "    for j in range(0,198):\n",
    "        data = torch.from_numpy(X[j*100:(j+1)*100,:,:])\n",
    "        data = data.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        recon_batch, mu, log_var = vae(data.float())\n",
    "        loss = loss_function(recon_batch.float(), data.float(), mu, log_var)\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Testing Part:\n",
    "    data_test = torch.from_numpy(X_test)\n",
    "    data_test = data_test.cuda()\n",
    "    recon_batch_test,mu_test,log_var_test = vae(data_test.float())\n",
    "    test_loss = loss_function(recon_batch_test.float(), data_test.float(), mu_test, log_var_test).item()\n",
    "        \n",
    "        \n",
    "    print(\"Training Epoch:\"+str(epoch)+' Training Loss: '+str(train_loss/(198*100))+' Testing Loss: '+str(test_loss/50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch:1 Training Loss: 135.77713946101642 Testing Loss: 128.789462890625\n",
      "Training Epoch:2 Training Loss: 135.7536481021149 Testing Loss: 128.838857421875\n",
      "Training Epoch:3 Training Loss: 135.74420005721277 Testing Loss: 128.117958984375\n",
      "Training Epoch:4 Training Loss: 135.76207909169824 Testing Loss: 128.946357421875\n",
      "Training Epoch:5 Training Loss: 135.75337846235794 Testing Loss: 129.151220703125\n",
      "Training Epoch:6 Training Loss: 135.72132191051136 Testing Loss: 128.259091796875\n",
      "Training Epoch:7 Training Loss: 135.73805989583335 Testing Loss: 128.4937890625\n",
      "Training Epoch:8 Training Loss: 135.7175998263889 Testing Loss: 128.193994140625\n",
      "Training Epoch:9 Training Loss: 135.70734153053976 Testing Loss: 129.2716796875\n",
      "Training Epoch:10 Training Loss: 135.68647633562185 Testing Loss: 128.3391796875\n",
      "Training Epoch:11 Training Loss: 135.68139747080176 Testing Loss: 128.8798046875\n",
      "Training Epoch:12 Training Loss: 135.70294551964963 Testing Loss: 129.04337890625\n",
      "Training Epoch:13 Training Loss: 135.71829737807766 Testing Loss: 128.400556640625\n",
      "Training Epoch:14 Training Loss: 135.66465287642046 Testing Loss: 128.29708984375\n",
      "Training Epoch:15 Training Loss: 135.68112931068498 Testing Loss: 128.888349609375\n",
      "Training Epoch:16 Training Loss: 135.66941682449496 Testing Loss: 127.73236328125\n",
      "Training Epoch:17 Training Loss: 135.67496241714016 Testing Loss: 129.0573828125\n",
      "Training Epoch:18 Training Loss: 135.6341090100221 Testing Loss: 128.9539453125\n",
      "Training Epoch:19 Training Loss: 135.65132610282512 Testing Loss: 128.6668359375\n",
      "Training Epoch:20 Training Loss: 135.64891547309028 Testing Loss: 128.688134765625\n",
      "Training Epoch:21 Training Loss: 135.627164762863 Testing Loss: 128.65501953125\n",
      "Training Epoch:22 Training Loss: 135.63324430831756 Testing Loss: 128.839873046875\n",
      "Training Epoch:23 Training Loss: 135.61833338265467 Testing Loss: 129.073505859375\n",
      "Training Epoch:24 Training Loss: 135.63596329505998 Testing Loss: 128.861005859375\n",
      "Training Epoch:25 Training Loss: 135.61196279198234 Testing Loss: 129.488955078125\n",
      "Training Epoch:26 Training Loss: 135.6100335385101 Testing Loss: 128.866181640625\n",
      "Training Epoch:27 Training Loss: 135.58184861308396 Testing Loss: 128.67462890625\n",
      "Training Epoch:28 Training Loss: 135.56481632733585 Testing Loss: 128.551787109375\n",
      "Training Epoch:29 Training Loss: 135.5947333688447 Testing Loss: 128.668310546875\n",
      "Training Epoch:30 Training Loss: 135.5755281328914 Testing Loss: 129.139970703125\n",
      "Training Epoch:31 Training Loss: 135.55268510298296 Testing Loss: 128.4171875\n",
      "Training Epoch:32 Training Loss: 135.56427009351324 Testing Loss: 128.52595703125\n",
      "Training Epoch:33 Training Loss: 135.5717669369476 Testing Loss: 128.479130859375\n",
      "Training Epoch:34 Training Loss: 135.58966185290404 Testing Loss: 128.608466796875\n",
      "Training Epoch:35 Training Loss: 135.52688954979482 Testing Loss: 128.7297265625\n",
      "Training Epoch:36 Training Loss: 135.53239173966224 Testing Loss: 129.1380078125\n",
      "Training Epoch:37 Training Loss: 135.55879221906565 Testing Loss: 128.526201171875\n",
      "Training Epoch:38 Training Loss: 135.54360770793878 Testing Loss: 128.80873046875\n",
      "Training Epoch:39 Training Loss: 135.5104574554135 Testing Loss: 128.400380859375\n",
      "Training Epoch:40 Training Loss: 135.53787538470644 Testing Loss: 129.15681640625\n",
      "Training Epoch:41 Training Loss: 135.5364617365057 Testing Loss: 128.40349609375\n",
      "Training Epoch:42 Training Loss: 135.5322472281408 Testing Loss: 128.894365234375\n",
      "Training Epoch:43 Training Loss: 135.5263434146149 Testing Loss: 128.520927734375\n",
      "Training Epoch:44 Training Loss: 135.51063279277147 Testing Loss: 129.115556640625\n",
      "Training Epoch:45 Training Loss: 135.4864879754577 Testing Loss: 128.665029296875\n",
      "Training Epoch:46 Training Loss: 135.4515312795928 Testing Loss: 129.22326171875\n",
      "Training Epoch:47 Training Loss: 135.45866062973485 Testing Loss: 128.8292578125\n",
      "Training Epoch:48 Training Loss: 135.4569132733586 Testing Loss: 128.84796875\n",
      "Training Epoch:49 Training Loss: 135.4610685467961 Testing Loss: 128.86888671875\n",
      "Training Epoch:50 Training Loss: 135.4489837338226 Testing Loss: 128.90857421875\n",
      "Training Epoch:51 Training Loss: 135.44334906684028 Testing Loss: 128.832392578125\n",
      "Training Epoch:52 Training Loss: 135.4609494357639 Testing Loss: 128.45591796875\n",
      "Training Epoch:53 Training Loss: 135.44913002091224 Testing Loss: 128.734375\n",
      "Training Epoch:54 Training Loss: 135.45197285353535 Testing Loss: 128.9477734375\n",
      "Training Epoch:55 Training Loss: 135.4405292672822 Testing Loss: 128.10419921875\n",
      "Training Epoch:56 Training Loss: 135.445732421875 Testing Loss: 128.91345703125\n",
      "Training Epoch:57 Training Loss: 135.4245497455019 Testing Loss: 129.304697265625\n",
      "Training Epoch:58 Training Loss: 135.43372090041035 Testing Loss: 128.8666796875\n",
      "Training Epoch:59 Training Loss: 135.41670055042613 Testing Loss: 128.541982421875\n",
      "Training Epoch:60 Training Loss: 135.40519121882892 Testing Loss: 128.737294921875\n",
      "Training Epoch:61 Training Loss: 135.41724135890152 Testing Loss: 128.498388671875\n",
      "Training Epoch:62 Training Loss: 135.3710129123264 Testing Loss: 128.952744140625\n",
      "Training Epoch:63 Training Loss: 135.39486801609848 Testing Loss: 128.21630859375\n",
      "Training Epoch:64 Training Loss: 135.39246961805554 Testing Loss: 129.363515625\n",
      "Training Epoch:65 Training Loss: 135.3668878728693 Testing Loss: 128.485107421875\n",
      "Training Epoch:66 Training Loss: 135.36491176412562 Testing Loss: 128.8136328125\n",
      "Training Epoch:67 Training Loss: 135.37452237215908 Testing Loss: 129.044091796875\n",
      "Training Epoch:68 Training Loss: 135.36933761442552 Testing Loss: 128.713818359375\n",
      "Training Epoch:69 Training Loss: 135.38595811631944 Testing Loss: 129.023828125\n",
      "Training Epoch:70 Training Loss: 135.37297654277145 Testing Loss: 128.8105859375\n",
      "Training Epoch:71 Training Loss: 135.3391841757418 Testing Loss: 128.919482421875\n",
      "Training Epoch:72 Training Loss: 135.33561064749054 Testing Loss: 129.1694140625\n",
      "Training Epoch:73 Training Loss: 135.3392951980745 Testing Loss: 128.85849609375\n",
      "Training Epoch:74 Training Loss: 135.32756377249052 Testing Loss: 128.85806640625\n",
      "Training Epoch:75 Training Loss: 135.31637251420455 Testing Loss: 128.9888671875\n",
      "Training Epoch:76 Training Loss: 135.32448030105746 Testing Loss: 128.46216796875\n",
      "Training Epoch:77 Training Loss: 135.30757403132893 Testing Loss: 128.32595703125\n",
      "Training Epoch:78 Training Loss: 135.3359643801294 Testing Loss: 128.9066015625\n",
      "Training Epoch:79 Training Loss: 135.28596689551767 Testing Loss: 128.710029296875\n",
      "Training Epoch:80 Training Loss: 135.29646464646464 Testing Loss: 129.0230078125\n",
      "Training Epoch:81 Training Loss: 135.30614075323547 Testing Loss: 129.06462890625\n",
      "Training Epoch:82 Training Loss: 135.2815122415562 Testing Loss: 128.7923828125\n",
      "Training Epoch:83 Training Loss: 135.28681581439395 Testing Loss: 129.211884765625\n",
      "Training Epoch:84 Training Loss: 135.2813117009943 Testing Loss: 128.886591796875\n",
      "Training Epoch:85 Training Loss: 135.29245753432764 Testing Loss: 128.55119140625\n",
      "Training Epoch:86 Training Loss: 135.259095150726 Testing Loss: 128.89435546875\n",
      "Training Epoch:87 Training Loss: 135.2481095624211 Testing Loss: 128.75544921875\n",
      "Training Epoch:88 Training Loss: 135.26784672900885 Testing Loss: 129.04203125\n",
      "Training Epoch:89 Training Loss: 135.26445026436238 Testing Loss: 128.64625\n",
      "Training Epoch:90 Training Loss: 135.2441474313447 Testing Loss: 128.683134765625\n",
      "Training Epoch:91 Training Loss: 135.24154134114585 Testing Loss: 128.355654296875\n",
      "Training Epoch:92 Training Loss: 135.25909958964647 Testing Loss: 129.025478515625\n",
      "Training Epoch:93 Training Loss: 135.22100334398675 Testing Loss: 128.48357421875\n",
      "Training Epoch:94 Training Loss: 135.22549439709596 Testing Loss: 128.7617578125\n",
      "Training Epoch:95 Training Loss: 135.19763854363953 Testing Loss: 128.351064453125\n",
      "Training Epoch:96 Training Loss: 135.18489460029988 Testing Loss: 129.0178125\n",
      "Training Epoch:97 Training Loss: 135.19693028922032 Testing Loss: 129.26857421875\n",
      "Training Epoch:98 Training Loss: 135.20316672585227 Testing Loss: 128.749345703125\n",
      "Training Epoch:99 Training Loss: 135.20041533499054 Testing Loss: 128.46505859375\n",
      "Training Epoch:100 Training Loss: 135.19465721669823 Testing Loss: 128.9487109375\n",
      "Training Epoch:101 Training Loss: 135.1780864307134 Testing Loss: 128.31431640625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch:102 Training Loss: 135.177102667298 Testing Loss: 128.8262109375\n",
      "Training Epoch:103 Training Loss: 135.2064433889678 Testing Loss: 129.025458984375\n",
      "Training Epoch:104 Training Loss: 135.17700012823548 Testing Loss: 128.804404296875\n",
      "Training Epoch:105 Training Loss: 135.16783888691603 Testing Loss: 128.34677734375\n",
      "Training Epoch:106 Training Loss: 135.18393327809343 Testing Loss: 128.96048828125\n",
      "Training Epoch:107 Training Loss: 135.16604206123736 Testing Loss: 128.88298828125\n",
      "Training Epoch:108 Training Loss: 135.13331917810922 Testing Loss: 128.839853515625\n",
      "Training Epoch:109 Training Loss: 135.15892454821653 Testing Loss: 128.529443359375\n",
      "Training Epoch:110 Training Loss: 135.15734325678662 Testing Loss: 129.147734375\n",
      "Training Epoch:111 Training Loss: 135.12480843592172 Testing Loss: 129.298076171875\n",
      "Training Epoch:112 Training Loss: 135.1424877683081 Testing Loss: 128.584111328125\n",
      "Training Epoch:113 Training Loss: 135.1419163806029 Testing Loss: 128.75443359375\n",
      "Training Epoch:114 Training Loss: 135.11551905776514 Testing Loss: 129.082490234375\n",
      "Training Epoch:115 Training Loss: 135.14394713738952 Testing Loss: 129.396083984375\n",
      "Training Epoch:116 Training Loss: 135.1279096630366 Testing Loss: 128.900234375\n",
      "Training Epoch:117 Training Loss: 135.1218838778409 Testing Loss: 128.506796875\n",
      "Training Epoch:118 Training Loss: 135.11017173690024 Testing Loss: 129.496689453125\n",
      "Training Epoch:119 Training Loss: 135.11147934422348 Testing Loss: 129.125107421875\n",
      "Training Epoch:120 Training Loss: 135.08636753274936 Testing Loss: 128.46974609375\n",
      "Training Epoch:121 Training Loss: 135.10136777935605 Testing Loss: 128.606201171875\n",
      "Training Epoch:122 Training Loss: 135.0840618588226 Testing Loss: 128.83736328125\n",
      "Training Epoch:123 Training Loss: 135.08883720012625 Testing Loss: 128.5833984375\n",
      "Training Epoch:124 Training Loss: 135.06185448232324 Testing Loss: 128.8152734375\n",
      "Training Epoch:125 Training Loss: 135.07836470170454 Testing Loss: 129.470869140625\n",
      "Training Epoch:126 Training Loss: 135.0826691228693 Testing Loss: 129.097333984375\n",
      "Training Epoch:127 Training Loss: 135.06563165838068 Testing Loss: 129.206416015625\n",
      "Training Epoch:128 Training Loss: 135.05446002998738 Testing Loss: 128.810107421875\n",
      "Training Epoch:129 Training Loss: 135.07663584083016 Testing Loss: 128.695654296875\n",
      "Training Epoch:130 Training Loss: 135.07348746251577 Testing Loss: 128.714775390625\n",
      "Training Epoch:131 Training Loss: 135.0498905066288 Testing Loss: 128.76654296875\n",
      "Training Epoch:132 Training Loss: 135.07132694128788 Testing Loss: 129.2664453125\n",
      "Training Epoch:133 Training Loss: 135.0320223721591 Testing Loss: 128.708798828125\n",
      "Training Epoch:134 Training Loss: 135.04270364780618 Testing Loss: 129.201904296875\n",
      "Training Epoch:135 Training Loss: 135.02884435172032 Testing Loss: 128.592509765625\n",
      "Training Epoch:136 Training Loss: 135.01208609532827 Testing Loss: 129.087744140625\n",
      "Training Epoch:137 Training Loss: 135.0366315991951 Testing Loss: 128.707880859375\n",
      "Training Epoch:138 Training Loss: 135.01107303503787 Testing Loss: 128.510107421875\n",
      "Training Epoch:139 Training Loss: 135.01092082938763 Testing Loss: 128.9293359375\n",
      "Training Epoch:140 Training Loss: 135.0035553286774 Testing Loss: 128.99701171875\n",
      "Training Epoch:141 Training Loss: 134.97633128156565 Testing Loss: 128.9728515625\n",
      "Training Epoch:142 Training Loss: 134.98508039378157 Testing Loss: 129.65109375\n",
      "Training Epoch:143 Training Loss: 134.98562638099747 Testing Loss: 129.16603515625\n",
      "Training Epoch:144 Training Loss: 134.95846965751264 Testing Loss: 128.71470703125\n",
      "Training Epoch:145 Training Loss: 134.99859217171718 Testing Loss: 128.63068359375\n",
      "Training Epoch:146 Training Loss: 134.97753575797032 Testing Loss: 128.4963671875\n",
      "Training Epoch:147 Training Loss: 134.9659485479798 Testing Loss: 128.66544921875\n",
      "Training Epoch:148 Training Loss: 134.98147026909723 Testing Loss: 128.827431640625\n",
      "Training Epoch:149 Training Loss: 134.97201674952652 Testing Loss: 128.205224609375\n",
      "Training Epoch:150 Training Loss: 134.97582105232007 Testing Loss: 128.7409765625\n",
      "Training Epoch:151 Training Loss: 134.95365076546716 Testing Loss: 129.17818359375\n",
      "Training Epoch:152 Training Loss: 134.9412784090909 Testing Loss: 129.250185546875\n",
      "Training Epoch:153 Training Loss: 134.9561646740846 Testing Loss: 128.911259765625\n",
      "Training Epoch:154 Training Loss: 134.9339226740057 Testing Loss: 128.4484375\n",
      "Training Epoch:155 Training Loss: 134.95364908854165 Testing Loss: 128.72986328125\n",
      "Training Epoch:156 Training Loss: 134.94616768268625 Testing Loss: 128.600419921875\n",
      "Training Epoch:157 Training Loss: 134.92779390585542 Testing Loss: 128.103828125\n",
      "Training Epoch:158 Training Loss: 134.9125391611427 Testing Loss: 128.980615234375\n",
      "Training Epoch:159 Training Loss: 134.91544058751577 Testing Loss: 128.736455078125\n",
      "Training Epoch:160 Training Loss: 134.92851128472222 Testing Loss: 128.71513671875\n",
      "Training Epoch:161 Training Loss: 134.9430806601168 Testing Loss: 129.061962890625\n",
      "Training Epoch:162 Training Loss: 134.91847202493688 Testing Loss: 129.389228515625\n",
      "Training Epoch:163 Training Loss: 134.9171362551294 Testing Loss: 129.32291015625\n",
      "Training Epoch:164 Training Loss: 134.89268327809344 Testing Loss: 128.5265234375\n",
      "Training Epoch:165 Training Loss: 134.90569952454229 Testing Loss: 128.872607421875\n",
      "Training Epoch:166 Training Loss: 134.88674202967172 Testing Loss: 129.112255859375\n",
      "Training Epoch:167 Training Loss: 134.8645709536774 Testing Loss: 129.126845703125\n",
      "Training Epoch:168 Training Loss: 134.89182755287248 Testing Loss: 129.12884765625\n",
      "Training Epoch:169 Training Loss: 134.8877806384154 Testing Loss: 128.959560546875\n",
      "Training Epoch:170 Training Loss: 134.86059437144885 Testing Loss: 129.039990234375\n",
      "Training Epoch:171 Training Loss: 134.8569971689552 Testing Loss: 129.16380859375\n",
      "Training Epoch:172 Training Loss: 134.88656269728534 Testing Loss: 128.8181640625\n",
      "Training Epoch:173 Training Loss: 134.8665368035827 Testing Loss: 129.102451171875\n",
      "Training Epoch:174 Training Loss: 134.88853757299557 Testing Loss: 128.26826171875\n",
      "Training Epoch:175 Training Loss: 134.85737773240214 Testing Loss: 128.620283203125\n",
      "Training Epoch:176 Training Loss: 134.86591190222538 Testing Loss: 128.966494140625\n",
      "Training Epoch:177 Training Loss: 134.83989612926138 Testing Loss: 129.054033203125\n",
      "Training Epoch:178 Training Loss: 134.85184032709913 Testing Loss: 128.738125\n",
      "Training Epoch:179 Training Loss: 134.83383315577652 Testing Loss: 129.3020703125\n",
      "Training Epoch:180 Training Loss: 134.85552699850064 Testing Loss: 128.157802734375\n",
      "Training Epoch:181 Training Loss: 134.8447360815183 Testing Loss: 128.799873046875\n",
      "Training Epoch:182 Training Loss: 134.81361742424244 Testing Loss: 128.655791015625\n",
      "Training Epoch:183 Training Loss: 134.84890950520833 Testing Loss: 128.498486328125\n",
      "Training Epoch:184 Training Loss: 134.80987107402146 Testing Loss: 128.6765234375\n",
      "Training Epoch:185 Training Loss: 134.8070647885101 Testing Loss: 129.09162109375\n",
      "Training Epoch:186 Training Loss: 134.83535358467486 Testing Loss: 127.958876953125\n",
      "Training Epoch:187 Training Loss: 134.81214414654357 Testing Loss: 128.41021484375\n",
      "Training Epoch:188 Training Loss: 134.80108931107955 Testing Loss: 128.90865234375\n",
      "Training Epoch:189 Training Loss: 134.78593552714648 Testing Loss: 128.294619140625\n",
      "Training Epoch:190 Training Loss: 134.78125754616477 Testing Loss: 128.497890625\n",
      "Training Epoch:191 Training Loss: 134.78724288786302 Testing Loss: 128.65982421875\n",
      "Training Epoch:192 Training Loss: 134.81517765546087 Testing Loss: 129.00892578125\n",
      "Training Epoch:193 Training Loss: 134.76102154356062 Testing Loss: 128.905029296875\n",
      "Training Epoch:194 Training Loss: 134.7810274127999 Testing Loss: 129.1666796875\n",
      "Training Epoch:195 Training Loss: 134.79714597143308 Testing Loss: 129.291943359375\n",
      "Training Epoch:196 Training Loss: 134.78466264204545 Testing Loss: 128.5094140625\n",
      "Training Epoch:197 Training Loss: 134.77428706005367 Testing Loss: 128.9103515625\n",
      "Training Epoch:198 Training Loss: 134.76110642558396 Testing Loss: 128.615810546875\n",
      "Training Epoch:199 Training Loss: 134.7610590770991 Testing Loss: 129.147880859375\n",
      "Training Epoch:200 Training Loss: 134.7669860223327 Testing Loss: 128.475302734375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch:201 Training Loss: 134.761865234375 Testing Loss: 128.42677734375\n",
      "Training Epoch:202 Training Loss: 134.7711249704072 Testing Loss: 128.078525390625\n",
      "Training Epoch:203 Training Loss: 134.73835010258838 Testing Loss: 128.784140625\n",
      "Training Epoch:204 Training Loss: 134.72497795336173 Testing Loss: 128.774287109375\n",
      "Training Epoch:205 Training Loss: 134.73868282433713 Testing Loss: 129.55431640625\n",
      "Training Epoch:206 Training Loss: 134.73184885969064 Testing Loss: 128.9212109375\n",
      "Training Epoch:207 Training Loss: 134.7166384548611 Testing Loss: 129.10228515625\n",
      "Training Epoch:208 Training Loss: 134.7157875631313 Testing Loss: 128.774375\n",
      "Training Epoch:209 Training Loss: 134.7487777679135 Testing Loss: 128.78359375\n",
      "Training Epoch:210 Training Loss: 134.71532951586175 Testing Loss: 128.407900390625\n",
      "Training Epoch:211 Training Loss: 134.69090810448233 Testing Loss: 129.475908203125\n",
      "Training Epoch:212 Training Loss: 134.7170012133049 Testing Loss: 128.88462890625\n",
      "Training Epoch:213 Training Loss: 134.69617434106692 Testing Loss: 128.98197265625\n",
      "Training Epoch:214 Training Loss: 134.7076044625947 Testing Loss: 128.247119140625\n",
      "Training Epoch:215 Training Loss: 134.68135322956124 Testing Loss: 128.83173828125\n",
      "Training Epoch:216 Training Loss: 134.6764762863005 Testing Loss: 128.85958984375\n",
      "Training Epoch:217 Training Loss: 134.69162089646466 Testing Loss: 128.628857421875\n",
      "Training Epoch:218 Training Loss: 134.67851015033145 Testing Loss: 129.556923828125\n",
      "Training Epoch:219 Training Loss: 134.68248865609218 Testing Loss: 129.209052734375\n",
      "Training Epoch:220 Training Loss: 134.66108531605113 Testing Loss: 128.671884765625\n",
      "Training Epoch:221 Training Loss: 134.68466737689394 Testing Loss: 128.483037109375\n",
      "Training Epoch:222 Training Loss: 134.66267163825756 Testing Loss: 129.17171875\n",
      "Training Epoch:223 Training Loss: 134.66721748737373 Testing Loss: 128.745322265625\n",
      "Training Epoch:224 Training Loss: 134.6790128827336 Testing Loss: 128.472197265625\n",
      "Training Epoch:225 Training Loss: 134.6643762823548 Testing Loss: 128.265400390625\n",
      "Training Epoch:226 Training Loss: 134.66345737649937 Testing Loss: 129.2943359375\n",
      "Training Epoch:227 Training Loss: 134.64901835740216 Testing Loss: 128.671416015625\n",
      "Training Epoch:228 Training Loss: 134.6572726779514 Testing Loss: 128.68826171875\n",
      "Training Epoch:229 Training Loss: 134.63747514204545 Testing Loss: 128.926259765625\n",
      "Training Epoch:230 Training Loss: 134.6224925524779 Testing Loss: 129.238291015625\n",
      "Training Epoch:231 Training Loss: 134.65864780618688 Testing Loss: 128.99427734375\n",
      "Training Epoch:232 Training Loss: 134.64576142282198 Testing Loss: 128.77796875\n",
      "Training Epoch:233 Training Loss: 134.61780110677083 Testing Loss: 129.162412109375\n",
      "Training Epoch:234 Training Loss: 134.6215734000158 Testing Loss: 129.05240234375\n",
      "Training Epoch:235 Training Loss: 134.61675825639205 Testing Loss: 129.3658203125\n",
      "Training Epoch:236 Training Loss: 134.59655169862688 Testing Loss: 129.09396484375\n",
      "Training Epoch:237 Training Loss: 134.6008718532986 Testing Loss: 128.874052734375\n",
      "Training Epoch:238 Training Loss: 134.63151460898044 Testing Loss: 128.977314453125\n",
      "Training Epoch:239 Training Loss: 134.58166296756627 Testing Loss: 129.017666015625\n",
      "Training Epoch:240 Training Loss: 134.5812782611269 Testing Loss: 128.9826953125\n",
      "Training Epoch:241 Training Loss: 134.60379044349747 Testing Loss: 128.604169921875\n",
      "Training Epoch:242 Training Loss: 134.61465366556186 Testing Loss: 128.951748046875\n",
      "Training Epoch:243 Training Loss: 134.5913331557765 Testing Loss: 129.2541015625\n",
      "Training Epoch:244 Training Loss: 134.5842592428188 Testing Loss: 129.000458984375\n",
      "Training Epoch:245 Training Loss: 134.60827607125947 Testing Loss: 129.21349609375\n",
      "Training Epoch:246 Training Loss: 134.57384430239898 Testing Loss: 129.45591796875\n",
      "Training Epoch:247 Training Loss: 134.57061651672979 Testing Loss: 129.15\n",
      "Training Epoch:248 Training Loss: 134.57586706912878 Testing Loss: 128.411220703125\n",
      "Training Epoch:249 Training Loss: 134.56131160235165 Testing Loss: 128.754765625\n",
      "Training Epoch:250 Training Loss: 134.56374467329545 Testing Loss: 128.8471875\n",
      "Training Epoch:251 Training Loss: 134.55697388928346 Testing Loss: 128.828564453125\n",
      "Training Epoch:252 Training Loss: 134.53922230113636 Testing Loss: 128.658017578125\n",
      "Training Epoch:253 Training Loss: 134.5534703973327 Testing Loss: 128.87666015625\n",
      "Training Epoch:254 Training Loss: 134.55312953756314 Testing Loss: 128.94271484375\n",
      "Training Epoch:255 Training Loss: 134.56324327256945 Testing Loss: 128.700498046875\n",
      "Training Epoch:256 Training Loss: 134.56687583846275 Testing Loss: 129.65828125\n",
      "Training Epoch:257 Training Loss: 134.54129961726642 Testing Loss: 128.667265625\n",
      "Training Epoch:258 Training Loss: 134.54493824968435 Testing Loss: 128.8721875\n",
      "Training Epoch:259 Training Loss: 134.53607495857008 Testing Loss: 128.5741796875\n",
      "Training Epoch:260 Training Loss: 134.52062361900252 Testing Loss: 128.243779296875\n",
      "Training Epoch:261 Training Loss: 134.52730513139204 Testing Loss: 128.672822265625\n",
      "Training Epoch:262 Training Loss: 134.51897855508207 Testing Loss: 129.054697265625\n",
      "Training Epoch:263 Training Loss: 134.52488537720959 Testing Loss: 128.78966796875\n",
      "Training Epoch:264 Training Loss: 134.51220777107008 Testing Loss: 128.654189453125\n",
      "Training Epoch:265 Training Loss: 134.50701270517678 Testing Loss: 128.620146484375\n",
      "Training Epoch:266 Training Loss: 134.50996187460544 Testing Loss: 128.8026953125\n",
      "Training Epoch:267 Training Loss: 134.51594795612374 Testing Loss: 129.4744921875\n",
      "Training Epoch:268 Training Loss: 134.51211494831125 Testing Loss: 128.95953125\n",
      "Training Epoch:269 Training Loss: 134.5002720071812 Testing Loss: 128.7205078125\n",
      "Training Epoch:270 Training Loss: 134.49411172269572 Testing Loss: 129.19533203125\n",
      "Training Epoch:271 Training Loss: 134.4869060724432 Testing Loss: 128.63052734375\n",
      "Training Epoch:272 Training Loss: 134.47435601128473 Testing Loss: 128.91888671875\n",
      "Training Epoch:273 Training Loss: 134.47724150686554 Testing Loss: 129.43146484375\n",
      "Training Epoch:274 Training Loss: 134.48135771780304 Testing Loss: 128.927578125\n",
      "Training Epoch:275 Training Loss: 134.46375907512626 Testing Loss: 129.022099609375\n",
      "Training Epoch:276 Training Loss: 134.46040966303661 Testing Loss: 128.99509765625\n",
      "Training Epoch:277 Training Loss: 134.48605626578282 Testing Loss: 128.49935546875\n",
      "Training Epoch:278 Training Loss: 134.45655806107953 Testing Loss: 128.60556640625\n",
      "Training Epoch:279 Training Loss: 134.46763366082703 Testing Loss: 129.14625\n",
      "Training Epoch:280 Training Loss: 134.44537459556503 Testing Loss: 129.12927734375\n",
      "Training Epoch:281 Training Loss: 134.4722445647885 Testing Loss: 128.72220703125\n",
      "Training Epoch:282 Training Loss: 134.43998456242107 Testing Loss: 128.449638671875\n",
      "Training Epoch:283 Training Loss: 134.44238187539457 Testing Loss: 129.03271484375\n",
      "Training Epoch:284 Training Loss: 134.447005159012 Testing Loss: 129.163857421875\n",
      "Training Epoch:285 Training Loss: 134.44521287089646 Testing Loss: 128.9866015625\n",
      "Training Epoch:286 Training Loss: 134.4251694681187 Testing Loss: 128.4633203125\n",
      "Training Epoch:287 Training Loss: 134.4533088699495 Testing Loss: 128.330458984375\n",
      "Training Epoch:288 Training Loss: 134.4366930535827 Testing Loss: 128.348603515625\n",
      "Training Epoch:289 Training Loss: 134.44889875315656 Testing Loss: 129.1290625\n",
      "Training Epoch:290 Training Loss: 134.41276312934028 Testing Loss: 128.86306640625\n",
      "Training Epoch:291 Training Loss: 134.42595313486427 Testing Loss: 128.999326171875\n",
      "Training Epoch:292 Training Loss: 134.39274931936552 Testing Loss: 128.557001953125\n",
      "Training Epoch:293 Training Loss: 134.39481474905304 Testing Loss: 128.71650390625\n",
      "Training Epoch:294 Training Loss: 134.3955420908302 Testing Loss: 128.708212890625\n",
      "Training Epoch:295 Training Loss: 134.38876602943498 Testing Loss: 128.475986328125\n",
      "Training Epoch:296 Training Loss: 134.38552537089646 Testing Loss: 128.6356640625\n",
      "Training Epoch:297 Training Loss: 134.39514406762942 Testing Loss: 129.3387890625\n",
      "Training Epoch:298 Training Loss: 134.39445307567865 Testing Loss: 128.695224609375\n",
      "Training Epoch:299 Training Loss: 134.38345505839646 Testing Loss: 128.93841796875\n",
      "Training Epoch:300 Training Loss: 134.39527511442552 Testing Loss: 128.247890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch:301 Training Loss: 134.39103505760733 Testing Loss: 128.65181640625\n",
      "Training Epoch:302 Training Loss: 134.39332090435607 Testing Loss: 128.74224609375\n",
      "Training Epoch:303 Training Loss: 134.4016641512784 Testing Loss: 128.683955078125\n",
      "Training Epoch:304 Training Loss: 134.36039659288195 Testing Loss: 129.471416015625\n",
      "Training Epoch:305 Training Loss: 134.38780352351642 Testing Loss: 128.9965234375\n",
      "Training Epoch:306 Training Loss: 134.3603604403409 Testing Loss: 129.278203125\n",
      "Training Epoch:307 Training Loss: 134.36296189433398 Testing Loss: 128.89529296875\n",
      "Training Epoch:308 Training Loss: 134.3582795730745 Testing Loss: 128.720986328125\n",
      "Training Epoch:309 Training Loss: 134.36703129932133 Testing Loss: 129.264365234375\n",
      "Training Epoch:310 Training Loss: 134.3721926294192 Testing Loss: 128.641943359375\n",
      "Training Epoch:311 Training Loss: 134.3528447561553 Testing Loss: 128.71677734375\n",
      "Training Epoch:312 Training Loss: 134.33276293205492 Testing Loss: 128.80681640625\n",
      "Training Epoch:313 Training Loss: 134.35799350931188 Testing Loss: 129.382666015625\n",
      "Training Epoch:314 Training Loss: 134.31635850694445 Testing Loss: 129.081318359375\n",
      "Training Epoch:315 Training Loss: 134.34320716934974 Testing Loss: 128.291181640625\n",
      "Training Epoch:316 Training Loss: 134.32710222340594 Testing Loss: 128.392158203125\n",
      "Training Epoch:317 Training Loss: 134.3409944168245 Testing Loss: 129.11392578125\n",
      "Training Epoch:318 Training Loss: 134.34333584872158 Testing Loss: 129.058037109375\n",
      "Training Epoch:319 Training Loss: 134.32138050426136 Testing Loss: 129.10521484375\n",
      "Training Epoch:320 Training Loss: 134.31390033143938 Testing Loss: 128.508056640625\n",
      "Training Epoch:321 Training Loss: 134.33239331794508 Testing Loss: 129.311171875\n",
      "Training Epoch:322 Training Loss: 134.3153056936553 Testing Loss: 128.78626953125\n",
      "Training Epoch:323 Training Loss: 134.29901490490846 Testing Loss: 129.49025390625\n",
      "Training Epoch:324 Training Loss: 134.3080309442077 Testing Loss: 128.77626953125\n",
      "Training Epoch:325 Training Loss: 134.3131434955019 Testing Loss: 129.022958984375\n",
      "Training Epoch:326 Training Loss: 134.28625172624683 Testing Loss: 128.79388671875\n",
      "Training Epoch:327 Training Loss: 134.30500024660668 Testing Loss: 128.928779296875\n",
      "Training Epoch:328 Training Loss: 134.31636664496529 Testing Loss: 129.202177734375\n",
      "Training Epoch:329 Training Loss: 134.27874610361428 Testing Loss: 128.7911328125\n",
      "Training Epoch:330 Training Loss: 134.29828771109533 Testing Loss: 128.826162109375\n",
      "Training Epoch:331 Training Loss: 134.31222005208335 Testing Loss: 128.997666015625\n",
      "Training Epoch:332 Training Loss: 134.28421569207703 Testing Loss: 129.5493359375\n",
      "Training Epoch:333 Training Loss: 134.2518558633207 Testing Loss: 129.220537109375\n",
      "Training Epoch:334 Training Loss: 134.2811699514678 Testing Loss: 128.65119140625\n",
      "Training Epoch:335 Training Loss: 134.27369318181817 Testing Loss: 128.713056640625\n",
      "Training Epoch:336 Training Loss: 134.26786616161615 Testing Loss: 128.253251953125\n",
      "Training Epoch:337 Training Loss: 134.25570983270202 Testing Loss: 128.640615234375\n",
      "Training Epoch:338 Training Loss: 134.26583318536933 Testing Loss: 128.541865234375\n",
      "Training Epoch:339 Training Loss: 134.27259775489267 Testing Loss: 128.911376953125\n",
      "Training Epoch:340 Training Loss: 134.27659258601642 Testing Loss: 128.736005859375\n",
      "Training Epoch:341 Training Loss: 134.28740707859848 Testing Loss: 129.371787109375\n",
      "Training Epoch:342 Training Loss: 134.25451157078598 Testing Loss: 129.039501953125\n",
      "Training Epoch:343 Training Loss: 134.2652803424874 Testing Loss: 128.744365234375\n",
      "Training Epoch:344 Training Loss: 134.22925796046403 Testing Loss: 128.66619140625\n",
      "Training Epoch:345 Training Loss: 134.26786675347222 Testing Loss: 129.1176171875\n",
      "Training Epoch:346 Training Loss: 134.25016128077652 Testing Loss: 129.0746875\n",
      "Training Epoch:347 Training Loss: 134.23854645083648 Testing Loss: 129.2613671875\n",
      "Training Epoch:348 Training Loss: 134.24035042810922 Testing Loss: 128.879375\n",
      "Training Epoch:349 Training Loss: 134.2562015664457 Testing Loss: 128.43375\n",
      "Training Epoch:350 Training Loss: 134.23014978890467 Testing Loss: 129.405908203125\n",
      "Training Epoch:351 Training Loss: 134.22357481060607 Testing Loss: 128.8578515625\n",
      "Training Epoch:352 Training Loss: 134.2103345959596 Testing Loss: 128.918798828125\n",
      "Training Epoch:353 Training Loss: 134.22181290443498 Testing Loss: 129.1276953125\n",
      "Training Epoch:354 Training Loss: 134.2218942846433 Testing Loss: 129.1076953125\n",
      "Training Epoch:355 Training Loss: 134.19068482678347 Testing Loss: 129.04271484375\n",
      "Training Epoch:356 Training Loss: 134.220227667298 Testing Loss: 128.578955078125\n",
      "Training Epoch:357 Training Loss: 134.20315888375947 Testing Loss: 129.246875\n",
      "Training Epoch:358 Training Loss: 134.21064147332703 Testing Loss: 128.84615234375\n",
      "Training Epoch:359 Training Loss: 134.20364903922032 Testing Loss: 129.162841796875\n",
      "Training Epoch:360 Training Loss: 134.17319538154987 Testing Loss: 129.29703125\n",
      "Training Epoch:361 Training Loss: 134.20697023950441 Testing Loss: 128.529404296875\n",
      "Training Epoch:362 Training Loss: 134.191243588226 Testing Loss: 128.647158203125\n",
      "Training Epoch:363 Training Loss: 134.2171770931976 Testing Loss: 129.01869140625\n",
      "Training Epoch:364 Training Loss: 134.19396775370896 Testing Loss: 129.313955078125\n",
      "Training Epoch:365 Training Loss: 134.17514682962437 Testing Loss: 128.768759765625\n",
      "Training Epoch:366 Training Loss: 134.18622509272413 Testing Loss: 128.971943359375\n",
      "Training Epoch:367 Training Loss: 134.17621034564394 Testing Loss: 128.76619140625\n",
      "Training Epoch:368 Training Loss: 134.18202602193813 Testing Loss: 128.54857421875\n",
      "Training Epoch:369 Training Loss: 134.17282542219067 Testing Loss: 128.62529296875\n",
      "Training Epoch:370 Training Loss: 134.17195524581754 Testing Loss: 129.032431640625\n",
      "Training Epoch:371 Training Loss: 134.15720155658144 Testing Loss: 129.597861328125\n",
      "Training Epoch:372 Training Loss: 134.1730615234375 Testing Loss: 128.712373046875\n",
      "Training Epoch:373 Training Loss: 134.14799311474115 Testing Loss: 129.223076171875\n",
      "Training Epoch:374 Training Loss: 134.16377924755366 Testing Loss: 128.866142578125\n",
      "Training Epoch:375 Training Loss: 134.15065523398042 Testing Loss: 128.80880859375\n",
      "Training Epoch:376 Training Loss: 134.12946555397727 Testing Loss: 128.605751953125\n",
      "Training Epoch:377 Training Loss: 134.1477688012942 Testing Loss: 128.70900390625\n",
      "Training Epoch:378 Training Loss: 134.1407578716856 Testing Loss: 129.054677734375\n",
      "Training Epoch:379 Training Loss: 134.15950303819446 Testing Loss: 129.19009765625\n",
      "Training Epoch:380 Training Loss: 134.12957997948232 Testing Loss: 128.453330078125\n",
      "Training Epoch:381 Training Loss: 134.1450341796875 Testing Loss: 128.637509765625\n",
      "Training Epoch:382 Training Loss: 134.13092955926453 Testing Loss: 128.501611328125\n",
      "Training Epoch:383 Training Loss: 134.14160891137942 Testing Loss: 128.4955859375\n",
      "Training Epoch:384 Training Loss: 134.10801042653094 Testing Loss: 128.694013671875\n",
      "Training Epoch:385 Training Loss: 134.12815138691604 Testing Loss: 128.59208984375\n",
      "Training Epoch:386 Training Loss: 134.08557597458966 Testing Loss: 129.068720703125\n",
      "Training Epoch:387 Training Loss: 134.12498041942865 Testing Loss: 129.255498046875\n",
      "Training Epoch:388 Training Loss: 134.12981307212752 Testing Loss: 129.0809765625\n",
      "Training Epoch:389 Training Loss: 134.09663228969382 Testing Loss: 129.756494140625\n",
      "Training Epoch:390 Training Loss: 134.11622356376262 Testing Loss: 128.8074609375\n",
      "Training Epoch:391 Training Loss: 134.11053760258838 Testing Loss: 128.80537109375\n",
      "Training Epoch:392 Training Loss: 134.11705970841226 Testing Loss: 128.773349609375\n",
      "Training Epoch:393 Training Loss: 134.08819784761678 Testing Loss: 128.4832421875\n",
      "Training Epoch:394 Training Loss: 134.10020113241794 Testing Loss: 129.085693359375\n",
      "Training Epoch:395 Training Loss: 134.09931522253788 Testing Loss: 128.837626953125\n",
      "Training Epoch:396 Training Loss: 134.11560862531564 Testing Loss: 128.94416015625\n",
      "Training Epoch:397 Training Loss: 134.08180718315973 Testing Loss: 129.475859375\n",
      "Training Epoch:398 Training Loss: 134.09643342605744 Testing Loss: 128.869521484375\n",
      "Training Epoch:399 Training Loss: 134.1098544034091 Testing Loss: 129.054189453125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch:400 Training Loss: 134.09384716303663 Testing Loss: 128.70439453125\n",
      "Training Epoch:401 Training Loss: 134.06547210385102 Testing Loss: 128.6586328125\n",
      "Training Epoch:402 Training Loss: 134.06930501302082 Testing Loss: 128.6662890625\n",
      "Training Epoch:403 Training Loss: 134.08906471946023 Testing Loss: 129.3660546875\n",
      "Training Epoch:404 Training Loss: 134.08195613360164 Testing Loss: 128.69806640625\n",
      "Training Epoch:405 Training Loss: 134.07592304884784 Testing Loss: 128.692099609375\n",
      "Training Epoch:406 Training Loss: 134.0736640329072 Testing Loss: 129.076103515625\n",
      "Training Epoch:407 Training Loss: 134.0685368331755 Testing Loss: 128.738251953125\n",
      "Training Epoch:408 Training Loss: 134.06558574021466 Testing Loss: 128.4219140625\n",
      "Training Epoch:409 Training Loss: 134.0735054648043 Testing Loss: 128.91681640625\n",
      "Training Epoch:410 Training Loss: 134.0625254498106 Testing Loss: 129.182021484375\n",
      "Training Epoch:411 Training Loss: 134.04936760179925 Testing Loss: 128.859228515625\n",
      "Training Epoch:412 Training Loss: 134.05822877209596 Testing Loss: 128.952919921875\n",
      "Training Epoch:413 Training Loss: 134.0408981908933 Testing Loss: 128.964765625\n",
      "Training Epoch:414 Training Loss: 134.0353185665246 Testing Loss: 129.501796875\n",
      "Training Epoch:415 Training Loss: 134.02310596196338 Testing Loss: 128.883994140625\n",
      "Training Epoch:416 Training Loss: 134.03362122198547 Testing Loss: 128.93470703125\n",
      "Training Epoch:417 Training Loss: 134.04320894491792 Testing Loss: 128.71064453125\n",
      "Training Epoch:418 Training Loss: 134.03783933080808 Testing Loss: 129.22021484375\n",
      "Training Epoch:419 Training Loss: 134.03153557054924 Testing Loss: 128.97611328125\n",
      "Training Epoch:420 Training Loss: 134.0125541055082 Testing Loss: 129.05109375\n",
      "Training Epoch:421 Training Loss: 134.04490757181188 Testing Loss: 129.152861328125\n",
      "Training Epoch:422 Training Loss: 134.02998850812816 Testing Loss: 129.077529296875\n",
      "Training Epoch:423 Training Loss: 134.01152698863638 Testing Loss: 128.837490234375\n",
      "Training Epoch:424 Training Loss: 133.99720757378472 Testing Loss: 129.5946875\n",
      "Training Epoch:425 Training Loss: 134.02366778132892 Testing Loss: 128.809892578125\n",
      "Training Epoch:426 Training Loss: 134.0086186572759 Testing Loss: 128.97259765625\n",
      "Training Epoch:427 Training Loss: 134.02051999487057 Testing Loss: 129.351416015625\n",
      "Training Epoch:428 Training Loss: 133.99618080216226 Testing Loss: 128.70751953125\n",
      "Training Epoch:429 Training Loss: 134.0267444464173 Testing Loss: 129.407998046875\n",
      "Training Epoch:430 Training Loss: 133.98673739346592 Testing Loss: 129.33236328125\n",
      "Training Epoch:431 Training Loss: 133.9933580926452 Testing Loss: 129.1009765625\n",
      "Training Epoch:432 Training Loss: 133.9664963107639 Testing Loss: 129.0404296875\n",
      "Training Epoch:433 Training Loss: 133.9960309146149 Testing Loss: 128.411357421875\n",
      "Training Epoch:434 Training Loss: 133.98980715356691 Testing Loss: 129.138173828125\n",
      "Training Epoch:435 Training Loss: 133.96397120620264 Testing Loss: 129.223095703125\n",
      "Training Epoch:436 Training Loss: 133.9862417633365 Testing Loss: 128.603427734375\n",
      "Training Epoch:437 Training Loss: 133.96085029987373 Testing Loss: 128.873701171875\n",
      "Training Epoch:438 Training Loss: 133.97449948705807 Testing Loss: 129.346875\n",
      "Training Epoch:439 Training Loss: 133.97476399739583 Testing Loss: 128.98623046875\n",
      "Training Epoch:440 Training Loss: 133.9667026712437 Testing Loss: 129.243212890625\n",
      "Training Epoch:441 Training Loss: 133.9689124151673 Testing Loss: 129.3628515625\n",
      "Training Epoch:442 Training Loss: 133.96898777817236 Testing Loss: 129.079541015625\n",
      "Training Epoch:443 Training Loss: 133.97723810369317 Testing Loss: 128.901552734375\n",
      "Training Epoch:444 Training Loss: 133.9616435842803 Testing Loss: 129.2890234375\n",
      "Training Epoch:445 Training Loss: 133.93966436829228 Testing Loss: 128.842119140625\n",
      "Training Epoch:446 Training Loss: 133.95525474471276 Testing Loss: 128.90255859375\n",
      "Training Epoch:447 Training Loss: 133.9659185112847 Testing Loss: 128.952783203125\n",
      "Training Epoch:448 Training Loss: 133.961435398911 Testing Loss: 129.32193359375\n",
      "Training Epoch:449 Training Loss: 133.95274280894887 Testing Loss: 128.5481640625\n",
      "Training Epoch:450 Training Loss: 133.95671825678662 Testing Loss: 129.00294921875\n",
      "Training Epoch:451 Training Loss: 133.9326452020202 Testing Loss: 128.8316796875\n",
      "Training Epoch:452 Training Loss: 133.9583818655303 Testing Loss: 129.02181640625\n",
      "Training Epoch:453 Training Loss: 133.91455260613952 Testing Loss: 128.889541015625\n",
      "Training Epoch:454 Training Loss: 133.94554051254735 Testing Loss: 129.1468359375\n",
      "Training Epoch:455 Training Loss: 133.92530850497158 Testing Loss: 128.385654296875\n",
      "Training Epoch:456 Training Loss: 133.96264022056502 Testing Loss: 128.58611328125\n",
      "Training Epoch:457 Training Loss: 133.9244028665562 Testing Loss: 128.8394140625\n",
      "Training Epoch:458 Training Loss: 133.93911630958019 Testing Loss: 129.383271484375\n",
      "Training Epoch:459 Training Loss: 133.93508858112375 Testing Loss: 128.6720703125\n",
      "Training Epoch:460 Training Loss: 133.91326088028725 Testing Loss: 129.183037109375\n",
      "Training Epoch:461 Training Loss: 133.91234434185606 Testing Loss: 128.9175\n",
      "Training Epoch:462 Training Loss: 133.92502475931187 Testing Loss: 129.2145703125\n",
      "Training Epoch:463 Training Loss: 133.90368257773042 Testing Loss: 129.225107421875\n",
      "Training Epoch:464 Training Loss: 133.90580448035038 Testing Loss: 128.999951171875\n",
      "Training Epoch:465 Training Loss: 133.91307947640468 Testing Loss: 128.785986328125\n",
      "Training Epoch:466 Training Loss: 133.92886442550505 Testing Loss: 129.145595703125\n",
      "Training Epoch:467 Training Loss: 133.91005341500946 Testing Loss: 128.603701171875\n",
      "Training Epoch:468 Training Loss: 133.9073842428188 Testing Loss: 128.5387109375\n",
      "Training Epoch:469 Training Loss: 133.8965334497317 Testing Loss: 128.660478515625\n",
      "Training Epoch:470 Training Loss: 133.90363779395517 Testing Loss: 128.359453125\n",
      "Training Epoch:471 Training Loss: 133.91038490372475 Testing Loss: 128.786455078125\n",
      "Training Epoch:472 Training Loss: 133.88064408735795 Testing Loss: 128.620703125\n",
      "Training Epoch:473 Training Loss: 133.8813154000947 Testing Loss: 128.963876953125\n",
      "Training Epoch:474 Training Loss: 133.90466757417929 Testing Loss: 128.678740234375\n",
      "Training Epoch:475 Training Loss: 133.8694717684659 Testing Loss: 128.9431640625\n",
      "Training Epoch:476 Training Loss: 133.90403354837437 Testing Loss: 128.810380859375\n",
      "Training Epoch:477 Training Loss: 133.8610737255366 Testing Loss: 128.632197265625\n",
      "Training Epoch:478 Training Loss: 133.86684086963385 Testing Loss: 129.150908203125\n",
      "Training Epoch:479 Training Loss: 133.86961904198233 Testing Loss: 128.813642578125\n",
      "Training Epoch:480 Training Loss: 133.86916804766415 Testing Loss: 128.54388671875\n",
      "Training Epoch:481 Training Loss: 133.88190631904988 Testing Loss: 129.07671875\n",
      "Training Epoch:482 Training Loss: 133.85669364543875 Testing Loss: 128.845380859375\n",
      "Training Epoch:483 Training Loss: 133.8643729285038 Testing Loss: 128.96931640625\n",
      "Training Epoch:484 Training Loss: 133.86802946456754 Testing Loss: 129.3660546875\n",
      "Training Epoch:485 Training Loss: 133.86538534761678 Testing Loss: 129.04267578125\n",
      "Training Epoch:486 Training Loss: 133.8384336529356 Testing Loss: 128.48771484375\n",
      "Training Epoch:487 Training Loss: 133.85972256747158 Testing Loss: 128.65857421875\n",
      "Training Epoch:488 Training Loss: 133.86216545336174 Testing Loss: 128.834951171875\n",
      "Training Epoch:489 Training Loss: 133.84817979600695 Testing Loss: 128.212001953125\n",
      "Training Epoch:490 Training Loss: 133.8683352075442 Testing Loss: 128.8766015625\n",
      "Training Epoch:491 Training Loss: 133.83550998263888 Testing Loss: 129.08896484375\n",
      "Training Epoch:492 Training Loss: 133.84986017400567 Testing Loss: 128.55318359375\n",
      "Training Epoch:493 Training Loss: 133.8405295138889 Testing Loss: 129.09556640625\n",
      "Training Epoch:494 Training Loss: 133.83469864662248 Testing Loss: 128.5237890625\n",
      "Training Epoch:495 Training Loss: 133.8263792712279 Testing Loss: 128.587978515625\n",
      "Training Epoch:496 Training Loss: 133.82747987689393 Testing Loss: 129.341376953125\n",
      "Training Epoch:497 Training Loss: 133.83251509232954 Testing Loss: 129.627294921875\n",
      "Training Epoch:498 Training Loss: 133.80927403922033 Testing Loss: 129.1286328125\n",
      "Training Epoch:499 Training Loss: 133.8177974569918 Testing Loss: 128.87263671875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch:500 Training Loss: 133.81440538194445 Testing Loss: 129.73849609375\n",
      "Training Epoch:501 Training Loss: 133.82542253590594 Testing Loss: 128.49939453125\n",
      "Training Epoch:502 Training Loss: 133.81853081597222 Testing Loss: 129.187763671875\n",
      "Training Epoch:503 Training Loss: 133.79161369554924 Testing Loss: 128.707861328125\n",
      "Training Epoch:504 Training Loss: 133.79919936671402 Testing Loss: 128.7740234375\n",
      "Training Epoch:505 Training Loss: 133.81683021622476 Testing Loss: 129.163388671875\n",
      "Training Epoch:506 Training Loss: 133.79726725260417 Testing Loss: 129.0109765625\n",
      "Training Epoch:507 Training Loss: 133.81953889480744 Testing Loss: 128.536962890625\n",
      "Training Epoch:508 Training Loss: 133.81994249131944 Testing Loss: 129.088125\n",
      "Training Epoch:509 Training Loss: 133.79369298453284 Testing Loss: 129.188076171875\n",
      "Training Epoch:510 Training Loss: 133.7607953559028 Testing Loss: 128.837001953125\n",
      "Training Epoch:511 Training Loss: 133.7908128649779 Testing Loss: 129.18478515625\n",
      "Training Epoch:512 Training Loss: 133.7931370837279 Testing Loss: 129.210771484375\n",
      "Training Epoch:513 Training Loss: 133.80488360164142 Testing Loss: 128.961044921875\n",
      "Training Epoch:514 Training Loss: 133.80259158972538 Testing Loss: 128.75728515625\n",
      "Training Epoch:515 Training Loss: 133.79098909011995 Testing Loss: 129.067080078125\n",
      "Training Epoch:516 Training Loss: 133.7803064827967 Testing Loss: 128.889384765625\n",
      "Training Epoch:517 Training Loss: 133.78441628196023 Testing Loss: 128.7645703125\n",
      "Training Epoch:518 Training Loss: 133.77974486071653 Testing Loss: 128.625556640625\n",
      "Training Epoch:519 Training Loss: 133.78109039614898 Testing Loss: 128.847177734375\n",
      "Training Epoch:520 Training Loss: 133.78341224747476 Testing Loss: 128.575712890625\n",
      "Training Epoch:521 Training Loss: 133.78026288273358 Testing Loss: 129.146455078125\n",
      "Training Epoch:522 Training Loss: 133.78667786261047 Testing Loss: 128.820888671875\n",
      "Training Epoch:523 Training Loss: 133.76819982047033 Testing Loss: 129.667236328125\n",
      "Training Epoch:524 Training Loss: 133.7508260830966 Testing Loss: 127.98001953125\n",
      "Training Epoch:525 Training Loss: 133.7550620462437 Testing Loss: 129.24970703125\n",
      "Training Epoch:526 Training Loss: 133.75716096511994 Testing Loss: 128.537041015625\n",
      "Training Epoch:527 Training Loss: 133.75319227430555 Testing Loss: 128.67978515625\n",
      "Training Epoch:528 Training Loss: 133.75222266611428 Testing Loss: 129.273271484375\n",
      "Training Epoch:529 Training Loss: 133.75143687855115 Testing Loss: 129.507958984375\n",
      "Training Epoch:530 Training Loss: 133.73249028369634 Testing Loss: 128.950390625\n",
      "Training Epoch:531 Training Loss: 133.74509888928347 Testing Loss: 128.95912109375\n",
      "Training Epoch:532 Training Loss: 133.76061543166034 Testing Loss: 129.0359765625\n",
      "Training Epoch:533 Training Loss: 133.73011200875948 Testing Loss: 129.173671875\n",
      "Training Epoch:534 Training Loss: 133.74515284682766 Testing Loss: 129.13376953125\n",
      "Training Epoch:535 Training Loss: 133.75736357717804 Testing Loss: 129.33080078125\n",
      "Training Epoch:536 Training Loss: 133.72264317984533 Testing Loss: 129.05466796875\n",
      "Training Epoch:537 Training Loss: 133.73617217092803 Testing Loss: 129.477099609375\n",
      "Training Epoch:538 Training Loss: 133.71405909682764 Testing Loss: 129.516396484375\n",
      "Training Epoch:539 Training Loss: 133.72023057725696 Testing Loss: 129.16484375\n",
      "Training Epoch:540 Training Loss: 133.7078239425505 Testing Loss: 128.38771484375\n",
      "Training Epoch:541 Training Loss: 133.7204972577336 Testing Loss: 128.6728515625\n",
      "Training Epoch:542 Training Loss: 133.71614869397095 Testing Loss: 128.456064453125\n",
      "Training Epoch:543 Training Loss: 133.7195711016414 Testing Loss: 129.0615625\n",
      "Training Epoch:544 Training Loss: 133.70559432212752 Testing Loss: 129.31140625\n",
      "Training Epoch:545 Training Loss: 133.72152901081122 Testing Loss: 128.810576171875\n",
      "Training Epoch:546 Training Loss: 133.69167682686236 Testing Loss: 128.979443359375\n",
      "Training Epoch:547 Training Loss: 133.71696259469698 Testing Loss: 129.1186328125\n",
      "Training Epoch:548 Training Loss: 133.66654844341855 Testing Loss: 129.017802734375\n",
      "Training Epoch:549 Training Loss: 133.705937352036 Testing Loss: 128.9341015625\n",
      "Training Epoch:550 Training Loss: 133.71972907788825 Testing Loss: 129.3233203125\n",
      "Training Epoch:551 Training Loss: 133.71783573035037 Testing Loss: 128.750859375\n",
      "Training Epoch:552 Training Loss: 133.6994669842961 Testing Loss: 129.21888671875\n",
      "Training Epoch:553 Training Loss: 133.71651520083648 Testing Loss: 128.9512109375\n",
      "Training Epoch:554 Training Loss: 133.6825841422033 Testing Loss: 128.994697265625\n",
      "Training Epoch:555 Training Loss: 133.69967497238005 Testing Loss: 128.600478515625\n",
      "Training Epoch:556 Training Loss: 133.6820132477115 Testing Loss: 128.784580078125\n",
      "Training Epoch:557 Training Loss: 133.67030026830807 Testing Loss: 128.834755859375\n",
      "Training Epoch:558 Training Loss: 133.6717181088226 Testing Loss: 128.499521484375\n",
      "Training Epoch:559 Training Loss: 133.6741034860322 Testing Loss: 128.846484375\n",
      "Training Epoch:560 Training Loss: 133.68173329979481 Testing Loss: 128.96384765625\n",
      "Training Epoch:561 Training Loss: 133.68706962200127 Testing Loss: 128.592548828125\n",
      "Training Epoch:562 Training Loss: 133.66126627604166 Testing Loss: 129.06984375\n",
      "Training Epoch:563 Training Loss: 133.67076305042613 Testing Loss: 128.2915234375\n",
      "Training Epoch:564 Training Loss: 133.68213196417298 Testing Loss: 128.3833984375\n",
      "Training Epoch:565 Training Loss: 133.67885663273358 Testing Loss: 128.7415234375\n",
      "Training Epoch:566 Training Loss: 133.6718062460543 Testing Loss: 129.02220703125\n",
      "Training Epoch:567 Training Loss: 133.65329969618057 Testing Loss: 128.964052734375\n",
      "Training Epoch:568 Training Loss: 133.66044206715594 Testing Loss: 128.531123046875\n",
      "Training Epoch:569 Training Loss: 133.64923828125 Testing Loss: 128.930205078125\n",
      "Training Epoch:570 Training Loss: 133.65518357402146 Testing Loss: 128.677529296875\n",
      "Training Epoch:571 Training Loss: 133.64604334359217 Testing Loss: 129.03947265625\n",
      "Training Epoch:572 Training Loss: 133.65184959951074 Testing Loss: 129.153046875\n",
      "Training Epoch:573 Training Loss: 133.64327360519255 Testing Loss: 128.8084375\n",
      "Training Epoch:574 Training Loss: 133.65025627367424 Testing Loss: 129.146904296875\n",
      "Training Epoch:575 Training Loss: 133.66218034840594 Testing Loss: 129.219599609375\n",
      "Training Epoch:576 Training Loss: 133.6581300307765 Testing Loss: 128.582578125\n",
      "Training Epoch:577 Training Loss: 133.61189561631946 Testing Loss: 129.185302734375\n",
      "Training Epoch:578 Training Loss: 133.61834877091223 Testing Loss: 128.344892578125\n",
      "Training Epoch:579 Training Loss: 133.63765842013888 Testing Loss: 129.0901953125\n",
      "Training Epoch:580 Training Loss: 133.62132151594065 Testing Loss: 128.622138671875\n",
      "Training Epoch:581 Training Loss: 133.6274913687658 Testing Loss: 128.859970703125\n",
      "Training Epoch:582 Training Loss: 133.63301190617108 Testing Loss: 128.76755859375\n",
      "Training Epoch:583 Training Loss: 133.6229120797822 Testing Loss: 129.181669921875\n",
      "Training Epoch:584 Training Loss: 133.62833205097854 Testing Loss: 128.43720703125\n",
      "Training Epoch:585 Training Loss: 133.60531822127524 Testing Loss: 129.08271484375\n",
      "Training Epoch:586 Training Loss: 133.59578499842172 Testing Loss: 128.845341796875\n",
      "Training Epoch:587 Training Loss: 133.62404040404041 Testing Loss: 128.8720703125\n",
      "Training Epoch:588 Training Loss: 133.59942437065973 Testing Loss: 128.637529296875\n",
      "Training Epoch:589 Training Loss: 133.6153526968908 Testing Loss: 128.505283203125\n",
      "Training Epoch:590 Training Loss: 133.60199968434344 Testing Loss: 128.847705078125\n",
      "Training Epoch:591 Training Loss: 133.61067155934344 Testing Loss: 128.764248046875\n",
      "Training Epoch:592 Training Loss: 133.5992168264678 Testing Loss: 128.3204296875\n",
      "Training Epoch:593 Training Loss: 133.58712836371527 Testing Loss: 129.634541015625\n",
      "Training Epoch:594 Training Loss: 133.60747203480113 Testing Loss: 128.94068359375\n",
      "Training Epoch:595 Training Loss: 133.58953342013888 Testing Loss: 128.610703125\n",
      "Training Epoch:596 Training Loss: 133.60064408735795 Testing Loss: 129.342216796875\n",
      "Training Epoch:597 Training Loss: 133.60224135890152 Testing Loss: 129.155126953125\n",
      "Training Epoch:598 Training Loss: 133.5776755346433 Testing Loss: 129.24369140625\n",
      "Training Epoch:599 Training Loss: 133.59208259351325 Testing Loss: 128.31560546875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch:600 Training Loss: 133.59389268663193 Testing Loss: 128.7295703125\n",
      "Training Epoch:601 Training Loss: 133.5841981830019 Testing Loss: 128.664306640625\n",
      "Training Epoch:602 Training Loss: 133.58100753630052 Testing Loss: 129.21396484375\n",
      "Training Epoch:603 Training Loss: 133.5714000848327 Testing Loss: 129.317412109375\n",
      "Training Epoch:604 Training Loss: 133.58670129024622 Testing Loss: 129.71873046875\n",
      "Training Epoch:605 Training Loss: 133.58430762705177 Testing Loss: 128.317314453125\n",
      "Training Epoch:606 Training Loss: 133.582594154435 Testing Loss: 128.1290625\n",
      "Training Epoch:607 Training Loss: 133.58458249487057 Testing Loss: 128.399921875\n",
      "Training Epoch:608 Training Loss: 133.5726817984533 Testing Loss: 129.095625\n",
      "Training Epoch:609 Training Loss: 133.57664156210544 Testing Loss: 128.23494140625\n",
      "Training Epoch:610 Training Loss: 133.5828082583649 Testing Loss: 129.06365234375\n",
      "Training Epoch:611 Training Loss: 133.58869278724748 Testing Loss: 128.635498046875\n",
      "Training Epoch:612 Training Loss: 133.54669527304293 Testing Loss: 128.812060546875\n",
      "Training Epoch:613 Training Loss: 133.56753807607322 Testing Loss: 128.361298828125\n",
      "Training Epoch:614 Training Loss: 133.56812879774304 Testing Loss: 128.7834375\n",
      "Training Epoch:615 Training Loss: 133.55304144965277 Testing Loss: 129.023212890625\n",
      "Training Epoch:616 Training Loss: 133.553502505524 Testing Loss: 129.271865234375\n",
      "Training Epoch:617 Training Loss: 133.55618603022413 Testing Loss: 128.809921875\n",
      "Training Epoch:618 Training Loss: 133.5640407986111 Testing Loss: 128.594697265625\n",
      "Training Epoch:619 Training Loss: 133.530618440262 Testing Loss: 128.733359375\n",
      "Training Epoch:620 Training Loss: 133.54927842881943 Testing Loss: 128.94712890625\n",
      "Training Epoch:621 Training Loss: 133.53467023753157 Testing Loss: 128.9060546875\n",
      "Training Epoch:622 Training Loss: 133.53496744791667 Testing Loss: 128.755859375\n",
      "Training Epoch:623 Training Loss: 133.5494861209754 Testing Loss: 129.206943359375\n",
      "Training Epoch:624 Training Loss: 133.55513938210228 Testing Loss: 128.8880859375\n",
      "Training Epoch:625 Training Loss: 133.55565143623738 Testing Loss: 128.69083984375\n",
      "Training Epoch:626 Training Loss: 133.54870876736112 Testing Loss: 128.95279296875\n",
      "Training Epoch:627 Training Loss: 133.52596837515782 Testing Loss: 128.74326171875\n",
      "Training Epoch:628 Training Loss: 133.506298828125 Testing Loss: 128.858388671875\n",
      "Training Epoch:629 Training Loss: 133.5278755819918 Testing Loss: 128.82046875\n",
      "Training Epoch:630 Training Loss: 133.5360192254577 Testing Loss: 128.33353515625\n",
      "Training Epoch:631 Training Loss: 133.4975842901673 Testing Loss: 129.6725\n",
      "Training Epoch:632 Training Loss: 133.51361811474115 Testing Loss: 128.54638671875\n",
      "Training Epoch:633 Training Loss: 133.51678824376577 Testing Loss: 128.8582421875\n",
      "Training Epoch:634 Training Loss: 133.51754621409407 Testing Loss: 128.853544921875\n",
      "Training Epoch:635 Training Loss: 133.51881993765784 Testing Loss: 128.78228515625\n",
      "Training Epoch:636 Training Loss: 133.524033055161 Testing Loss: 128.677568359375\n",
      "Training Epoch:637 Training Loss: 133.5128673453283 Testing Loss: 129.4369140625\n",
      "Training Epoch:638 Training Loss: 133.51440143623736 Testing Loss: 129.033193359375\n",
      "Training Epoch:639 Training Loss: 133.50773610124685 Testing Loss: 129.178662109375\n",
      "Training Epoch:640 Training Loss: 133.4882907690183 Testing Loss: 128.2631640625\n",
      "Training Epoch:641 Training Loss: 133.49918215356692 Testing Loss: 129.277431640625\n",
      "Training Epoch:642 Training Loss: 133.50199080650253 Testing Loss: 128.477646484375\n",
      "Training Epoch:643 Training Loss: 133.5058897076231 Testing Loss: 128.75587890625\n",
      "Training Epoch:644 Training Loss: 133.4995759844539 Testing Loss: 128.58408203125\n",
      "Training Epoch:645 Training Loss: 133.5219982047033 Testing Loss: 128.567333984375\n",
      "Training Epoch:646 Training Loss: 133.47474929963698 Testing Loss: 128.896435546875\n",
      "Training Epoch:647 Training Loss: 133.49701921559344 Testing Loss: 128.94193359375\n",
      "Training Epoch:648 Training Loss: 133.47643751972853 Testing Loss: 128.881318359375\n",
      "Training Epoch:649 Training Loss: 133.4937674597538 Testing Loss: 128.639677734375\n",
      "Training Epoch:650 Training Loss: 133.48278108230744 Testing Loss: 129.429443359375\n",
      "Training Epoch:651 Training Loss: 133.49013154000946 Testing Loss: 129.113603515625\n",
      "Training Epoch:652 Training Loss: 133.46650864109847 Testing Loss: 129.26619140625\n",
      "Training Epoch:653 Training Loss: 133.48476700599747 Testing Loss: 129.00166015625\n",
      "Training Epoch:654 Training Loss: 133.4932955038668 Testing Loss: 129.228603515625\n",
      "Training Epoch:655 Training Loss: 133.45040857796718 Testing Loss: 129.44146484375\n",
      "Training Epoch:656 Training Loss: 133.46809111624054 Testing Loss: 128.8265234375\n",
      "Training Epoch:657 Training Loss: 133.4574887054135 Testing Loss: 129.18357421875\n",
      "Training Epoch:658 Training Loss: 133.47787272135417 Testing Loss: 128.71064453125\n",
      "Training Epoch:659 Training Loss: 133.48085079308711 Testing Loss: 129.152861328125\n",
      "Training Epoch:660 Training Loss: 133.4484238873106 Testing Loss: 129.093203125\n",
      "Training Epoch:661 Training Loss: 133.45631377249052 Testing Loss: 128.94099609375\n",
      "Training Epoch:662 Training Loss: 133.45995447640468 Testing Loss: 128.5995703125\n",
      "Training Epoch:663 Training Loss: 133.47750162760417 Testing Loss: 129.1269921875\n",
      "Training Epoch:664 Training Loss: 133.4852685546875 Testing Loss: 128.9382421875\n",
      "Training Epoch:665 Training Loss: 133.43434792258523 Testing Loss: 129.0037109375\n",
      "Training Epoch:666 Training Loss: 133.44183815696022 Testing Loss: 128.96333984375\n",
      "Training Epoch:667 Training Loss: 133.44116644965277 Testing Loss: 128.576796875\n",
      "Training Epoch:668 Training Loss: 133.44291114267676 Testing Loss: 128.59873046875\n",
      "Training Epoch:669 Training Loss: 133.44611979166666 Testing Loss: 128.94845703125\n",
      "Training Epoch:670 Training Loss: 133.43905046559343 Testing Loss: 129.021044921875\n",
      "Training Epoch:671 Training Loss: 133.43100013809973 Testing Loss: 128.8235546875\n",
      "Training Epoch:672 Training Loss: 133.44367044468117 Testing Loss: 128.7123828125\n",
      "Training Epoch:673 Training Loss: 133.4199600497159 Testing Loss: 128.71478515625\n",
      "Training Epoch:674 Training Loss: 133.44110854640152 Testing Loss: 128.679306640625\n",
      "Training Epoch:675 Training Loss: 133.41555501302082 Testing Loss: 128.8145703125\n",
      "Training Epoch:676 Training Loss: 133.41313496291036 Testing Loss: 129.0025\n",
      "Training Epoch:677 Training Loss: 133.4188797644413 Testing Loss: 129.0415234375\n",
      "Training Epoch:678 Training Loss: 133.4353010574495 Testing Loss: 128.88240234375\n",
      "Training Epoch:679 Training Loss: 133.44334413470645 Testing Loss: 128.999990234375\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 100000):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample the images from the latent space z, sample range between $-4.0 \\leqslant z_1 \\leqslant 4.0$ and $-4.0 \\leqslant z_2 \\leqslant 4.0$. Image are sampled at steps of 0.2 in both directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    z = torch.randn(1600, 2).cuda()\n",
    "    count_x=-4.0\n",
    "    count_y=-4.0\n",
    "    x=0\n",
    "    y=0\n",
    "    for i in range (0,40):\n",
    "        for j in range (0,40):\n",
    "            z[x,0]=count_x\n",
    "            z[x,1]=count_y\n",
    "            x=x+1\n",
    "            count_y=count_y+0.2\n",
    "        y=y+1\n",
    "        count_x=count_x+0.2\n",
    "        count_y=-4.0\n",
    "        \n",
    "    sample = vae.decoder(z).cuda()\n",
    "    \n",
    "    save_image(sample.view(1600, 1, 100, 100), 'sample_z_space_' +str(epoch)+'.png',nrow=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    z = torch.randn(100, 2).cuda()    \n",
    "    sample = vae.decoder(z).cuda()\n",
    "    save_image(sample.view(100, 1, 100, 100), 'sample_random_' +str(epoch)+'.png',nrow=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
