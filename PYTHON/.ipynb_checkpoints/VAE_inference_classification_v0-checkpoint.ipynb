{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.datasets as dset\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO, TraceEnum_ELBO, config_enumerate\n",
    "from pyro.optim import Adam,Adagrad\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image \n",
    "\n",
    "from MiraBest import MiraBest\n",
    "from FRDEEP import FRDEEPF\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.enable_validation(True)\n",
    "pyro.distributions.enable_validation(False)\n",
    "pyro.set_rng_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoke_test = 'CI' in os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 34 * 34, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # conv1 output width: input_width - (kernel_size - 1) => 150 - (5-1) = 146\n",
    "        # pool 1 output width: int(input_width/2) => 73\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # conv2 output width: input_width - (kernel_size - 1) => 73 - (5-1) = 69\n",
    "        # pool 2 output width: int(input_width/2) => 34\n",
    "        x = self.pool(F.relu(self.conv2(x)))  \n",
    "        x = x.view(-1, 16 * 34 * 34)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_procedure():\n",
    "    \n",
    "    transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize([0.5],[0.5])])\n",
    "\n",
    "    trainset = FRDEEPF(root='./FIRST_data', train=True, download=True, transform=transform)  \n",
    "    batch_size_train = 2\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size_train, shuffle=True, num_workers=2)\n",
    "\n",
    "    testset = FRDEEPF(root='./FIRST_data', train=False, download=True, transform=transform) \n",
    "    batch_size_test = 2\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size_test, shuffle=True, num_workers=2)\n",
    "\n",
    "    classes = ('FRI', 'FRII')\n",
    "\n",
    "    # get some random training images\n",
    "    dataiter = iter(trainloader)\n",
    "    images, labels = dataiter.next()\n",
    "\n",
    "    net = Net()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer2 = optim.Adagrad(net.parameters(), lr=0.01)\n",
    "\n",
    "    nepoch = 50  # number of epochs\n",
    "    print_num = 50\n",
    "    for epoch in range(nepoch):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer2.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer2.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % print_num == (print_num-1):    # print every 50 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / print_num))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "    dataiter = iter(testloader)\n",
    "    images, labels = dataiter.next()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 50 test images: %d %%' % (100 * correct / total))\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#--------------------Download FIRST Images using either MiraBest or FRDEEP------------------------------------------\n",
    "def dataloader_first():\n",
    "    transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize([0.5],[0.5])])\n",
    "\n",
    "    trainset = MiraBest(root='./FIRST_data', train=True, download=True, transform=transform)  \n",
    "    trainloader = torch.utils.data.DataLoader(trainset, shuffle=True, num_workers=2, batch_size=len(trainset))\n",
    "    \n",
    "    classes = ('FRI', 'FRII') #First class if FR1 and second class is FR2\n",
    "    \n",
    "    array_train= next(iter(trainloader))[0].numpy() # Training Datasets is loaded in numpy array\n",
    "    array_label= next(iter(trainloader))[1].numpy()\n",
    "    \n",
    "    augmented_data=np.zeros((len(array_train)*36,1,100,100))\n",
    "    \n",
    "    augmented_data_label = np.zeros((len(array_train)*36,1))\n",
    "    \n",
    "    count=0\n",
    "    \n",
    "    for j in range(0,len(array_train)):\n",
    "        image_object=Image.fromarray(array_train[j,0,:,:])\n",
    "        for i in range(0,36):\n",
    "            rotated=image_object.rotate(i*10)\n",
    "            imgarr = np.array(rotated)\n",
    "            temp_img_array=imgarr[25:125,25:125]\n",
    "            augmented_data[count,0,:,:]=temp_img_array\n",
    "            augmented_data_label[count,:]=array_label[j]\n",
    "            count+=1\n",
    "    augmented_data=(augmented_data-np.min(augmented_data))/(np.max(augmented_data)-np.min(augmented_data))\n",
    "    \n",
    "    X=augmented_data\n",
    "    Y=augmented_data_label\n",
    "    \n",
    "    X_random_mix=np.take(X,np.random.RandomState(seed=42).permutation(X.shape[0]),axis=0,out=X)\n",
    "    Y_random_mix=np.take(Y,np.random.RandomState(seed=42).permutation(Y.shape[0]),axis=0,out=Y)\n",
    "    \n",
    "    tensor_x = torch.stack([torch.Tensor(i) for i in X_random_mix])\n",
    "    tensor_y = torch.stack([torch.Tensor(i) for i in Y_random_mix])\n",
    "    \n",
    "    first_augmented_dataset = torch.utils.data.TensorDataset(tensor_x,tensor_y)\n",
    "    \n",
    "    first_dataloader = torch.utils.data.DataLoader(first_augmented_dataset,batch_size=100, shuffle=True) # create your dataloader\n",
    "    \n",
    "    #--------------------------------------Add Section for Test data------------------------------------\n",
    "    \n",
    "    # Cropping of the Testing Images to 100 by 100 pixels\n",
    "    \n",
    "    \n",
    "    testset = MiraBest(root='./FIRST_data', train=False, download=True, transform=transform) \n",
    "    \n",
    "    testloader = torch.utils.data.DataLoader(testset, shuffle=True, num_workers=2, batch_size=len(testset))\n",
    "\n",
    "    array_test= next(iter(testloader))[0].numpy()\n",
    "    \n",
    "    test_labels = next(iter(testloader))[1].numpy()\n",
    "    \n",
    "    test_data_reduced=np.zeros((len(array_test),1,100,100))\n",
    "    test_data_label = np.zeros((len(array_test),1))\n",
    "    \n",
    "    for k in range (0,len(array_test)):\n",
    "        test_data_reduced[k][0][:][:] = array_test[k][0][25:125,25:125]\n",
    "        test_data_label[k,:]=test_labels[k]\n",
    "    \n",
    "    test_data_reduced=(test_data_reduced-np.min(test_data_reduced))/(np.max(test_data_reduced)-np.min(test_data_reduced))\n",
    "    \n",
    "    \n",
    "    \n",
    "    tensor_test = torch.stack([torch.Tensor(i) for i in test_data_reduced])\n",
    "    tensor_test_label = torch.stack([torch.Tensor(i) for i in test_data_label])\n",
    "    \n",
    "    first_augmented_dataset_test = torch.utils.data.TensorDataset(tensor_test,tensor_test_label) # create your datset\n",
    "    \n",
    "    first_dataloader_test = torch.utils.data.DataLoader(first_augmented_dataset_test,batch_size=50, shuffle=True) # create your dataloader\n",
    "    \n",
    "    return first_dataloader,first_dataloader_test\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#-----------------------------------Encoder Z Network- Encodes the images to the z latent space --------------------------\n",
    "class EncoderZ(nn.Module):\n",
    "    #def __init__(self, z_dim, hidden_dim):\n",
    "    def __init__(self, x_dim, y_dim, h_dim1, z_dim):\n",
    "        super(EncoderZ, self).__init__()\n",
    "        self.fc1 = nn.Linear(x_dim+y_dim, h_dim1) # x_dim=10000 + y_dim=2 to h_dim1=500\n",
    "        self.fc21 = nn.Linear(h_dim1, z_dim) #h_dim5=500 to z_dim=2\n",
    "        self.fc22 = nn.Linear(h_dim1, z_dim) #h_dim5=500 to z_dim=2\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, x_y_2):\n",
    "        [x,y]=x_y_2\n",
    "        \n",
    "        x = x.reshape(-1, 10000) \n",
    "        y = y.reshape(-1, 2) \n",
    "        \n",
    "        x_y_1 = torch.cat((x,y), dim=1) \n",
    "        x_y_1 = x_y_1.view(x_y_1.size(0), -1)\n",
    "        \n",
    "        slope_param=0.0001\n",
    "        \n",
    "        # then compute the hidden units\n",
    "        # We use fully connected layers\n",
    "        hidden = self.softplus(self.fc1(x_y_1))\n",
    "        \n",
    "        z_loc = self.fc21(hidden)\n",
    "        z_scale = torch.exp(self.fc22(hidden)) # mu, log_var\n",
    "        \n",
    "        \n",
    "        return z_loc, z_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#-----------------------------------Encoder Z Network- Encodes the images to the z latent space --------------------------\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, x_dim, y_dim, h_dim1, z_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        # setup the two linear transformations used\n",
    "        self.fc3 = nn.Linear(z_dim+y_dim, h_dim1) #z_dim=2 to h_dim5=500\n",
    "        self.fc4 = nn.Linear(h_dim1, x_dim)  #h_dim1=4096 to x_dim=10000\n",
    "        \n",
    "        self.softplus = nn.Softplus()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,z_y_2):\n",
    "        # define the forward computation on the latent z\n",
    "        # first compute the hidden units\n",
    "        \n",
    "        [z,y]=z_y_2\n",
    "        \n",
    "        z = z.reshape(-1, 2) #@David Change this to reshape if something fucks up\n",
    "        y = y.reshape(-1, 2)\n",
    "        z_y_1 = torch.cat((z,y), dim=1)\n",
    "        z_y_1 = z_y_1.view(z_y_1.size(0), -1)\n",
    "        \n",
    "        slope_param=0.0001\n",
    "        hidden = F.leaky_relu(self.fc3(z_y_1),slope_param)\n",
    "        loc_img = self.sigmoid(self.fc4(hidden))\n",
    "        return loc_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    # by default our latent space is 50-dimensional\n",
    "    # and we use 400 hidden units\n",
    "    def __init__(self, x_dim=10000, y_dim=2, h_dim1=500, z_dim=2, use_cuda=True):\n",
    "        super(VAE, self).__init__()\n",
    "    \n",
    "        # create the encoder and decoder networks\n",
    "        # a split in the final layer's size is used for multiple outputs\n",
    "        # and potentially applying separate activation functions on them\n",
    "        # e.g. in this network the final output is of size [z_dim,z_dim]\n",
    "        # to produce loc and scale, and apply different activations [None,Exp] on them\n",
    "              \n",
    "        self.encoder_z = EncoderZ(x_dim, y_dim, h_dim1, z_dim)\n",
    "        \n",
    "        self.decoder = Decoder(x_dim, y_dim, h_dim1, z_dim)\n",
    "\n",
    "        if use_cuda:\n",
    "            # calling cuda() here will put all the parameters of\n",
    "            # the encoder and decoder networks into gpu memory\n",
    "            self.cuda()\n",
    "            \n",
    "            \n",
    "        self.use_cuda = use_cuda\n",
    "        self.z_dim = z_dim\n",
    "        self.output_size = y_dim\n",
    "        \n",
    "        \n",
    "    # define the model p(x|z)p(z)\n",
    "    def model(self, xs, ys):\n",
    "        # register this pytorch module and all of its sub-modules with pyro\n",
    "        pyro.module(\"ss_vae\", self)\n",
    "        batch_size = xs.size(0)\n",
    "\n",
    "        # inform Pyro that the variables in the batch of xs, ys are conditionally independent\n",
    "        with pyro.plate(\"data\"):\n",
    "\n",
    "            # sample the handwriting style from the constant prior distribution\n",
    "            prior_loc = xs.new_zeros([batch_size, self.z_dim])\n",
    "            prior_scale = xs.new_ones([batch_size, self.z_dim])\n",
    "            zs = pyro.sample(\"z\", dist.Normal(prior_loc, prior_scale).to_event(1))\n",
    "\n",
    "            # if the label y (which digit to write) is supervised, sample from the\n",
    "            # constant prior, otherwise, observe the value (i.e. score it against the constant prior)\n",
    "            alpha_prior = xs.new_ones([batch_size, self.output_size]) / (1.0 * self.output_size)\n",
    "            ys = pyro.sample(\"y\", dist.OneHotCategorical(alpha_prior), obs=ys)\n",
    "            \n",
    "            # finally, score the image (x) using the handwriting style (z) and\n",
    "            # the class label y (which digit to write) against the\n",
    "            # parametrized distribution p(x|y,z) = bernoulli(decoder(y,z))\n",
    "            # where `decoder` is a neural network\n",
    "            loc = self.decoder.forward([zs, ys])\n",
    "            pyro.sample(\"x\", dist.Bernoulli(loc).to_event(1), obs=xs)\n",
    "            \n",
    "    def guide(self, xs, ys):\n",
    "        with pyro.plate(\"data\"):\n",
    "           # if the class label (the digit) is not supervised, sample\n",
    "           # (and score) the digit with the variational distribution\n",
    "           # q(y|x) = categorical(alpha(x))\n",
    "           \n",
    "            #-------------------REMOVED THIS PART FOR THE CLASSIFIER ASSUME ALL DATA ARE LABELLED---------\n",
    "\n",
    "           # sample (and score) the latent handwriting-style with the variational\n",
    "           # distribution q(z|x,y) = normal(loc(x,y),scale(x,y))\n",
    "           loc, scale = self.encoder_z.forward([xs, ys])\n",
    "           pyro.sample(\"z\", dist.Normal(loc, scale).to_event(1))\n",
    "\n",
    "    # define a helper function for reconstructing images\n",
    "    def reconstruct_img(self, xs, ys):\n",
    "        # encode image x\n",
    "        z_loc, z_scale = self.encoder_z.forward([xs,ys])\n",
    "        # sample in latent space\n",
    "        z = dist.Normal(z_loc, z_scale).sample()\n",
    "        # decode the image (note we don't sample in image space)\n",
    "        loc_img = self.decoder.forward([zs,ys])\n",
    "        \n",
    "        return loc_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def train(svi, train_loader, use_cuda=True):\n",
    "    # initialize loss accumulator\n",
    "    epoch_loss = 0.\n",
    "    # do a training epoch over each mini-batch x returned\n",
    "    # by the data loader\n",
    "    for x, y in train_loader:\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "        # do ELBO gradient and accumulate loss\n",
    "        labels_y = torch.tensor(np.zeros((y.shape[0],2)))\n",
    "        y_2=torch.Tensor.cpu(y.reshape(1,y.size()[0])[0]).numpy().astype(int)  \n",
    "        labels_y=np.eye(2)[y_2]\n",
    "        labels_y = torch.from_numpy(labels_y)   \n",
    "         \n",
    "        epoch_loss += svi.step(x.reshape(-1,10000),labels_y.cuda().float())\n",
    "\n",
    "    # return epoch loss\n",
    "    normalizer_train = len(train_loader.dataset)\n",
    "    total_epoch_loss_train = epoch_loss / normalizer_train\n",
    "    return total_epoch_loss_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(svi, test_loader, use_cuda=True):\n",
    "    # initialize loss accumulator\n",
    "    test_loss = 0.\n",
    "    # compute the loss over the entire test set\n",
    "    for x,y in test_loader:\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "        # compute ELBO estimate and accumulate loss\n",
    "        test_loss += svi.evaluate_loss(x) #Data entry point <---------------------------------Data Entry Point\n",
    "    normalizer_test = len(test_loader.dataset)\n",
    "    total_epoch_loss_test = test_loss / normalizer_test\n",
    "    return total_epoch_loss_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def image_sample_plotter(epoch):\n",
    "    with torch.no_grad():\n",
    "        count = 0\n",
    "        z_fr1 = z_fr2 = torch.randn(100, 2)\n",
    "        labels_y1 = torch.tensor(np.zeros((100,2)))\n",
    "        labels_y2 = torch.tensor(np.zeros((100,2)))\n",
    "        for i in range (0,10):\n",
    "            for j in range (0,10):\n",
    "                z_fr1[count,0] = z_fr2[count,0] = np.random.uniform(-1,1)\n",
    "                z_fr1[count,1] = z_fr2[count,1] = np.random.uniform(-1,1)\n",
    "                labels_y1[count,0] = 1\n",
    "                labels_y2[count,1] = 1\n",
    "                count = count +1 \n",
    "        \n",
    "        sample1 = vae.decoder([z_fr1.cuda(),labels_y1.cuda().float()])\n",
    "    \n",
    "        save_image(sample1.view(100, 1, 100, 100), 'fr1_sample_2_z_space_' +str(epoch)+'.png',nrow=10)\n",
    "    \n",
    "        sample2 = vae.decoder([z_fr2.cuda(),labels_y2.cuda().float()])\n",
    "\n",
    "        save_image(sample2.view(100, 1, 100, 100), 'fr2_sample_2_z_space_' +str(epoch)+'.png',nrow=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_checkpoint(epoch):\n",
    "    print(\"loading model from ...\")\n",
    "    vae.load_state_dict(torch.load('/raid/scratch/davidb/1_DEVELOPMENT/VAE_FIRST/models/file_conf00x4_semi_supervised_vae_FRDEEP_epoch_'+str(epoch)))\n",
    "    print(\"loading optimizer states from ...\")\n",
    "    optimizer.load('/raid/scratch/davidb/1_DEVELOPMENT/VAE_FIRST/models/file_conf00x4_semi_supervised_vae_FRDEEP_epoch_'+str(epoch)+'_opt')\n",
    "    print(\"done loading model and optimizer states.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch):\n",
    "    print(\"saving model to ...\")\n",
    "    torch.save(vae.state_dict(), '/raid/scratch/davidb/1_DEVELOPMENT/VAE_FIRST/models/file_conf00x4_semi_supervised_vae_FRDEEP_epoch_'+str(epoch))\n",
    "    print(\"saving optimizer states...\")\n",
    "    optimizer.save('/raid/scratch/davidb/1_DEVELOPMENT/VAE_FIRST/models/file_conf00x4_semi_supervised_vae_FRDEEP_epoch_'+str(epoch)+'_opt')\n",
    "    print(\"done saving model and optimizer checkpoints to disk.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Run options\n",
    "LEARNING_RATE = 1.0e-3\n",
    "USE_CUDA = True\n",
    "\n",
    "# Run only for a single iteration for testing\n",
    "NUM_EPOCHS = 20000\n",
    "TEST_FREQUENCY = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "results_array=np.zeros((1,2))\n",
    "\n",
    "results_array_temp=np.zeros((1,2))\n",
    "\n",
    "results_array_test=np.zeros((1,2))\n",
    "\n",
    "results_array_temp_test=np.zeros((1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_loader,test_loader = dataloader_first()\n",
    "# clear param store\n",
    "pyro.clear_param_store()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# setup the VAE\n",
    "vae = VAE(use_cuda=USE_CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# setup the optimizer\n",
    "adagrad_params = {\"lr\": 0.001}\n",
    "optimizer = Adagrad(adagrad_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "svi = SVI(vae.model, vae.guide, optimizer, loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [epoch 000]  average testing loss: 236.2094\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "saving model to ...\n",
      "saving optimizer states...\n",
      "done saving model and optimizer checkpoints to disk.\n",
      "[epoch 000]  average training loss: 194.3382\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-177-baffa0191e68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtotal_epoch_loss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mUSE_CUDA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtrain_elbo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtotal_epoch_loss_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-f3800c47aa58>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(svi, train_loader, use_cuda)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# do a training epoch over each mini-batch x returned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# by the data loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;31m# if on GPU put mini-batch into CUDA memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda-python-3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda-python-3.6/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/anaconda-python-3.6/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda-python-3.6/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda-python-3.6/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_elbo = []\n",
    "test_elbo = []\n",
    "# training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_epoch_loss_train = train(svi, train_loader, use_cuda=USE_CUDA)\n",
    "    \n",
    "    train_elbo.append(-total_epoch_loss_train)\n",
    "    \n",
    "    \n",
    "    if epoch%100 == 0:\n",
    "        # -------------------------------------Do testing for each epoch here-----------------------------------------\n",
    "        # initialize loss accumulator\n",
    "        test_loss = 0.\n",
    "        # compute the loss over the entire test set\n",
    "        for x_test,y_test in test_loader:\n",
    "\n",
    "            x_test = x_test.cuda()\n",
    "            y_test = y_test.cuda()\n",
    "            # compute ELBO estimate and accumulate loss\n",
    "            labels_y_test = torch.tensor(np.zeros((y_test.shape[0],2)))\n",
    "            y_test_2=torch.Tensor.cpu(y_test.reshape(1,y_test.size()[0])[0]).numpy().astype(int)  \n",
    "            labels_y_test=np.eye(2)[y_test_2]\n",
    "            labels_y_test = torch.from_numpy(labels_y_test)\n",
    "        \n",
    "            test_loss += svi.evaluate_loss(x_test.reshape(-1,10000),labels_y_test.cuda().float()) \n",
    "            \n",
    "            \n",
    "        normalizer_test = len(test_loader.dataset)\n",
    "        total_epoch_loss_test = test_loss / normalizer_test\n",
    "        \n",
    "      \n",
    "        \n",
    "        image_sample_plotter(epoch)\n",
    "        \n",
    "        results_array_temp_test[0,:][0] = epoch\n",
    "        results_array_temp_test[0,:][1] = total_epoch_loss_test\n",
    "        results_array_test=np.vstack((results_array_test,results_array_temp_test))\n",
    "        print(\"    [epoch %03d]  average testing loss: %.4f\" % (epoch, total_epoch_loss_test))\n",
    "        \n",
    "        \n",
    "        \n",
    "        #-------INSERT PROCEDURE FOR GENERATION AND TESTING OF CLASSIFIER--------\n",
    "        z_fr1 = z_fr2 = torch.randn(100, 2)\n",
    "        labels_y1 = torch.tensor(np.zeros((100,2)))\n",
    "        labels_y2 = torch.tensor(np.zeros((100,2)))\n",
    "        \n",
    "        labels_y1[:,0] = 0\n",
    "        labels_y2[:,1] = 0\n",
    "        \n",
    "        labels_y1[:,1] = 1\n",
    "        labels_y2[:,0] = 1\n",
    "\n",
    "        sample_fr1 = vae.decoder([z_fr1.cuda(),labels_y1.cuda().float()])\n",
    "\n",
    "        img1=sample1.reshape(100,100,100).cpu().detach().numpy()\n",
    " \n",
    "\n",
    "        sample_fr2 = vae.decoder([z_fr2.cuda(),labels_y2.cuda().float()])\n",
    "\n",
    "        img2=sample2.reshape(100,100,100).cpu().detach().numpy()\n",
    "\n",
    "        classification_results = np.zeros((1,100))\n",
    "\n",
    "        i = 10\n",
    "\n",
    "        for i in range (0,100):\n",
    "            img_tensor = torch.tensor(np.zeros((1,1,150,150)))\n",
    "            img_tensor[0,0,25:125,25:125]=sample1[i].reshape(100,100)\n",
    "            outputs = net(img_tensor.float())\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            print(predicted)     \n",
    "        \n",
    "    # ----------------------\n",
    "    if epoch%500 == 0:\n",
    "        save_checkpoint(epoch)     \n",
    "    \n",
    "  # --------------------------------------------Plotting Mechanism-----------------------------------------------  \n",
    "    results_array_temp[0,:][0] = epoch\n",
    "    results_array_temp[0,:][1] = total_epoch_loss_train\n",
    "    results_array=np.vstack((results_array,results_array_temp))\n",
    "    \n",
    "    print(\"[epoch %03d]  average training loss: %.4f\" % (epoch, total_epoch_loss_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[1,    50] loss: 1.220\n",
      "[1,   100] loss: 0.752\n",
      "[1,   150] loss: 0.711\n",
      "[1,   200] loss: 0.705\n",
      "[1,   250] loss: 0.704\n",
      "[2,    50] loss: 0.737\n",
      "[2,   100] loss: 0.728\n",
      "[2,   150] loss: 0.685\n",
      "[2,   200] loss: 0.694\n",
      "[2,   250] loss: 0.619\n",
      "[3,    50] loss: 0.582\n",
      "[3,   100] loss: 0.567\n",
      "[3,   150] loss: 0.578\n",
      "[3,   200] loss: 0.551\n",
      "[3,   250] loss: 0.470\n",
      "[4,    50] loss: 0.459\n",
      "[4,   100] loss: 0.413\n",
      "[4,   150] loss: 0.536\n",
      "[4,   200] loss: 0.457\n",
      "[4,   250] loss: 0.480\n",
      "[5,    50] loss: 0.373\n",
      "[5,   100] loss: 0.376\n",
      "[5,   150] loss: 0.319\n",
      "[5,   200] loss: 0.403\n",
      "[5,   250] loss: 0.359\n",
      "[6,    50] loss: 0.306\n",
      "[6,   100] loss: 0.245\n",
      "[6,   150] loss: 0.313\n",
      "[6,   200] loss: 0.307\n",
      "[6,   250] loss: 0.288\n",
      "[7,    50] loss: 0.245\n",
      "[7,   100] loss: 0.244\n",
      "[7,   150] loss: 0.219\n",
      "[7,   200] loss: 0.283\n",
      "[7,   250] loss: 0.170\n",
      "[8,    50] loss: 0.204\n",
      "[8,   100] loss: 0.221\n",
      "[8,   150] loss: 0.153\n",
      "[8,   200] loss: 0.133\n",
      "[8,   250] loss: 0.236\n",
      "[9,    50] loss: 0.103\n",
      "[9,   100] loss: 0.155\n",
      "[9,   150] loss: 0.132\n",
      "[9,   200] loss: 0.224\n",
      "[9,   250] loss: 0.301\n",
      "[10,    50] loss: 0.142\n",
      "[10,   100] loss: 0.123\n",
      "[10,   150] loss: 0.186\n",
      "[10,   200] loss: 0.246\n",
      "[10,   250] loss: 0.121\n",
      "[11,    50] loss: 0.144\n",
      "[11,   100] loss: 0.128\n",
      "[11,   150] loss: 0.186\n",
      "[11,   200] loss: 0.122\n",
      "[11,   250] loss: 0.133\n",
      "[12,    50] loss: 0.127\n",
      "[12,   100] loss: 0.099\n",
      "[12,   150] loss: 0.187\n",
      "[12,   200] loss: 0.109\n",
      "[12,   250] loss: 0.189\n",
      "[13,    50] loss: 0.110\n",
      "[13,   100] loss: 0.099\n",
      "[13,   150] loss: 0.100\n",
      "[13,   200] loss: 0.158\n",
      "[13,   250] loss: 0.142\n",
      "[14,    50] loss: 0.084\n",
      "[14,   100] loss: 0.222\n",
      "[14,   150] loss: 0.095\n",
      "[14,   200] loss: 0.107\n",
      "[14,   250] loss: 0.066\n",
      "[15,    50] loss: 0.104\n",
      "[15,   100] loss: 0.134\n",
      "[15,   150] loss: 0.097\n",
      "[15,   200] loss: 0.094\n",
      "[15,   250] loss: 0.069\n",
      "[16,    50] loss: 0.089\n",
      "[16,   100] loss: 0.128\n",
      "[16,   150] loss: 0.073\n",
      "[16,   200] loss: 0.086\n",
      "[16,   250] loss: 0.090\n",
      "[17,    50] loss: 0.098\n",
      "[17,   100] loss: 0.073\n",
      "[17,   150] loss: 0.074\n",
      "[17,   200] loss: 0.075\n",
      "[17,   250] loss: 0.054\n",
      "[18,    50] loss: 0.079\n",
      "[18,   100] loss: 0.067\n",
      "[18,   150] loss: 0.053\n",
      "[18,   200] loss: 0.111\n",
      "[18,   250] loss: 0.058\n",
      "[19,    50] loss: 0.053\n",
      "[19,   100] loss: 0.074\n",
      "[19,   150] loss: 0.090\n",
      "[19,   200] loss: 0.062\n",
      "[19,   250] loss: 0.044\n",
      "[20,    50] loss: 0.102\n",
      "[20,   100] loss: 0.075\n",
      "[20,   150] loss: 0.041\n",
      "[20,   200] loss: 0.078\n",
      "[20,   250] loss: 0.105\n",
      "[21,    50] loss: 0.057\n",
      "[21,   100] loss: 0.060\n",
      "[21,   150] loss: 0.128\n",
      "[21,   200] loss: 0.100\n",
      "[21,   250] loss: 0.094\n",
      "[22,    50] loss: 0.053\n",
      "[22,   100] loss: 0.072\n",
      "[22,   150] loss: 0.089\n",
      "[22,   200] loss: 0.026\n",
      "[22,   250] loss: 0.069\n",
      "[23,    50] loss: 0.046\n",
      "[23,   100] loss: 0.071\n",
      "[23,   150] loss: 0.068\n",
      "[23,   200] loss: 0.034\n",
      "[23,   250] loss: 0.051\n",
      "[24,    50] loss: 0.024\n",
      "[24,   100] loss: 0.096\n",
      "[24,   150] loss: 0.058\n",
      "[24,   200] loss: 0.100\n",
      "[24,   250] loss: 0.041\n",
      "[25,    50] loss: 0.071\n",
      "[25,   100] loss: 0.032\n",
      "[25,   150] loss: 0.074\n",
      "[25,   200] loss: 0.049\n",
      "[25,   250] loss: 0.035\n",
      "[26,    50] loss: 0.050\n",
      "[26,   100] loss: 0.044\n",
      "[26,   150] loss: 0.050\n",
      "[26,   200] loss: 0.063\n",
      "[26,   250] loss: 0.037\n",
      "[27,    50] loss: 0.026\n",
      "[27,   100] loss: 0.043\n",
      "[27,   150] loss: 0.031\n",
      "[27,   200] loss: 0.088\n",
      "[27,   250] loss: 0.106\n",
      "[28,    50] loss: 0.061\n",
      "[28,   100] loss: 0.019\n",
      "[28,   150] loss: 0.038\n",
      "[28,   200] loss: 0.032\n",
      "[28,   250] loss: 0.088\n",
      "[29,    50] loss: 0.027\n",
      "[29,   100] loss: 0.071\n",
      "[29,   150] loss: 0.047\n",
      "[29,   200] loss: 0.043\n",
      "[29,   250] loss: 0.043\n",
      "[30,    50] loss: 0.062\n",
      "[30,   100] loss: 0.059\n",
      "[30,   150] loss: 0.034\n",
      "[30,   200] loss: 0.037\n",
      "[30,   250] loss: 0.041\n",
      "[31,    50] loss: 0.020\n",
      "[31,   100] loss: 0.033\n",
      "[31,   150] loss: 0.042\n",
      "[31,   200] loss: 0.033\n",
      "[31,   250] loss: 0.035\n",
      "[32,    50] loss: 0.023\n",
      "[32,   100] loss: 0.039\n",
      "[32,   150] loss: 0.043\n",
      "[32,   200] loss: 0.040\n",
      "[32,   250] loss: 0.040\n",
      "[33,    50] loss: 0.032\n",
      "[33,   100] loss: 0.023\n",
      "[33,   150] loss: 0.021\n",
      "[33,   200] loss: 0.063\n",
      "[33,   250] loss: 0.056\n",
      "[34,    50] loss: 0.037\n",
      "[34,   100] loss: 0.024\n",
      "[34,   150] loss: 0.017\n",
      "[34,   200] loss: 0.052\n",
      "[34,   250] loss: 0.058\n",
      "[35,    50] loss: 0.034\n",
      "[35,   100] loss: 0.014\n",
      "[35,   150] loss: 0.073\n",
      "[35,   200] loss: 0.056\n",
      "[35,   250] loss: 0.018\n",
      "[36,    50] loss: 0.026\n",
      "[36,   100] loss: 0.031\n",
      "[36,   150] loss: 0.030\n",
      "[36,   200] loss: 0.027\n",
      "[36,   250] loss: 0.036\n",
      "[37,    50] loss: 0.049\n",
      "[37,   100] loss: 0.015\n",
      "[37,   150] loss: 0.035\n",
      "[37,   200] loss: 0.041\n",
      "[37,   250] loss: 0.013\n",
      "[38,    50] loss: 0.016\n",
      "[38,   100] loss: 0.024\n",
      "[38,   150] loss: 0.060\n",
      "[38,   200] loss: 0.028\n",
      "[38,   250] loss: 0.013\n",
      "[39,    50] loss: 0.018\n",
      "[39,   100] loss: 0.033\n",
      "[39,   150] loss: 0.040\n",
      "[39,   200] loss: 0.013\n",
      "[39,   250] loss: 0.018\n",
      "[40,    50] loss: 0.013\n",
      "[40,   100] loss: 0.028\n",
      "[40,   150] loss: 0.018\n",
      "[40,   200] loss: 0.025\n",
      "[40,   250] loss: 0.012\n",
      "[41,    50] loss: 0.042\n",
      "[41,   100] loss: 0.023\n",
      "[41,   150] loss: 0.034\n",
      "[41,   200] loss: 0.034\n",
      "[41,   250] loss: 0.017\n",
      "[42,    50] loss: 0.017\n",
      "[42,   100] loss: 0.028\n",
      "[42,   150] loss: 0.072\n",
      "[42,   200] loss: 0.020\n",
      "[42,   250] loss: 0.031\n",
      "[43,    50] loss: 0.039\n",
      "[43,   100] loss: 0.018\n",
      "[43,   150] loss: 0.014\n",
      "[43,   200] loss: 0.021\n",
      "[43,   250] loss: 0.030\n",
      "[44,    50] loss: 0.016\n",
      "[44,   100] loss: 0.020\n",
      "[44,   150] loss: 0.025\n",
      "[44,   200] loss: 0.044\n",
      "[44,   250] loss: 0.017\n",
      "[45,    50] loss: 0.023\n",
      "[45,   100] loss: 0.035\n",
      "[45,   150] loss: 0.008\n",
      "[45,   200] loss: 0.045\n",
      "[45,   250] loss: 0.023\n",
      "[46,    50] loss: 0.013\n",
      "[46,   100] loss: 0.037\n",
      "[46,   150] loss: 0.023\n",
      "[46,   200] loss: 0.031\n",
      "[46,   250] loss: 0.022\n",
      "[47,    50] loss: 0.014\n",
      "[47,   100] loss: 0.028\n",
      "[47,   150] loss: 0.023\n",
      "[47,   200] loss: 0.020\n",
      "[47,   250] loss: 0.016\n",
      "[48,    50] loss: 0.014\n",
      "[48,   100] loss: 0.017\n",
      "[48,   150] loss: 0.011\n",
      "[48,   200] loss: 0.025\n",
      "[48,   250] loss: 0.035\n",
      "[49,    50] loss: 0.022\n",
      "[49,   100] loss: 0.014\n",
      "[49,   150] loss: 0.008\n",
      "[49,   200] loss: 0.034\n",
      "[49,   250] loss: 0.008\n",
      "[50,    50] loss: 0.030\n",
      "[50,   100] loss: 0.024\n",
      "[50,   150] loss: 0.028\n",
      "[50,   200] loss: 0.012\n",
      "[50,   250] loss: 0.019\n",
      "Finished Training\n",
      "Accuracy of the network on the 50 test images: 78 %\n"
     ]
    }
   ],
   "source": [
    "net = classification_procedure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_fr1 = z_fr2 = torch.randn(100, 2)\n",
    "labels_y1 = torch.tensor(np.zeros((100,2)))\n",
    "labels_y2 = torch.tensor(np.zeros((100,2)))\n",
    "        \n",
    "labels_y1[:,1] = 0\n",
    "labels_y2[:,1] = 0\n",
    "\n",
    "labels_y1[:,0] = 0\n",
    "labels_y2[:,0] = 0\n",
    "\n",
    "labels_y1[:,1] = 2\n",
    "labels_y2[:,0] = 2\n",
    "\n",
    "sample_fr1 = vae.decoder([z_fr1.cuda(),labels_y1.cuda().float()])\n",
    "img1=sample_fr1.reshape(100,100,100).cpu().detach().numpy()\n",
    " \n",
    "\n",
    "sample_fr2 = vae.decoder([z_fr2.cuda(),labels_y2.cuda().float()])\n",
    "img2=sample_fr2.reshape(100,100,100).cpu().detach().numpy()\n",
    "\n",
    "classification_results = np.zeros((1,100))\n",
    "\n",
    "#for i in range (0,100):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n",
      "tensor([0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fad88567518>"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 53\n",
    "img_tensor = torch.tensor(np.zeros((1,1,150,150)))\n",
    "img_tensor[0,0,25:125,25:125]=sample_fr2[i].reshape(100,100)\n",
    "outputs = net(img_tensor.float())\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print(predicted)\n",
    "\n",
    "img_tensor = torch.tensor(np.zeros((1,1,150,150)))\n",
    "img_tensor[0,0,25:125,25:125]=sample_fr1[i].reshape(100,100)\n",
    "outputs = net(img_tensor.float())\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print(predicted)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax1 = fig.add_subplot(2,2,1)\n",
    "ax1.imshow(sample_fr1[i].reshape(100,100).cpu().detach().numpy())\n",
    "ax2 = fig.add_subplot(2,2,2)\n",
    "ax2.imshow(sample_fr2[i].reshape(100,100).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fad9cf0f128>"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num = 85\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax1 = fig.add_subplot(2,2,1)\n",
    "ax1.imshow(img1[num])\n",
    "ax2 = fig.add_subplot(2,2,2)\n",
    "ax2.imshow(img2[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of backend CPU but got backend CUDA for argument #2 'other'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-280-3fe3478fbe50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimg_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimg_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m125\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m125\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0msample_fr2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of backend CPU but got backend CUDA for argument #2 'other'"
     ]
    }
   ],
   "source": [
    "img_tensor = torch.tensor(np.zeros((1,1,150,150)))\n",
    "img_tensor[0,0,25:125,25:125]==sample_fr2[i].reshape(100,100)\n",
    "outputs = net(img_tensor.float())\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax1 = fig.add_subplot(2,2,1)\n",
    "ax1.imshow(img1[i])\n",
    "ax2 = fig.add_subplot(2,2,2)\n",
    "ax2.imshow(img2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[1,    50] loss: 0.942\n",
      "[1,   100] loss: 0.767\n",
      "[1,   150] loss: 0.713\n",
      "[1,   200] loss: 0.695\n",
      "[1,   250] loss: 0.644\n",
      "Accuracy of the network on the 50 test images: 74 %\n",
      "[2,    50] loss: 0.596\n",
      "[2,   100] loss: 0.566\n",
      "[2,   150] loss: 0.622\n",
      "[2,   200] loss: 0.588\n",
      "[2,   250] loss: 0.562\n",
      "[3,    50] loss: 0.484\n",
      "[3,   100] loss: 0.570\n",
      "[3,   150] loss: 0.521\n",
      "[3,   200] loss: 0.488\n",
      "[3,   250] loss: 0.423\n",
      "[4,    50] loss: 0.362\n",
      "[4,   100] loss: 0.381\n",
      "[4,   150] loss: 0.457\n",
      "[4,   200] loss: 0.376\n",
      "[4,   250] loss: 0.435\n",
      "[5,    50] loss: 0.321\n",
      "[5,   100] loss: 0.324\n",
      "[5,   150] loss: 0.349\n",
      "[5,   200] loss: 0.368\n",
      "[5,   250] loss: 0.370\n",
      "[6,    50] loss: 0.196\n",
      "[6,   100] loss: 0.177\n",
      "[6,   150] loss: 0.454\n",
      "[6,   200] loss: 0.344\n",
      "[6,   250] loss: 0.208\n",
      "[7,    50] loss: 0.201\n",
      "[7,   100] loss: 0.175\n",
      "[7,   150] loss: 0.309\n",
      "[7,   200] loss: 0.232\n",
      "[7,   250] loss: 0.287\n",
      "[8,    50] loss: 0.215\n",
      "[8,   100] loss: 0.219\n",
      "[8,   150] loss: 0.139\n",
      "[8,   200] loss: 0.167\n",
      "[8,   250] loss: 0.303\n",
      "[9,    50] loss: 0.218\n",
      "[9,   100] loss: 0.250\n",
      "[9,   150] loss: 0.128\n",
      "[9,   200] loss: 0.127\n",
      "[9,   250] loss: 0.217\n",
      "[10,    50] loss: 0.148\n",
      "[10,   100] loss: 0.156\n",
      "[10,   150] loss: 0.184\n",
      "[10,   200] loss: 0.149\n",
      "[10,   250] loss: 0.163\n",
      "[11,    50] loss: 0.153\n",
      "[11,   100] loss: 0.137\n",
      "[11,   150] loss: 0.134\n",
      "[11,   200] loss: 0.140\n",
      "[11,   250] loss: 0.229\n",
      "Accuracy of the network on the 50 test images: 84 %\n",
      "[12,    50] loss: 0.052\n",
      "[12,   100] loss: 0.170\n",
      "[12,   150] loss: 0.174\n",
      "[12,   200] loss: 0.254\n",
      "[12,   250] loss: 0.065\n",
      "[13,    50] loss: 0.162\n",
      "[13,   100] loss: 0.101\n",
      "[13,   150] loss: 0.042\n",
      "[13,   200] loss: 0.167\n",
      "[13,   250] loss: 0.087\n",
      "[14,    50] loss: 0.163\n",
      "[14,   100] loss: 0.090\n",
      "[14,   150] loss: 0.090\n",
      "[14,   200] loss: 0.113\n",
      "[14,   250] loss: 0.101\n",
      "[15,    50] loss: 0.117\n",
      "[15,   100] loss: 0.144\n",
      "[15,   150] loss: 0.101\n",
      "[15,   200] loss: 0.060\n",
      "[15,   250] loss: 0.101\n",
      "[16,    50] loss: 0.051\n",
      "[16,   100] loss: 0.119\n",
      "[16,   150] loss: 0.109\n",
      "[16,   200] loss: 0.053\n",
      "[16,   250] loss: 0.082\n",
      "[17,    50] loss: 0.120\n",
      "[17,   100] loss: 0.068\n",
      "[17,   150] loss: 0.117\n",
      "[17,   200] loss: 0.079\n",
      "[17,   250] loss: 0.046\n",
      "[18,    50] loss: 0.069\n",
      "[18,   100] loss: 0.086\n",
      "[18,   150] loss: 0.134\n",
      "[18,   200] loss: 0.093\n",
      "[18,   250] loss: 0.099\n",
      "[19,    50] loss: 0.057\n",
      "[19,   100] loss: 0.105\n",
      "[19,   150] loss: 0.078\n",
      "[19,   200] loss: 0.067\n",
      "[19,   250] loss: 0.071\n",
      "[20,    50] loss: 0.104\n",
      "[20,   100] loss: 0.031\n",
      "[20,   150] loss: 0.054\n",
      "[20,   200] loss: 0.040\n",
      "[20,   250] loss: 0.114\n",
      "[21,    50] loss: 0.083\n",
      "[21,   100] loss: 0.060\n",
      "[21,   150] loss: 0.057\n",
      "[21,   200] loss: 0.077\n",
      "[21,   250] loss: 0.057\n",
      "Accuracy of the network on the 50 test images: 80 %\n",
      "[22,    50] loss: 0.065\n",
      "[22,   100] loss: 0.080\n",
      "[22,   150] loss: 0.077\n",
      "[22,   200] loss: 0.042\n",
      "[22,   250] loss: 0.072\n",
      "[23,    50] loss: 0.044\n",
      "[23,   100] loss: 0.052\n",
      "[23,   150] loss: 0.100\n",
      "[23,   200] loss: 0.080\n",
      "[23,   250] loss: 0.041\n",
      "[24,    50] loss: 0.084\n",
      "[24,   100] loss: 0.057\n",
      "[24,   150] loss: 0.095\n",
      "[24,   200] loss: 0.050\n",
      "[24,   250] loss: 0.017\n",
      "[25,    50] loss: 0.047\n",
      "[25,   100] loss: 0.049\n",
      "[25,   150] loss: 0.055\n",
      "[25,   200] loss: 0.020\n",
      "[25,   250] loss: 0.066\n",
      "[26,    50] loss: 0.052\n",
      "[26,   100] loss: 0.021\n",
      "[26,   150] loss: 0.057\n",
      "[26,   200] loss: 0.034\n",
      "[26,   250] loss: 0.036\n",
      "[27,    50] loss: 0.063\n",
      "[27,   100] loss: 0.038\n",
      "[27,   150] loss: 0.041\n",
      "[27,   200] loss: 0.034\n",
      "[27,   250] loss: 0.053\n",
      "[28,    50] loss: 0.026\n",
      "[28,   100] loss: 0.018\n",
      "[28,   150] loss: 0.077\n",
      "[28,   200] loss: 0.050\n",
      "[28,   250] loss: 0.040\n",
      "[29,    50] loss: 0.028\n",
      "[29,   100] loss: 0.014\n",
      "[29,   150] loss: 0.043\n",
      "[29,   200] loss: 0.051\n",
      "[29,   250] loss: 0.042\n",
      "[30,    50] loss: 0.058\n",
      "[30,   100] loss: 0.027\n",
      "[30,   150] loss: 0.021\n",
      "[30,   200] loss: 0.313\n",
      "[30,   250] loss: 0.061\n",
      "[31,    50] loss: 0.067\n",
      "[31,   100] loss: 0.039\n",
      "[31,   150] loss: 0.030\n",
      "[31,   200] loss: 0.092\n",
      "[31,   250] loss: 0.023\n",
      "Accuracy of the network on the 50 test images: 82 %\n",
      "[32,    50] loss: 0.044\n",
      "[32,   100] loss: 0.037\n",
      "[32,   150] loss: 0.036\n",
      "[32,   200] loss: 0.058\n",
      "[32,   250] loss: 0.030\n",
      "[33,    50] loss: 0.084\n",
      "[33,   100] loss: 0.015\n",
      "[33,   150] loss: 0.025\n",
      "[33,   200] loss: 0.026\n",
      "[33,   250] loss: 0.028\n",
      "[34,    50] loss: 0.041\n",
      "[34,   100] loss: 0.054\n",
      "[34,   150] loss: 0.011\n",
      "[34,   200] loss: 0.026\n",
      "[34,   250] loss: 0.046\n",
      "[35,    50] loss: 0.024\n",
      "[35,   100] loss: 0.023\n",
      "[35,   150] loss: 0.032\n",
      "[35,   200] loss: 0.030\n",
      "[35,   250] loss: 0.057\n",
      "[36,    50] loss: 0.043\n",
      "[36,   100] loss: 0.011\n",
      "[36,   150] loss: 0.045\n",
      "[36,   200] loss: 0.041\n",
      "[36,   250] loss: 0.017\n",
      "[37,    50] loss: 0.020\n",
      "[37,   100] loss: 0.009\n",
      "[37,   150] loss: 0.029\n",
      "[37,   200] loss: 0.063\n",
      "[37,   250] loss: 0.030\n",
      "[38,    50] loss: 0.038\n",
      "[38,   100] loss: 0.008\n",
      "[38,   150] loss: 0.022\n",
      "[38,   200] loss: 0.049\n",
      "[38,   250] loss: 0.037\n",
      "[39,    50] loss: 0.029\n",
      "[39,   100] loss: 0.011\n",
      "[39,   150] loss: 0.070\n",
      "[39,   200] loss: 0.020\n",
      "[39,   250] loss: 0.021\n",
      "[40,    50] loss: 0.014\n",
      "[40,   100] loss: 0.042\n",
      "[40,   150] loss: 0.040\n",
      "[40,   200] loss: 0.013\n",
      "[40,   250] loss: 0.018\n",
      "[41,    50] loss: 0.021\n",
      "[41,   100] loss: 0.008\n",
      "[41,   150] loss: 0.042\n",
      "[41,   200] loss: 0.048\n",
      "[41,   250] loss: 0.042\n",
      "Accuracy of the network on the 50 test images: 80 %\n",
      "[42,    50] loss: 0.009\n",
      "[42,   100] loss: 0.041\n",
      "[42,   150] loss: 0.083\n",
      "[42,   200] loss: 0.025\n",
      "[42,   250] loss: 0.005\n",
      "[43,    50] loss: 0.017\n",
      "[43,   100] loss: 0.032\n",
      "[43,   150] loss: 0.011\n",
      "[43,   200] loss: 0.072\n",
      "[43,   250] loss: 0.015\n",
      "[44,    50] loss: 0.025\n",
      "[44,   100] loss: 0.010\n",
      "[44,   150] loss: 0.024\n",
      "[44,   200] loss: 0.013\n",
      "[44,   250] loss: 0.026\n",
      "[45,    50] loss: 0.055\n",
      "[45,   100] loss: 0.027\n",
      "[45,   150] loss: 0.028\n",
      "[45,   200] loss: 0.017\n",
      "[45,   250] loss: 0.017\n",
      "[46,    50] loss: 0.030\n",
      "[46,   100] loss: 0.021\n",
      "[46,   150] loss: 0.036\n",
      "[46,   200] loss: 0.015\n",
      "[46,   250] loss: 0.028\n",
      "[47,    50] loss: 0.036\n",
      "[47,   100] loss: 0.035\n",
      "[47,   150] loss: 0.029\n",
      "[47,   200] loss: 0.042\n",
      "[47,   250] loss: 0.003\n",
      "[48,    50] loss: 0.050\n",
      "[48,   100] loss: 0.011\n",
      "[48,   150] loss: 0.013\n",
      "[48,   200] loss: 0.018\n",
      "[48,   250] loss: 0.028\n",
      "[49,    50] loss: 0.012\n",
      "[49,   100] loss: 0.009\n",
      "[49,   150] loss: 0.090\n",
      "[49,   200] loss: 0.043\n",
      "[49,   250] loss: 0.039\n",
      "[50,    50] loss: 0.012\n",
      "[50,   100] loss: 0.046\n",
      "[50,   150] loss: 0.029\n",
      "[50,   200] loss: 0.022\n",
      "[50,   250] loss: 0.025\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Insert Classification Procedure Here ------------------------\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize([0.5],[0.5])])\n",
    "\n",
    "trainset = FRDEEPF(root='./FIRST_data', train=True, download=True, transform=transform)  \n",
    "batch_size_train = 2\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size_train, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = FRDEEPF(root='./FIRST_data', train=False, download=True, transform=transform) \n",
    "batch_size_test = 2\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size_test, shuffle=True, num_workers=2)\n",
    "\n",
    "classes = ('FRI', 'FRII')\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "net = Net()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer2 = optim.Adagrad(net.parameters(), lr=0.01)\n",
    "\n",
    "nepoch = 50  # number of epochs\n",
    "print_num = 50\n",
    "for epoch in range(nepoch):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer2.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer2.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(str(running_loss/print_num))\n",
    "\n",
    "    if epoch%10==0:\n",
    "        dataiter = iter(testloader)\n",
    "        images, labels = dataiter.next()\n",
    "    \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in testloader:\n",
    "                images, labels = data\n",
    "                outputs = net(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print('Accuracy of the network on the 50 test images: %d %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
