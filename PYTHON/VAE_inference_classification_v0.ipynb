{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.datasets as dset\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO, TraceEnum_ELBO, config_enumerate\n",
    "from pyro.optim import Adam,Adagrad\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image \n",
    "\n",
    "from MiraBest import MiraBest\n",
    "from FRDEEP import FRDEEPF\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.enable_validation(True)\n",
    "pyro.distributions.enable_validation(False)\n",
    "pyro.set_rng_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoke_test = 'CI' in os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------Download FIRST Images using either MiraBest or FRDEEP------------------------------------------\n",
    "def dataloader_first():\n",
    "    transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize([0.5],[0.5])])\n",
    "\n",
    "    trainset = MiraBest(root='./FIRST_data', train=True, download=True, transform=transform)  \n",
    "    trainloader = torch.utils.data.DataLoader(trainset, shuffle=True, num_workers=2, batch_size=len(trainset))\n",
    "    \n",
    "    classes = ('FRI', 'FRII') #First class if FR1 and second class is FR2\n",
    "    \n",
    "    array_train= next(iter(trainloader))[0].numpy() # Training Datasets is loaded in numpy array\n",
    "    array_label= next(iter(trainloader))[1].numpy()\n",
    "    \n",
    "    augmented_data=np.zeros((len(array_train)*36,1,100,100))\n",
    "    \n",
    "    augmented_data_label = np.zeros((len(array_train)*36,1))\n",
    "    \n",
    "    count=0\n",
    "    \n",
    "    for j in range(0,len(array_train)):\n",
    "        image_object=Image.fromarray(array_train[j,0,:,:])\n",
    "        for i in range(0,36):\n",
    "            rotated=image_object.rotate(i*10)\n",
    "            imgarr = np.array(rotated)\n",
    "            temp_img_array=imgarr[25:125,25:125]\n",
    "            augmented_data[count,0,:,:]=temp_img_array\n",
    "            augmented_data_label[count,:]=array_label[j]\n",
    "            count+=1\n",
    "    augmented_data=(augmented_data-np.min(augmented_data))/(np.max(augmented_data)-np.min(augmented_data))\n",
    "    \n",
    "    X=augmented_data\n",
    "    Y=augmented_data_label\n",
    "    \n",
    "    X_random_mix=np.take(X,np.random.RandomState(seed=42).permutation(X.shape[0]),axis=0,out=X)\n",
    "    Y_random_mix=np.take(Y,np.random.RandomState(seed=42).permutation(Y.shape[0]),axis=0,out=Y)\n",
    "    \n",
    "    tensor_x = torch.stack([torch.Tensor(i) for i in X_random_mix])\n",
    "    tensor_y = torch.stack([torch.Tensor(i) for i in Y_random_mix])\n",
    "    \n",
    "    first_augmented_dataset = torch.utils.data.TensorDataset(tensor_x,tensor_y)\n",
    "    \n",
    "    first_dataloader = torch.utils.data.DataLoader(first_augmented_dataset,batch_size=100, shuffle=True) # create your dataloader\n",
    "    \n",
    "    #--------------------------------------Add Section for Test data------------------------------------\n",
    "    \n",
    "    # Cropping of the Testing Images to 100 by 100 pixels\n",
    "    \n",
    "    \n",
    "    testset = MiraBest(root='./FIRST_data', train=False, download=True, transform=transform) \n",
    "    \n",
    "    testloader = torch.utils.data.DataLoader(testset, shuffle=True, num_workers=2, batch_size=len(testset))\n",
    "\n",
    "    array_test= next(iter(testloader))[0].numpy()\n",
    "    \n",
    "    test_labels = next(iter(testloader))[1].numpy()\n",
    "    \n",
    "    test_data_reduced=np.zeros((len(array_test),1,100,100))\n",
    "    test_data_label = np.zeros((len(array_test),1))\n",
    "    \n",
    "    for k in range (0,len(array_test)):\n",
    "        test_data_reduced[k][0][:][:] = array_test[k][0][25:125,25:125]\n",
    "        test_data_label[k,:]=test_labels[k]\n",
    "    \n",
    "    test_data_reduced=(test_data_reduced-np.min(test_data_reduced))/(np.max(test_data_reduced)-np.min(test_data_reduced))\n",
    "    \n",
    "    \n",
    "    \n",
    "    tensor_test = torch.stack([torch.Tensor(i) for i in test_data_reduced])\n",
    "    tensor_test_label = torch.stack([torch.Tensor(i) for i in test_data_label])\n",
    "    \n",
    "    first_augmented_dataset_test = torch.utils.data.TensorDataset(tensor_test,tensor_test_label) # create your datset\n",
    "    \n",
    "    first_dataloader_test = torch.utils.data.DataLoader(first_augmented_dataset_test,batch_size=50, shuffle=True) # create your dataloader\n",
    "    \n",
    "    return first_dataloader,first_dataloader_test\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------Encoder Z Network- Encodes the images to the z latent space --------------------------\n",
    "class EncoderZ(nn.Module):\n",
    "    #def __init__(self, z_dim, hidden_dim):\n",
    "    def __init__(self, x_dim, y_dim, h_dim1, h_dim2, h_dim3, h_dim4, h_dim5, z_dim):\n",
    "        super(EncoderZ, self).__init__()\n",
    "        self.fc1 = nn.Linear(x_dim+y_dim, h_dim1) # x_dim=10000 + y_dim=2 to h_dim1=4096 \n",
    "        self.fc2 = nn.Linear(h_dim1, h_dim2) #h_dim1=4096 to h_dim2=2048\n",
    "        self.fc3 = nn.Linear(h_dim2, h_dim3) #h_dim2=2048 to h_dim3=1024\n",
    "        self.fc4 = nn.Linear(h_dim3, h_dim4) #h_dim3=1024 to h_dim4=512\n",
    "        self.fc5 = nn.Linear(h_dim4, h_dim5) #h_dim4=512 to h_dim5=256\n",
    "        self.fc61 = nn.Linear(h_dim5, z_dim) #h_dim5=256 to z_dim=2\n",
    "        self.fc62 = nn.Linear(h_dim5, z_dim) #h_dim5=256 to z_dim=2\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, x_y_2):\n",
    "        [x,y]=x_y_2\n",
    "        \n",
    "        x = x.reshape(-1, 10000) \n",
    "        y = y.reshape(-1, 2) \n",
    "        \n",
    "        x_y_1 = torch.cat((x,y), dim=1) \n",
    "        x_y_1 = x_y_1.view(x_y_1.size(0), -1)\n",
    "        \n",
    "        slope_param=0.0001\n",
    "        \n",
    "        # then compute the hidden units\n",
    "        # We use fully connected layers\n",
    "        hidden = self.softplus(self.fc1(x_y_1))\n",
    "        \n",
    "        \n",
    "       # hidden = F.leaky_relu(self.fc1(x),slope_param)\n",
    "        hidden = F.leaky_relu(self.fc2(hidden),slope_param)\n",
    "        hidden = F.leaky_relu(self.fc3(hidden),slope_param)\n",
    "        hidden = F.leaky_relu(self.fc4(hidden),slope_param)\n",
    "        hidden = F.leaky_relu(self.fc5(hidden),slope_param)\n",
    "        \n",
    "        z_loc = self.fc61(hidden)\n",
    "        z_scale = torch.exp(self.fc62(hidden)) # mu, log_var\n",
    "        \n",
    "        \n",
    "        return z_loc, z_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------Encoder Z Network- Encodes the images to the z latent space --------------------------\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, x_dim, y_dim, h_dim1, h_dim2, h_dim3, h_dim4, h_dim5, z_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        # setup the two linear transformations used\n",
    "        self.fc7 = nn.Linear(z_dim+y_dim, h_dim5) #z_dim=2 to h_dim5=256\n",
    "        self.fc8 = nn.Linear(h_dim5, h_dim4) #h_dim5=256 to h_dim4=512\n",
    "        self.fc9 = nn.Linear(h_dim4, h_dim3) #h_dim4=512 to h_dim3=1024\n",
    "        self.fc10 = nn.Linear(h_dim3, h_dim2) #h_dim3=1024 to h_dim2=2048\n",
    "        self.fc11 = nn.Linear(h_dim2, h_dim1) #h_dim2=2048 to h_dim1=4096\n",
    "        self.fc12 = nn.Linear(h_dim1, x_dim)  #h_dim1=4096 to x_dim=10000\n",
    "        \n",
    "        self.softplus = nn.Softplus()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,z_y_2):\n",
    "        # define the forward computation on the latent z\n",
    "        # first compute the hidden units\n",
    "        \n",
    "        [z,y]=z_y_2\n",
    "        \n",
    "        z = z.reshape(-1, 2) #@David Change this to reshape if something fucks up\n",
    "        y = y.reshape(-1, 2)\n",
    "        z_y_1 = torch.cat((z,y), dim=1)\n",
    "        z_y_1 = z_y_1.view(z_y_1.size(0), -1)\n",
    "        \n",
    "        slope_param=0.0001\n",
    "        hidden = F.leaky_relu(self.fc7(z_y_1),slope_param)\n",
    "        hidden = F.leaky_relu(self.fc8(hidden),slope_param)\n",
    "        hidden = F.leaky_relu(self.fc9(hidden),slope_param)\n",
    "        hidden = F.leaky_relu(self.fc10(hidden),slope_param)\n",
    "        hidden = F.leaky_relu(self.fc11(hidden),slope_param)\n",
    "        \n",
    "        loc_img = self.sigmoid(self.fc12(hidden))\n",
    "        return loc_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    # by default our latent space is 50-dimensional\n",
    "    # and we use 400 hidden units\n",
    "    def __init__(self, x_dim=10000, y_dim=2, h_dim1=4096, h_dim2=2048, h_dim3=1024, h_dim4=512, h_dim5=256, z_dim=2, use_cuda=True):\n",
    "        super(VAE, self).__init__()\n",
    "    \n",
    "        # create the encoder and decoder networks\n",
    "        # a split in the final layer's size is used for multiple outputs\n",
    "        # and potentially applying separate activation functions on them\n",
    "        # e.g. in this network the final output is of size [z_dim,z_dim]\n",
    "        # to produce loc and scale, and apply different activations [None,Exp] on them\n",
    "              \n",
    "        self.encoder_z = EncoderZ(x_dim, y_dim, h_dim1, h_dim2, h_dim3, h_dim4, h_dim5, z_dim)\n",
    "        \n",
    "        self.decoder = Decoder(x_dim, y_dim, h_dim1, h_dim2, h_dim3, h_dim4, h_dim5, z_dim)\n",
    "\n",
    "        if use_cuda:\n",
    "            # calling cuda() here will put all the parameters of\n",
    "            # the encoder and decoder networks into gpu memory\n",
    "            self.cuda()\n",
    "            \n",
    "            \n",
    "        self.use_cuda = use_cuda\n",
    "        self.z_dim = z_dim\n",
    "        self.output_size = y_dim\n",
    "        \n",
    "        \n",
    "    # define the model p(x|z)p(z)\n",
    "    def model(self, xs, ys):\n",
    "        # register this pytorch module and all of its sub-modules with pyro\n",
    "        pyro.module(\"ss_vae\", self)\n",
    "        batch_size = xs.size(0)\n",
    "\n",
    "        # inform Pyro that the variables in the batch of xs, ys are conditionally independent\n",
    "        with pyro.plate(\"data\"):\n",
    "\n",
    "            # sample the handwriting style from the constant prior distribution\n",
    "            prior_loc = xs.new_zeros([batch_size, self.z_dim])\n",
    "            prior_scale = xs.new_ones([batch_size, self.z_dim])\n",
    "            zs = pyro.sample(\"z\", dist.Normal(prior_loc, prior_scale).to_event(1))\n",
    "\n",
    "            # if the label y (which digit to write) is supervised, sample from the\n",
    "            # constant prior, otherwise, observe the value (i.e. score it against the constant prior)\n",
    "            alpha_prior = xs.new_ones([batch_size, self.output_size]) / (1.0 * self.output_size)\n",
    "            ys = pyro.sample(\"y\", dist.OneHotCategorical(alpha_prior), obs=ys)\n",
    "            \n",
    "            # finally, score the image (x) using the handwriting style (z) and\n",
    "            # the class label y (which digit to write) against the\n",
    "            # parametrized distribution p(x|y,z) = bernoulli(decoder(y,z))\n",
    "            # where `decoder` is a neural network\n",
    "            loc = self.decoder.forward([zs, ys])\n",
    "            pyro.sample(\"x\", dist.Bernoulli(loc).to_event(1), obs=xs)\n",
    "            \n",
    "    def guide(self, xs, ys):\n",
    "        with pyro.plate(\"data\"):\n",
    "           # if the class label (the digit) is not supervised, sample\n",
    "           # (and score) the digit with the variational distribution\n",
    "           # q(y|x) = categorical(alpha(x))\n",
    "           \n",
    "            #-------------------REMOVED THIS PART FOR THE CLASSIFIER ASSUME ALL DATA ARE LABELLED---------\n",
    "\n",
    "           # sample (and score) the latent handwriting-style with the variational\n",
    "           # distribution q(z|x,y) = normal(loc(x,y),scale(x,y))\n",
    "           loc, scale = self.encoder_z.forward([xs, ys])\n",
    "           pyro.sample(\"z\", dist.Normal(loc, scale).to_event(1))\n",
    "\n",
    "    # define a helper function for reconstructing images\n",
    "    def reconstruct_img(self, xs, ys):\n",
    "        # encode image x\n",
    "        z_loc, z_scale = self.encoder_z.forward([xs,ys])\n",
    "        # sample in latent space\n",
    "        z = dist.Normal(z_loc, z_scale).sample()\n",
    "        # decode the image (note we don't sample in image space)\n",
    "        loc_img = self.decoder.forward([zs,ys])\n",
    "        \n",
    "        return loc_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(svi, train_loader, use_cuda=True):\n",
    "    # initialize loss accumulator\n",
    "    epoch_loss = 0.\n",
    "    # do a training epoch over each mini-batch x returned\n",
    "    # by the data loader\n",
    "    for x, y in train_loader:\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "        # do ELBO gradient and accumulate loss\n",
    "        labels_y = torch.tensor(np.zeros((y.shape[0],2)))\n",
    "        y_2=torch.Tensor.cpu(y.reshape(1,y.size()[0])[0]).numpy().astype(int)  \n",
    "        labels_y=np.eye(2)[y_2]\n",
    "        labels_y = torch.from_numpy(labels_y)   \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        epoch_loss += svi.step(x.reshape(-1,10000),labels_y.cuda().float())\n",
    "\n",
    "    # return epoch loss\n",
    "    normalizer_train = len(train_loader.dataset)\n",
    "    total_epoch_loss_train = epoch_loss / normalizer_train\n",
    "    return total_epoch_loss_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the mini batch logic is handled by the data loader, we should duplicate the same logic of the data loader with the FIRST Database. The core of the training loop is svi.step(x). This is the data entry point. It should be noted that we have to change the looping structure to that of the mini batch structure that is used for the FIRST database.\n",
    "\n",
    "# To do evaluate part afterwards\n",
    "\n",
    "def evaluate(svi, test_loader, use_cuda=True):\n",
    "    # initialize loss accumulator\n",
    "    test_loss = 0.\n",
    "    # compute the loss over the entire test set\n",
    "    for x,y in test_loader:\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "        # compute ELBO estimate and accumulate loss\n",
    "        test_loss += svi.evaluate_loss(x) #Data entry point <---------------------------------Data Entry Point\n",
    "    normalizer_test = len(test_loader.dataset)\n",
    "    total_epoch_loss_test = test_loss / normalizer_test\n",
    "    return total_epoch_loss_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_sample_plotter(epoch):\n",
    "    with torch.no_grad():\n",
    "        count = 0\n",
    "        z_fr1 = z_fr2 = torch.randn(100, 2)\n",
    "        labels_y1 = torch.tensor(np.zeros((100,2)))\n",
    "        labels_y2 = torch.tensor(np.zeros((100,2)))\n",
    "        for i in range (0,10):\n",
    "            for j in range (0,10):\n",
    "                z_fr1[count,0] = z_fr2[count,0] = np.random.uniform(-1,1)\n",
    "                z_fr1[count,1] = z_fr2[count,1] = np.random.uniform(-1,1)\n",
    "                labels_y1[count,0] = 1\n",
    "                labels_y2[count,1] = 1\n",
    "                count = count +1 \n",
    "        \n",
    "        sample1 = vae.decoder([z_fr1.cuda(),labels_y1.cuda().float()])\n",
    "    \n",
    "        save_image(sample1.view(100, 1, 100, 100), 'fr1_sample_2_z_space_' +str(epoch)+'.png',nrow=10)\n",
    "    \n",
    "        sample2 = vae.decoder([z_fr2.cuda(),labels_y2.cuda().float()])\n",
    "\n",
    "        save_image(sample2.view(100, 1, 100, 100), 'fr2_sample_2_z_space_' +str(epoch)+'.png',nrow=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(epoch):\n",
    "    print(\"loading model from ...\")\n",
    "    vae.load_state_dict(torch.load('/raid/scratch/davidb/1_DEVELOPMENT/VAE_FIRST/models/file_conf3_semi_supervised_vae_epoch_'+str(epoch)))\n",
    "    print(\"loading optimizer states from ...\")\n",
    "    optimizer.load('/raid/scratch/davidb/1_DEVELOPMENT/VAE_FIRST/models/file_conf3_semi_supervised_vae_epoch_'+str(epoch)+'_opt')\n",
    "    print(\"done loading model and optimizer states.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run options\n",
    "LEARNING_RATE = 1.0e-3\n",
    "USE_CUDA = True\n",
    "\n",
    "# Run only for a single iteration for testing\n",
    "NUM_EPOCHS = 20000\n",
    "TEST_FREQUENCY = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_array=np.zeros((1,2))\n",
    "\n",
    "results_array_temp=np.zeros((1,2))\n",
    "\n",
    "results_array_test=np.zeros((1,2))\n",
    "\n",
    "results_array_temp_test=np.zeros((1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_loader,test_loader = dataloader_first()\n",
    "# clear param store\n",
    "pyro.clear_param_store()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the VAE\n",
    "vae = VAE(use_cuda=USE_CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the optimizer\n",
    "adagrad_params = {\"lr\": 0.001}\n",
    "optimizer = Adagrad(adagrad_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "svi = SVI(vae.model, vae.guide, optimizer, loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model from ...\n",
      "loading optimizer states from ...\n",
      "done loading model and optimizer states.\n"
     ]
    }
   ],
   "source": [
    "load_checkpoint(3700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_fr1 = z_fr2 = torch.randn(100, 2)\n",
    "labels_y1 = torch.tensor(np.zeros((100,2)))\n",
    "labels_y2 = torch.tensor(np.zeros((100,2)))\n",
    "\n",
    "labels_y1[:,0] = 0\n",
    "labels_y2[:,1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f72c07b2e10>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_y1[:,0] = 1\n",
    "labels_y2[:,1] = 1\n",
    "\n",
    "num =1\n",
    "\n",
    "sample1 = vae.decoder([z_fr1.cuda(),labels_y1.cuda().float()])\n",
    "\n",
    "img=sample1[num].reshape(100,100).cpu().detach().numpy()\n",
    "\n",
    "\n",
    "sample2 = vae.decoder([z_fr2.cuda(),labels_y2.cuda().float()])\n",
    "\n",
    "img2=sample2[num].reshape(100,100).cpu().detach().numpy()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax1 = fig.add_subplot(2,2,1)\n",
    "ax1.imshow(img)\n",
    "ax2 = fig.add_subplot(2,2,2)\n",
    "ax2.imshow(img2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize([0.5],[0.5])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = FRDEEPF(root='./FIRST_data', train=True, download=True, transform=transform)  \n",
    "batch_size_train = 2\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size_train, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "testset = FRDEEPF(root='./FIRST_data', train=False, download=True, transform=transform) \n",
    "batch_size_test = 2\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size_test, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('FRI', 'FRII')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    # unnormalize\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAADLCAYAAABgQVj0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAVh0lEQVR4nO3df4wc5X3H8fdn9+y9Oww2hkCxTbCJrDRQpQRZLm2aCMVJ+NEophKRnEaNlSJZbUibNI0SKFIJf0RK+iNpI7VETqBxKsSPkESgirRBhCiqXEwczG8DNsaFC8YGjI3tW593b7/9Y2ePtdn7tbN74539vKTTzjwze/t9bva+++wzz8yjiMDMzPKrkHUAZmbWXU70ZmY550RvZpZzTvRmZjnnRG9mlnNO9GZmOde1RC/pcknPStop6bpuvY6ZmU1N3RhHL6kIPAd8BBgBfgV8MiKe7viLmZnZlLrVol8N7IyIXRFxDLgDWNul1zIzsykMdOn3LgVealofAX5vsp2Hh4dj0aJFXQrFzCyf9uzZ81pEvGO6/bqV6NWi7Lg+IkkbgA0ACxcuZMOGDV0Kxcwsn2666ab/m8l+3Ur0I8C5TevLgJebd4iIjcBGgCVLlgTATTfd1KVwzGbvxhtvnFj2e9NOJs3vzZnoVh/9r4CVklZImg+sA+7t0muZmdkUutKij4iqpM8B/w0UgVsj4qluvJaZmU2tW103RMR9wH3d+v1mZjYzvjLWzCznnOjNzHLOid7MLOec6M3Mcs6J3qyHlUqlrEOwHuBEb9bDxsbGJpYHBwcZHBxsuZ8/EPJjsmM8la4NrzSzuXX06NFJtzV/IFhva+eOw27Rm5n1kHY+tJ3ozczmWKlUmtPuNHfdmJnNsbnuSnOL3sws55zozcxyzonezCzn2k70ks6V9KCk7ZKekvT5pHyxpPsl7UgeT+9cuGZmNltpWvRV4G8i4j3AJcC1ki4ArgMeiIiVwAPJupmZZaTtRB8ReyLikWT5ELCd+qTga4FNyW6bgKvSBmlmZu3rSB+9pOXA+4AtwNkRsQfqHwbAWZ14DTMza0/qRC9pAfAj4AsR8eYsnrdB0lZJW0dHR9OGYWZmk0iV6CXNo57kb4uIHyfFeyWdk2w/B9jX6rkRsTEiVkXEquHh4TRhmJnZFNKMuhFwC7A9Ir7ZtOleYH2yvB64p/3wzMwsrTS3QHg/8KfAE5IeTcr+Fvg6cJeka4AXgU+kC9HMzNJoO9FHxP8AmmTzmnZ/r5mZdZavjDUzyzknejOznHOiN+szg4ODeKRbf3GiN+szjano+mUe2X6p51Sc6M36zNjYGLVarW/mke2Xek7Fid6sD001kbjljxO9mVnOOdGbmeWcE72ZWc450ZuZ5ZwTvZlZzjnRm5nlnBO9mVnOdWKGqaKkbZL+M1lfIWmLpB2S7pQ0P32YZmbWrk606D9PfWLwhm8A34qIlcAbwDUdeA0zM2tT2qkElwF/BHwvWRfwIeDuZJdNwFVpXsPMLA3f6yZ9i/6fgS8DtWT9DOBARFST9RFgacrXMDNrm+91k27O2I8B+yLi183FLXaNSZ6/QdJWSVtHR0fbDcPMbFJuzdelnTP245KuBAaB06i38BdJGkha9cuAl1s9OSI2AhsBlixZ0vLDwMwsDbfm69pu0UfE9RGxLCKWA+uAn0fEp4AHgauT3dYD96SO0szM2taNcfRfAb4oaSf1PvtbuvAaZmY2Q2m6biZExC+AXyTLu4DVnfi9ZmaWnq+MNTPLOSd6M7Occ6I3M8s5J3ozs5xzojczyzknejOznHOiNzPLOSd6M7Occ6I3M8s5J3ozs5xzojczyzknejOznHOiNzPLubRzxi6SdLekZyRtl/T7khZLul/SjuTx9E4Fa2Zms5e2Rf8vwH9FxG8DvwtsB64DHoiIlcADybqZmWUkzZyxpwEfJJlYJCKORcQBYC2wKdltE3BV2iDNzKx9aVr05wOvAv8uaZuk70k6BTg7IvYAJI9ntXqyJwc3M5sbaRL9AHAxcHNEvA84wiy6aSJiY0SsiohVw8PDKcIwM7OppEn0I8BIRGxJ1u+mnvj3SjoHIHncly5EM8uToaEhBgcHsw6jr7Sd6CPiFeAlSe9OitYATwP3AuuTsvXAPakiNLOeUCqVpt1ncHCQgYEBisXiHERkDWknB/9L4DZJ84FdwGeof3jcJeka4EXgEylfw8x6wNjY2JTbS6USxWJx4mfBggVUq1WOHj06RxH2r1SJPiIeBVa12LQmze81s3wZHBxEEoVCgYGBAQqFAsViEUlO9HPAV8aaWdc1knxzi75YLDIwMIAHY3SfE72ZWc450ZtZ1zVa8wMDAxNdN5Imfk40kxO7NnNpT8aamU2qMYyykeRLpRKlUonx8XGq1SoRQUS87XnTndi12XGL3szmRLFYpFQqTQyxbC4fGhrKMLL8c6I3s65p7pppnICVNNGKb2wvFAq+iKqL3HWTI4VCgUKh/tldq9Wo1WoZR2S9oDHqpVardXSoY3PibiRzSVSrVSqVCpVKhfHx8Y693myUSqW2hnaWSqWe7FZyos+JefPmMTw8zPDwMMVikXK5zOHDhwH3d9rUGsm20++TQqEw0XIvFArMmzdvItGPjY1x7NgxIuK4Fv5cGRsbe9sJ35kk8V79X3Kiz4Fiscgpp5zCO9/5TpYvX06pVGLPnj3s2rULgNdee41jx45lHKWdrLqZvBp98QsWLGDRokUAvPnmm1SrVcbHx6nVahNdO3N94dSJ9e7VJD4TTvQ5MDAwwBlnnMFFF13EBz7wARYsWMC2bdsm3riHDx+mUqnMeavJ7NRTTwVg5cqVLF26lAMHDvDcc89Rq9Uol8sZR9c/nOhzYN68eSxevJgLL7yQNWvWcNppp1Gr1Xj66acB2L17N4cPH3aitzk1f/58zjvvPAAuu+wyLrzwQp544glef/119u7dm3F0/cWJvsc1TnINDQ2xePFizjrrLE455RROPfXUiT7IxkgHs7lSKpUYGhpixYoVAFx66aWsXr2aefPmsXnz5uOGV1r3+a+dE+Pj4xw+fJiRkREGBgbYu3fvcX2OTvQ2lxo3L2sk9EqlwptvvsmhQ4eoVCoeETbHUo2jl/TXkp6S9KSk2yUNSlohaYukHZLuTG5hbGZmGWm7RS9pKfBXwAURUZZ0F7AOuBL4VkTcIek7wDXAzR2J1loaHx/n0KFDPP/882zevBmAZ555hjfeeAOAarWaZXjWpyKCV199FYDNmzezY8cOtm3bxsjIiEeBzbG0XTcDwJCkCjAM7AE+BPxJsn0T8FWc6LsmIqhUKuzfv5/t27dz5MgRJPHKK6/w+uuvA3Ds2DF/VbY5V61WJxL9ww8/TKFQ4MUXX2Tfvn1UKpWMo+svbSf6iPiNpH+kPotUGfgZ8GvgQEQ0mpAjwNJWz5e0AdgAsHDhwnbDMOr/UAcPHuSFF17g4MGDFItFDh06NNGib1yYYjaXarXaxEV7u3fvplKp8MYbb1Aul1uOWe/Vq057QZqum9OBtcAK4ADwQ+CKFru2zDARsRHYCLBkyRJnoRQal67v37+fcrlMsVhkbGyM0dFRAI+hPwkMDg4SEX2RyBpTBsJb3YaHDh2iXC5z5MiRSbsSG7fvsM5L03XzYeCFiHgVQNKPgT8AFkkaSFr1y4CX04dp06lWq4yOjlKpVCgUClSr1Yl/KHfbZK+fpstrnkmq0cAYGxvj6NGjVCqVSS+U8vu0e9J8hL4IXCJpWPWxe2uAp4EHgauTfdYD96QL0WYiIibuIdL4hxofH2d8fNyteZtTzfPBNm6ud+zYMcbGxqbsm++HbztZaTvRR8QW4G7gEeCJ5HdtBL4CfFHSTuAM4JYOxGkzUKvVJpJ7rVabdFIHs25qnhO28R5s3K3Stz3IRqpRNxFxI3DjCcW7gNVpfq+1z4ndsjQ8PDxxoZSkiTtjNhoelg1fGWtmHdPcbdN8v/l+vZbjZBlJ5ERvZh1Vq9UmbkPcnOj7sdvmZLn1iMczmZnlnBO9mXVMY0BAo9umcTK2UCi8bUanfnCyDKt1os8hn/SyrDSP+oK35jFujMJJqx8/LDrBiT6nGi0pJ32bS2NjY1Sr1eOmCIS35qUtlUqzStZO7J3hRG9mHVUul6lWq0TERAu/UqkwOjqKpFmdoOyneV27yaNuckiSW/KWqXK5TKFQmHgfNvqqjx496lZ6Bpzoc6YxhjkiJvpJnfQtC0eOHGlZ7lb53HPXTY405o8dGBigUCjM+muymeWTW/Q51GjBuyVvZuBEnyuNk18ecWNmzZzoc6ZxW1gzs4Zp++gl3Sppn6Qnm8oWS7pf0o7k8fSkXJK+LWmnpMclXdzN4M3MbHozORn7feDyE8quAx6IiJXAA8k61KcSXJn8bMCTgpuZZW7aRB8RvwT2n1C8FtiULG8Crmoq/0HUPUR9WsFzOhWsmVmDx+PPXLvDK8+OiD0AyeNZSflS4KWm/UaSsreRtEHSVklbG5NYm5nNlMfjz1ynx9G3GrTdcuhHRGyMiFURsWp4eLjDYZiZWUO7iX5vo0smedyXlI8A5zbttwx4uf3wzMwsrXYT/b3A+mR5PXBPU/mnk9E3lwAHG108ZmaWjWnH0Uu6HbgUOFPSCPXJwL8O3CXpGuBF4BPJ7vcBVwI7gVHgM12I2czMZmHaRB8Rn5xk05oW+wZwbdqgzMysc3xTMzOznHOiNzPLOSd6M7Occ6I3M8s5J3ozs5xzojczyzknejOznHOiNzPLOSd6M7Oc81SCZhkZGhoC6tM/+pa71k1u0ZtlpFwuUy6X5zzJ9+qEHe3E3at17TS36HNCqk8FUL/dkNnkevXbw2zjHhwcnPi/6HftTg7+D5KeSSYA/4mkRU3brk8mB39W0mXdCtze0vxm9hvbrO7o0aOUy+WswzgptDs5+P3A70TEe4HngOsBJF0ArAMuTJ7zb5KKHYvWWpI08dNYNzNraGty8Ij4WURUk9WHqM8kBfXJwe+IiLGIeIH6felXdzBeMzObpU6cjP0z4KfJ8ownB7fOaG7FN7fqzcwaUiV6STcAVeC2RlGL3VqeHZS0QdJWSVtHR0fThNHXJFEoFCgUChSLRQqFgpO9mR2n7UQvaT3wMeBT8dZQjxlPDh4RGyNiVUSsGh4ebjcMo57sBwYGGBgYoFgsOtGb2XHaSvSSLge+Anw8Ipqb4/cC6ySVJK0AVgIPpw/TJtM8nNJdN2bWSruTg18PlID7k8TyUET8eUQ8Jeku4GnqXTrXRsR4t4K3eqKv1WqMj48TEYyPj1Or1bIOy8xOIu1ODn7LFPt/DfhamqBsdhpJXhK1Ws0XTZnZcXxlbE64FW9mk/G9bsxyxvd3sRM50Zv1ASf//uZEb5YzJ978q1QqeTRWn3OiNzPLOSd6s5wbGxvr+5FY/d515URv1gd69R70ndLv9XeiN8uxUqnU961Zc6I3M8s9XzBllmP93mVhdW7Rm5nlnBO9mVnOOdGbmeXctIle0q2S9kl6ssW2L0kKSWcm65L0bUk7JT0u6eJuBG1mZjM3kxb994HLTyyUdC7wEeDFpuIrqE82shLYANycPkQzM0tj2kQfEb8E9rfY9C3gyxw/J+xa4AdR9xCwSNI5HYnUzMza0u5Ugh8HfhMRj52waSnwUtP6SFJmZmYZmfU4eknDwA3AR1ttblHW8iYbkjZQ795h4cKFsw3DzMxmqJ0W/buAFcBjknYDy4BHJP0W9Rb8uU37LgNebvVLImJjRKyKiFXDw8NthGFmZjMx60QfEU9ExFkRsTwillNP7hdHxCvAvcCnk9E3lwAHI2JPZ0M2s9kYGhrKOgTL2EyGV94O/C/wbkkjkq6ZYvf7gF3ATuC7wGc7EqWZta1cLmcdgmVs2j76iPjkNNuXNy0HcG36sMzMrFN8ZayZWc450ZuZ5ZwTvZlZzjnRm5nlnBO9mVnOOdGbmeWcE72ZWc450ZuZ5dxJNTn4jTfemHUIZi35vWm9zC16M7OcU/2uBRkHIb0KHAFeyzqWLjiTfNYLXLdelde65bVeMHndzouId0z35JMi0QNI2hoRq7KOo9PyWi9w3XpVXuuW13pB+rq568bMLOec6M3Mcu5kSvQbsw6gS/JaL3DdelVe65bXekHKup00ffRmZtYdJ1OL3szMuiDzRC/pcknPStop6bqs40lL0m5JT0h6VNLWpGyxpPsl7UgeT886zpmQdKukfZKebCprWZdknuBvJ8fxcUkXZxf51Cap11cl/SY5bo9KurJp2/VJvZ6VdFk2Uc+MpHMlPShpu6SnJH0+Kc/DcZusbj197CQNSnpY0mNJvW5KyldI2pIcszslzU/KS8n6zmT78mlfJCIy+wGKwPPA+cB84DHggixj6kCddgNnnlD298B1yfJ1wDeyjnOGdfkgcDHw5HR1Aa4EfgoIuATYknX8s6zXV4Evtdj3guR9WQJWJO/XYtZ1mKJu5wAXJ8unAs8ldcjDcZusbj197JK//YJkeR6wJTkWdwHrkvLvAH+RLH8W+E6yvA64c7rXyLpFvxrYGRG7IuIYcAewNuOYumEtsClZ3gRclWEsMxYRvwT2n1A8WV3WAj+IuoeARZLOmZtIZ2eSek1mLXBHRIxFxAvUJ75f3bXgUoqIPRHxSLJ8CNgOLCUfx22yuk2mJ45d8rc/nKzOS34C+BBwd1J+4jFrHMu7gTWSNNVrZJ3olwIvNa2PMPWB6wUB/EzSryVtSMrOjog9UH+zAmdlFl16k9UlD8fyc0n3xa1N3Ws9W6/kK/37qLcQc3XcTqgb9Pixk1SU9CiwD7if+rePAxFRTXZpjn2iXsn2g8AZU/3+rBN9q0+hXh8G9P6IuBi4ArhW0gezDmiO9PqxvBl4F3ARsAf4p6S8J+slaQHwI+ALEfHmVLu2KDup69eibj1/7CJiPCIuApZR/9bxnla7JY+zrlfWiX4EOLdpfRnwckaxdEREvJw87gN+Qv2g7W18HU4e92UXYWqT1aWnj2VE7E3+2WrAd3nrK37P1UvSPOqJ8LaI+HFSnIvj1qpueTp2EXEA+AX1PvpFkhp3GG6OfaJeyfaFTNMVmXWi/xWwMjm7PJ/6iYV7M46pbZJOkXRqYxn4KPAk9TqtT3ZbD9yTTYQdMVld7gU+nYziuAQ42Ogq6AUn9Ev/MfXjBvV6rUtGOqwAVgIPz3V8M5X01d4CbI+IbzZt6vnjNlndev3YSXqHpEXJ8hDwYernHx4Erk52O/GYNY7l1cDPIzkzO6mT4IzzldTPnj8P3JB1PCnrcj71s/yPAU816kO9/+wBYEfyuDjrWGdYn9upfxWuUG9FXDNZXah/nfzX5Dg+AazKOv5Z1us/krgfT/6Rzmna/4akXs8CV2Qd/zR1+0PqX+MfBx5Nfq7MyXGbrG49feyA9wLbkvifBP4uKT+f+gfTTuCHQCkpH0zWdybbz5/uNXxlrJlZzmXddWNmZl3mRG9mlnNO9GZmOedEb2aWc070ZmY550RvZpZzTvRmZjnnRG9mlnP/D1TFqnK18gGNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " FRII   FRI\n"
     ]
    }
   ],
   "source": [
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(batch_size_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 34 * 34, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # conv1 output width: input_width - (kernel_size - 1) => 150 - (5-1) = 146\n",
    "        # pool 1 output width: int(input_width/2) => 73\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # conv2 output width: input_width - (kernel_size - 1) => 73 - (5-1) = 69\n",
    "        # pool 2 output width: int(input_width/2) => 34\n",
    "        x = self.pool(F.relu(self.conv2(x)))  \n",
    "        x = x.view(-1, 16 * 34 * 34)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer2 = optim.Adagrad(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] loss: 3.048\n",
      "[1,   100] loss: 0.719\n",
      "[1,   150] loss: 0.731\n",
      "[1,   200] loss: 0.712\n",
      "[1,   250] loss: 0.701\n",
      "[2,    50] loss: 0.703\n",
      "[2,   100] loss: 0.677\n",
      "[2,   150] loss: 0.700\n",
      "[2,   200] loss: 0.676\n",
      "[2,   250] loss: 0.653\n",
      "[3,    50] loss: 0.650\n",
      "[3,   100] loss: 0.657\n",
      "[3,   150] loss: 0.659\n",
      "[3,   200] loss: 0.591\n",
      "[3,   250] loss: 0.585\n",
      "[4,    50] loss: 0.513\n",
      "[4,   100] loss: 0.622\n",
      "[4,   150] loss: 0.527\n",
      "[4,   200] loss: 0.519\n",
      "[4,   250] loss: 0.540\n",
      "[5,    50] loss: 0.536\n",
      "[5,   100] loss: 0.488\n",
      "[5,   150] loss: 0.585\n",
      "[5,   200] loss: 0.499\n",
      "[5,   250] loss: 0.555\n",
      "[6,    50] loss: 0.447\n",
      "[6,   100] loss: 0.541\n",
      "[6,   150] loss: 0.456\n",
      "[6,   200] loss: 0.553\n",
      "[6,   250] loss: 0.487\n",
      "[7,    50] loss: 0.452\n",
      "[7,   100] loss: 0.522\n",
      "[7,   150] loss: 0.471\n",
      "[7,   200] loss: 0.436\n",
      "[7,   250] loss: 0.386\n",
      "[8,    50] loss: 0.460\n",
      "[8,   100] loss: 0.376\n",
      "[8,   150] loss: 0.431\n",
      "[8,   200] loss: 0.447\n",
      "[8,   250] loss: 0.384\n",
      "[9,    50] loss: 0.384\n",
      "[9,   100] loss: 0.376\n",
      "[9,   150] loss: 0.447\n",
      "[9,   200] loss: 0.439\n",
      "[9,   250] loss: 0.415\n",
      "[10,    50] loss: 0.307\n",
      "[10,   100] loss: 0.359\n",
      "[10,   150] loss: 0.308\n",
      "[10,   200] loss: 0.362\n",
      "[10,   250] loss: 0.420\n",
      "[11,    50] loss: 0.397\n",
      "[11,   100] loss: 0.359\n",
      "[11,   150] loss: 0.303\n",
      "[11,   200] loss: 0.333\n",
      "[11,   250] loss: 0.321\n",
      "[12,    50] loss: 0.313\n",
      "[12,   100] loss: 0.377\n",
      "[12,   150] loss: 0.240\n",
      "[12,   200] loss: 0.258\n",
      "[12,   250] loss: 0.293\n",
      "[13,    50] loss: 0.277\n",
      "[13,   100] loss: 0.258\n",
      "[13,   150] loss: 0.327\n",
      "[13,   200] loss: 0.326\n",
      "[13,   250] loss: 0.321\n",
      "[14,    50] loss: 0.259\n",
      "[14,   100] loss: 0.262\n",
      "[14,   150] loss: 0.290\n",
      "[14,   200] loss: 0.284\n",
      "[14,   250] loss: 0.285\n",
      "[15,    50] loss: 0.296\n",
      "[15,   100] loss: 0.262\n",
      "[15,   150] loss: 0.231\n",
      "[15,   200] loss: 0.328\n",
      "[15,   250] loss: 0.209\n",
      "[16,    50] loss: 0.253\n",
      "[16,   100] loss: 0.239\n",
      "[16,   150] loss: 0.293\n",
      "[16,   200] loss: 0.244\n",
      "[16,   250] loss: 0.192\n",
      "[17,    50] loss: 0.163\n",
      "[17,   100] loss: 0.236\n",
      "[17,   150] loss: 0.196\n",
      "[17,   200] loss: 0.246\n",
      "[17,   250] loss: 0.251\n",
      "[18,    50] loss: 0.239\n",
      "[18,   100] loss: 0.216\n",
      "[18,   150] loss: 0.214\n",
      "[18,   200] loss: 0.153\n",
      "[18,   250] loss: 0.247\n",
      "[19,    50] loss: 0.230\n",
      "[19,   100] loss: 0.146\n",
      "[19,   150] loss: 0.183\n",
      "[19,   200] loss: 0.214\n",
      "[19,   250] loss: 0.230\n",
      "[20,    50] loss: 0.199\n",
      "[20,   100] loss: 0.163\n",
      "[20,   150] loss: 0.237\n",
      "[20,   200] loss: 0.170\n",
      "[20,   250] loss: 0.171\n",
      "[21,    50] loss: 0.137\n",
      "[21,   100] loss: 0.165\n",
      "[21,   150] loss: 0.202\n",
      "[21,   200] loss: 0.178\n",
      "[21,   250] loss: 0.231\n",
      "[22,    50] loss: 0.200\n",
      "[22,   100] loss: 0.189\n",
      "[22,   150] loss: 0.147\n",
      "[22,   200] loss: 0.152\n",
      "[22,   250] loss: 0.130\n",
      "[23,    50] loss: 0.199\n",
      "[23,   100] loss: 0.113\n",
      "[23,   150] loss: 0.107\n",
      "[23,   200] loss: 0.196\n",
      "[23,   250] loss: 0.164\n",
      "[24,    50] loss: 0.150\n",
      "[24,   100] loss: 0.129\n",
      "[24,   150] loss: 0.086\n",
      "[24,   200] loss: 0.136\n",
      "[24,   250] loss: 0.167\n",
      "[25,    50] loss: 0.144\n",
      "[25,   100] loss: 0.167\n",
      "[25,   150] loss: 0.139\n",
      "[25,   200] loss: 0.113\n",
      "[25,   250] loss: 0.097\n",
      "[26,    50] loss: 0.195\n",
      "[26,   100] loss: 0.162\n",
      "[26,   150] loss: 0.130\n",
      "[26,   200] loss: 0.108\n",
      "[26,   250] loss: 0.082\n",
      "[27,    50] loss: 0.160\n",
      "[27,   100] loss: 0.098\n",
      "[27,   150] loss: 0.119\n",
      "[27,   200] loss: 0.101\n",
      "[27,   250] loss: 0.160\n",
      "[28,    50] loss: 0.096\n",
      "[28,   100] loss: 0.082\n",
      "[28,   150] loss: 0.102\n",
      "[28,   200] loss: 0.106\n",
      "[28,   250] loss: 0.203\n",
      "[29,    50] loss: 0.082\n",
      "[29,   100] loss: 0.185\n",
      "[29,   150] loss: 0.110\n",
      "[29,   200] loss: 0.085\n",
      "[29,   250] loss: 0.101\n",
      "[30,    50] loss: 0.095\n",
      "[30,   100] loss: 0.132\n",
      "[30,   150] loss: 0.111\n",
      "[30,   200] loss: 0.111\n",
      "[30,   250] loss: 0.120\n",
      "[31,    50] loss: 0.134\n",
      "[31,   100] loss: 0.166\n",
      "[31,   150] loss: 0.084\n",
      "[31,   200] loss: 0.054\n",
      "[31,   250] loss: 0.124\n",
      "[32,    50] loss: 0.101\n",
      "[32,   100] loss: 0.071\n",
      "[32,   150] loss: 0.107\n",
      "[32,   200] loss: 0.077\n",
      "[32,   250] loss: 0.154\n",
      "[33,    50] loss: 0.038\n",
      "[33,   100] loss: 0.074\n",
      "[33,   150] loss: 0.147\n",
      "[33,   200] loss: 0.106\n",
      "[33,   250] loss: 0.071\n",
      "[34,    50] loss: 0.067\n",
      "[34,   100] loss: 0.083\n",
      "[34,   150] loss: 0.076\n",
      "[34,   200] loss: 0.076\n",
      "[34,   250] loss: 0.151\n",
      "[35,    50] loss: 0.060\n",
      "[35,   100] loss: 0.099\n",
      "[35,   150] loss: 0.085\n",
      "[35,   200] loss: 0.134\n",
      "[35,   250] loss: 0.076\n",
      "[36,    50] loss: 0.062\n",
      "[36,   100] loss: 0.063\n",
      "[36,   150] loss: 0.062\n",
      "[36,   200] loss: 0.131\n",
      "[36,   250] loss: 0.081\n",
      "[37,    50] loss: 0.071\n",
      "[37,   100] loss: 0.061\n",
      "[37,   150] loss: 0.114\n",
      "[37,   200] loss: 0.055\n",
      "[37,   250] loss: 0.088\n",
      "[38,    50] loss: 0.103\n",
      "[38,   100] loss: 0.045\n",
      "[38,   150] loss: 0.083\n",
      "[38,   200] loss: 0.112\n",
      "[38,   250] loss: 0.059\n",
      "[39,    50] loss: 0.073\n",
      "[39,   100] loss: 0.060\n",
      "[39,   150] loss: 0.128\n",
      "[39,   200] loss: 0.112\n",
      "[39,   250] loss: 0.025\n",
      "[40,    50] loss: 0.091\n",
      "[40,   100] loss: 0.028\n",
      "[40,   150] loss: 0.059\n",
      "[40,   200] loss: 0.083\n",
      "[40,   250] loss: 0.098\n",
      "[41,    50] loss: 0.106\n",
      "[41,   100] loss: 0.060\n",
      "[41,   150] loss: 0.069\n",
      "[41,   200] loss: 0.037\n",
      "[41,   250] loss: 0.102\n",
      "[42,    50] loss: 0.079\n",
      "[42,   100] loss: 0.058\n",
      "[42,   150] loss: 0.070\n",
      "[42,   200] loss: 0.065\n",
      "[42,   250] loss: 0.055\n",
      "[43,    50] loss: 0.080\n",
      "[43,   100] loss: 0.067\n",
      "[43,   150] loss: 0.037\n",
      "[43,   200] loss: 0.053\n",
      "[43,   250] loss: 0.058\n",
      "[44,    50] loss: 0.076\n",
      "[44,   100] loss: 0.054\n",
      "[44,   150] loss: 0.047\n",
      "[44,   200] loss: 0.095\n",
      "[44,   250] loss: 0.058\n",
      "[45,    50] loss: 0.055\n",
      "[45,   100] loss: 0.059\n",
      "[45,   150] loss: 0.029\n",
      "[45,   200] loss: 0.098\n",
      "[45,   250] loss: 0.077\n",
      "[46,    50] loss: 0.096\n",
      "[46,   100] loss: 0.069\n",
      "[46,   150] loss: 0.045\n",
      "[46,   200] loss: 0.060\n",
      "[46,   250] loss: 0.044\n",
      "[47,    50] loss: 0.091\n",
      "[47,   100] loss: 0.037\n",
      "[47,   150] loss: 0.074\n",
      "[47,   200] loss: 0.050\n",
      "[47,   250] loss: 0.049\n",
      "[48,    50] loss: 0.051\n",
      "[48,   100] loss: 0.040\n",
      "[48,   150] loss: 0.079\n",
      "[48,   200] loss: 0.077\n",
      "[48,   250] loss: 0.047\n",
      "[49,    50] loss: 0.026\n",
      "[49,   100] loss: 0.096\n",
      "[49,   150] loss: 0.054\n",
      "[49,   200] loss: 0.032\n",
      "[49,   250] loss: 0.092\n",
      "[50,    50] loss: 0.063\n",
      "[50,   100] loss: 0.067\n",
      "[50,   150] loss: 0.039\n",
      "[50,   200] loss: 0.037\n",
      "[50,   250] loss: 0.072\n",
      "[51,    50] loss: 0.055\n",
      "[51,   100] loss: 0.040\n",
      "[51,   150] loss: 0.077\n",
      "[51,   200] loss: 0.033\n",
      "[51,   250] loss: 0.045\n",
      "[52,    50] loss: 0.063\n",
      "[52,   100] loss: 0.027\n",
      "[52,   150] loss: 0.041\n",
      "[52,   200] loss: 0.066\n",
      "[52,   250] loss: 0.075\n",
      "[53,    50] loss: 0.057\n",
      "[53,   100] loss: 0.037\n",
      "[53,   150] loss: 0.055\n",
      "[53,   200] loss: 0.028\n",
      "[53,   250] loss: 0.065\n",
      "[54,    50] loss: 0.045\n",
      "[54,   100] loss: 0.052\n",
      "[54,   150] loss: 0.049\n",
      "[54,   200] loss: 0.037\n",
      "[54,   250] loss: 0.044\n",
      "[55,    50] loss: 0.042\n",
      "[55,   100] loss: 0.029\n",
      "[55,   150] loss: 0.066\n",
      "[55,   200] loss: 0.056\n",
      "[55,   250] loss: 0.035\n",
      "[56,    50] loss: 0.026\n",
      "[56,   100] loss: 0.062\n",
      "[56,   150] loss: 0.022\n",
      "[56,   200] loss: 0.061\n",
      "[56,   250] loss: 0.031\n",
      "[57,    50] loss: 0.035\n",
      "[57,   100] loss: 0.064\n",
      "[57,   150] loss: 0.011\n",
      "[57,   200] loss: 0.083\n",
      "[57,   250] loss: 0.053\n",
      "[58,    50] loss: 0.026\n",
      "[58,   100] loss: 0.038\n",
      "[58,   150] loss: 0.039\n",
      "[58,   200] loss: 0.049\n",
      "[58,   250] loss: 0.059\n",
      "[59,    50] loss: 0.013\n",
      "[59,   100] loss: 0.047\n",
      "[59,   150] loss: 0.048\n",
      "[59,   200] loss: 0.036\n",
      "[59,   250] loss: 0.063\n",
      "[60,    50] loss: 0.050\n",
      "[60,   100] loss: 0.093\n",
      "[60,   150] loss: 0.022\n",
      "[60,   200] loss: 0.029\n",
      "[60,   250] loss: 0.050\n",
      "[61,    50] loss: 0.030\n",
      "[61,   100] loss: 0.058\n",
      "[61,   150] loss: 0.025\n",
      "[61,   200] loss: 0.028\n",
      "[61,   250] loss: 0.053\n",
      "[62,    50] loss: 0.053\n",
      "[62,   100] loss: 0.047\n",
      "[62,   150] loss: 0.049\n",
      "[62,   200] loss: 0.029\n",
      "[62,   250] loss: 0.030\n",
      "[63,    50] loss: 0.030\n",
      "[63,   100] loss: 0.021\n",
      "[63,   150] loss: 0.035\n",
      "[63,   200] loss: 0.060\n",
      "[63,   250] loss: 0.053\n",
      "[64,    50] loss: 0.040\n",
      "[64,   100] loss: 0.051\n",
      "[64,   150] loss: 0.024\n",
      "[64,   200] loss: 0.050\n",
      "[64,   250] loss: 0.028\n",
      "[65,    50] loss: 0.053\n",
      "[65,   100] loss: 0.037\n",
      "[65,   150] loss: 0.021\n",
      "[65,   200] loss: 0.042\n",
      "[65,   250] loss: 0.044\n",
      "[66,    50] loss: 0.029\n",
      "[66,   100] loss: 0.055\n",
      "[66,   150] loss: 0.022\n",
      "[66,   200] loss: 0.037\n",
      "[66,   250] loss: 0.023\n",
      "[67,    50] loss: 0.021\n",
      "[67,   100] loss: 0.047\n",
      "[67,   150] loss: 0.029\n",
      "[67,   200] loss: 0.034\n",
      "[67,   250] loss: 0.057\n",
      "[68,    50] loss: 0.032\n",
      "[68,   100] loss: 0.011\n",
      "[68,   150] loss: 0.050\n",
      "[68,   200] loss: 0.042\n",
      "[68,   250] loss: 0.055\n",
      "[69,    50] loss: 0.037\n",
      "[69,   100] loss: 0.025\n",
      "[69,   150] loss: 0.059\n",
      "[69,   200] loss: 0.051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69,   250] loss: 0.018\n",
      "[70,    50] loss: 0.055\n",
      "[70,   100] loss: 0.010\n",
      "[70,   150] loss: 0.048\n",
      "[70,   200] loss: 0.035\n",
      "[70,   250] loss: 0.052\n",
      "[71,    50] loss: 0.050\n",
      "[71,   100] loss: 0.049\n",
      "[71,   150] loss: 0.033\n",
      "[71,   200] loss: 0.014\n",
      "[71,   250] loss: 0.024\n",
      "[72,    50] loss: 0.046\n",
      "[72,   100] loss: 0.009\n",
      "[72,   150] loss: 0.042\n",
      "[72,   200] loss: 0.042\n",
      "[72,   250] loss: 0.020\n",
      "[73,    50] loss: 0.043\n",
      "[73,   100] loss: 0.037\n",
      "[73,   150] loss: 0.027\n",
      "[73,   200] loss: 0.045\n",
      "[73,   250] loss: 0.021\n",
      "[74,    50] loss: 0.033\n",
      "[74,   100] loss: 0.039\n",
      "[74,   150] loss: 0.074\n",
      "[74,   200] loss: 0.036\n",
      "[74,   250] loss: 0.010\n",
      "[75,    50] loss: 0.025\n",
      "[75,   100] loss: 0.022\n",
      "[75,   150] loss: 0.053\n",
      "[75,   200] loss: 0.020\n",
      "[75,   250] loss: 0.013\n",
      "[76,    50] loss: 0.026\n",
      "[76,   100] loss: 0.020\n",
      "[76,   150] loss: 0.026\n",
      "[76,   200] loss: 0.049\n",
      "[76,   250] loss: 0.030\n",
      "[77,    50] loss: 0.013\n",
      "[77,   100] loss: 0.027\n",
      "[77,   150] loss: 0.031\n",
      "[77,   200] loss: 0.026\n",
      "[77,   250] loss: 0.048\n",
      "[78,    50] loss: 0.032\n",
      "[78,   100] loss: 0.017\n",
      "[78,   150] loss: 0.023\n",
      "[78,   200] loss: 0.024\n",
      "[78,   250] loss: 0.045\n",
      "[79,    50] loss: 0.014\n",
      "[79,   100] loss: 0.050\n",
      "[79,   150] loss: 0.007\n",
      "[79,   200] loss: 0.042\n",
      "[79,   250] loss: 0.044\n",
      "[80,    50] loss: 0.032\n",
      "[80,   100] loss: 0.024\n",
      "[80,   150] loss: 0.012\n",
      "[80,   200] loss: 0.035\n",
      "[80,   250] loss: 0.042\n",
      "[81,    50] loss: 0.018\n",
      "[81,   100] loss: 0.018\n",
      "[81,   150] loss: 0.021\n",
      "[81,   200] loss: 0.046\n",
      "[81,   250] loss: 0.022\n",
      "[82,    50] loss: 0.029\n",
      "[82,   100] loss: 0.022\n",
      "[82,   150] loss: 0.043\n",
      "[82,   200] loss: 0.037\n",
      "[82,   250] loss: 0.013\n",
      "[83,    50] loss: 0.011\n",
      "[83,   100] loss: 0.032\n",
      "[83,   150] loss: 0.040\n",
      "[83,   200] loss: 0.038\n",
      "[83,   250] loss: 0.011\n",
      "[84,    50] loss: 0.018\n",
      "[84,   100] loss: 0.028\n",
      "[84,   150] loss: 0.010\n",
      "[84,   200] loss: 0.023\n",
      "[84,   250] loss: 0.025\n",
      "[85,    50] loss: 0.029\n",
      "[85,   100] loss: 0.035\n",
      "[85,   150] loss: 0.010\n",
      "[85,   200] loss: 0.007\n",
      "[85,   250] loss: 0.073\n",
      "[86,    50] loss: 0.017\n",
      "[86,   100] loss: 0.040\n",
      "[86,   150] loss: 0.025\n",
      "[86,   200] loss: 0.007\n",
      "[86,   250] loss: 0.038\n",
      "[87,    50] loss: 0.023\n",
      "[87,   100] loss: 0.037\n",
      "[87,   150] loss: 0.010\n",
      "[87,   200] loss: 0.010\n",
      "[87,   250] loss: 0.035\n",
      "[88,    50] loss: 0.030\n",
      "[88,   100] loss: 0.052\n",
      "[88,   150] loss: 0.016\n",
      "[88,   200] loss: 0.023\n",
      "[88,   250] loss: 0.042\n",
      "[89,    50] loss: 0.045\n",
      "[89,   100] loss: 0.013\n",
      "[89,   150] loss: 0.024\n",
      "[89,   200] loss: 0.022\n",
      "[89,   250] loss: 0.003\n",
      "[90,    50] loss: 0.009\n",
      "[90,   100] loss: 0.028\n",
      "[90,   150] loss: 0.053\n",
      "[90,   200] loss: 0.021\n",
      "[90,   250] loss: 0.030\n",
      "[91,    50] loss: 0.024\n",
      "[91,   100] loss: 0.018\n",
      "[91,   150] loss: 0.021\n",
      "[91,   200] loss: 0.028\n",
      "[91,   250] loss: 0.020\n",
      "[92,    50] loss: 0.034\n",
      "[92,   100] loss: 0.028\n",
      "[92,   150] loss: 0.015\n",
      "[92,   200] loss: 0.041\n",
      "[92,   250] loss: 0.019\n",
      "[93,    50] loss: 0.019\n",
      "[93,   100] loss: 0.011\n",
      "[93,   150] loss: 0.014\n",
      "[93,   200] loss: 0.024\n",
      "[93,   250] loss: 0.031\n",
      "[94,    50] loss: 0.016\n",
      "[94,   100] loss: 0.030\n",
      "[94,   150] loss: 0.029\n",
      "[94,   200] loss: 0.029\n",
      "[94,   250] loss: 0.033\n",
      "[95,    50] loss: 0.043\n",
      "[95,   100] loss: 0.022\n",
      "[95,   150] loss: 0.037\n",
      "[95,   200] loss: 0.035\n",
      "[95,   250] loss: 0.016\n",
      "[96,    50] loss: 0.019\n",
      "[96,   100] loss: 0.005\n",
      "[96,   150] loss: 0.019\n",
      "[96,   200] loss: 0.032\n",
      "[96,   250] loss: 0.026\n",
      "[97,    50] loss: 0.017\n",
      "[97,   100] loss: 0.015\n",
      "[97,   150] loss: 0.028\n",
      "[97,   200] loss: 0.016\n",
      "[97,   250] loss: 0.019\n",
      "[98,    50] loss: 0.015\n",
      "[98,   100] loss: 0.016\n",
      "[98,   150] loss: 0.047\n",
      "[98,   200] loss: 0.010\n",
      "[98,   250] loss: 0.012\n",
      "[99,    50] loss: 0.019\n",
      "[99,   100] loss: 0.013\n",
      "[99,   150] loss: 0.043\n",
      "[99,   200] loss: 0.011\n",
      "[99,   250] loss: 0.017\n",
      "[100,    50] loss: 0.025\n",
      "[100,   100] loss: 0.032\n",
      "[100,   150] loss: 0.013\n",
      "[100,   200] loss: 0.021\n",
      "[100,   250] loss: 0.007\n",
      "[101,    50] loss: 0.032\n",
      "[101,   100] loss: 0.015\n",
      "[101,   150] loss: 0.028\n",
      "[101,   200] loss: 0.024\n",
      "[101,   250] loss: 0.015\n",
      "[102,    50] loss: 0.010\n",
      "[102,   100] loss: 0.011\n",
      "[102,   150] loss: 0.006\n",
      "[102,   200] loss: 0.015\n",
      "[102,   250] loss: 0.027\n",
      "[103,    50] loss: 0.011\n",
      "[103,   100] loss: 0.009\n",
      "[103,   150] loss: 0.014\n",
      "[103,   200] loss: 0.039\n",
      "[103,   250] loss: 0.020\n",
      "[104,    50] loss: 0.015\n",
      "[104,   100] loss: 0.011\n",
      "[104,   150] loss: 0.020\n",
      "[104,   200] loss: 0.022\n",
      "[104,   250] loss: 0.039\n",
      "[105,    50] loss: 0.026\n",
      "[105,   100] loss: 0.018\n",
      "[105,   150] loss: 0.008\n",
      "[105,   200] loss: 0.026\n",
      "[105,   250] loss: 0.022\n",
      "[106,    50] loss: 0.044\n",
      "[106,   100] loss: 0.015\n",
      "[106,   150] loss: 0.006\n",
      "[106,   200] loss: 0.035\n",
      "[106,   250] loss: 0.008\n",
      "[107,    50] loss: 0.005\n",
      "[107,   100] loss: 0.024\n",
      "[107,   150] loss: 0.013\n",
      "[107,   200] loss: 0.061\n",
      "[107,   250] loss: 0.007\n",
      "[108,    50] loss: 0.030\n",
      "[108,   100] loss: 0.021\n",
      "[108,   150] loss: 0.036\n",
      "[108,   200] loss: 0.009\n",
      "[108,   250] loss: 0.006\n",
      "[109,    50] loss: 0.038\n",
      "[109,   100] loss: 0.038\n",
      "[109,   150] loss: 0.004\n",
      "[109,   200] loss: 0.020\n",
      "[109,   250] loss: 0.004\n",
      "[110,    50] loss: 0.010\n",
      "[110,   100] loss: 0.011\n",
      "[110,   150] loss: 0.005\n",
      "[110,   200] loss: 0.020\n",
      "[110,   250] loss: 0.030\n",
      "[111,    50] loss: 0.008\n",
      "[111,   100] loss: 0.029\n",
      "[111,   150] loss: 0.010\n",
      "[111,   200] loss: 0.019\n",
      "[111,   250] loss: 0.025\n",
      "[112,    50] loss: 0.012\n",
      "[112,   100] loss: 0.033\n",
      "[112,   150] loss: 0.010\n",
      "[112,   200] loss: 0.008\n",
      "[112,   250] loss: 0.018\n",
      "[113,    50] loss: 0.010\n",
      "[113,   100] loss: 0.012\n",
      "[113,   150] loss: 0.015\n",
      "[113,   200] loss: 0.031\n",
      "[113,   250] loss: 0.008\n",
      "[114,    50] loss: 0.005\n",
      "[114,   100] loss: 0.034\n",
      "[114,   150] loss: 0.011\n",
      "[114,   200] loss: 0.013\n",
      "[114,   250] loss: 0.014\n",
      "[115,    50] loss: 0.017\n",
      "[115,   100] loss: 0.016\n",
      "[115,   150] loss: 0.012\n",
      "[115,   200] loss: 0.013\n",
      "[115,   250] loss: 0.003\n",
      "[116,    50] loss: 0.018\n",
      "[116,   100] loss: 0.027\n",
      "[116,   150] loss: 0.019\n",
      "[116,   200] loss: 0.017\n",
      "[116,   250] loss: 0.014\n",
      "[117,    50] loss: 0.022\n",
      "[117,   100] loss: 0.024\n",
      "[117,   150] loss: 0.011\n",
      "[117,   200] loss: 0.047\n",
      "[117,   250] loss: 0.005\n",
      "[118,    50] loss: 0.012\n",
      "[118,   100] loss: 0.006\n",
      "[118,   150] loss: 0.018\n",
      "[118,   200] loss: 0.008\n",
      "[118,   250] loss: 0.039\n",
      "[119,    50] loss: 0.026\n",
      "[119,   100] loss: 0.004\n",
      "[119,   150] loss: 0.018\n",
      "[119,   200] loss: 0.009\n",
      "[119,   250] loss: 0.019\n",
      "[120,    50] loss: 0.018\n",
      "[120,   100] loss: 0.014\n",
      "[120,   150] loss: 0.027\n",
      "[120,   200] loss: 0.024\n",
      "[120,   250] loss: 0.015\n",
      "[121,    50] loss: 0.021\n",
      "[121,   100] loss: 0.002\n",
      "[121,   150] loss: 0.026\n",
      "[121,   200] loss: 0.011\n",
      "[121,   250] loss: 0.016\n",
      "[122,    50] loss: 0.017\n",
      "[122,   100] loss: 0.003\n",
      "[122,   150] loss: 0.018\n",
      "[122,   200] loss: 0.028\n",
      "[122,   250] loss: 0.011\n",
      "[123,    50] loss: 0.004\n",
      "[123,   100] loss: 0.007\n",
      "[123,   150] loss: 0.015\n",
      "[123,   200] loss: 0.033\n",
      "[123,   250] loss: 0.007\n",
      "[124,    50] loss: 0.007\n",
      "[124,   100] loss: 0.006\n",
      "[124,   150] loss: 0.026\n",
      "[124,   200] loss: 0.012\n",
      "[124,   250] loss: 0.028\n",
      "[125,    50] loss: 0.004\n",
      "[125,   100] loss: 0.017\n",
      "[125,   150] loss: 0.029\n",
      "[125,   200] loss: 0.012\n",
      "[125,   250] loss: 0.019\n",
      "[126,    50] loss: 0.026\n",
      "[126,   100] loss: 0.007\n",
      "[126,   150] loss: 0.015\n",
      "[126,   200] loss: 0.033\n",
      "[126,   250] loss: 0.006\n",
      "[127,    50] loss: 0.010\n",
      "[127,   100] loss: 0.004\n",
      "[127,   150] loss: 0.022\n",
      "[127,   200] loss: 0.003\n",
      "[127,   250] loss: 0.044\n",
      "[128,    50] loss: 0.017\n",
      "[128,   100] loss: 0.011\n",
      "[128,   150] loss: 0.029\n",
      "[128,   200] loss: 0.014\n",
      "[128,   250] loss: 0.018\n",
      "[129,    50] loss: 0.010\n",
      "[129,   100] loss: 0.021\n",
      "[129,   150] loss: 0.020\n",
      "[129,   200] loss: 0.010\n",
      "[129,   250] loss: 0.018\n",
      "[130,    50] loss: 0.009\n",
      "[130,   100] loss: 0.004\n",
      "[130,   150] loss: 0.014\n",
      "[130,   200] loss: 0.021\n",
      "[130,   250] loss: 0.012\n",
      "[131,    50] loss: 0.008\n",
      "[131,   100] loss: 0.024\n",
      "[131,   150] loss: 0.011\n",
      "[131,   200] loss: 0.018\n",
      "[131,   250] loss: 0.007\n",
      "[132,    50] loss: 0.021\n",
      "[132,   100] loss: 0.007\n",
      "[132,   150] loss: 0.007\n",
      "[132,   200] loss: 0.027\n",
      "[132,   250] loss: 0.005\n",
      "[133,    50] loss: 0.013\n",
      "[133,   100] loss: 0.006\n",
      "[133,   150] loss: 0.007\n",
      "[133,   200] loss: 0.045\n",
      "[133,   250] loss: 0.010\n",
      "[134,    50] loss: 0.006\n",
      "[134,   100] loss: 0.002\n",
      "[134,   150] loss: 0.031\n",
      "[134,   200] loss: 0.005\n",
      "[134,   250] loss: 0.011\n",
      "[135,    50] loss: 0.007\n",
      "[135,   100] loss: 0.008\n",
      "[135,   150] loss: 0.004\n",
      "[135,   200] loss: 0.007\n",
      "[135,   250] loss: 0.036\n",
      "[136,    50] loss: 0.008\n",
      "[136,   100] loss: 0.021\n",
      "[136,   150] loss: 0.010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[136,   200] loss: 0.011\n",
      "[136,   250] loss: 0.024\n",
      "[137,    50] loss: 0.042\n",
      "[137,   100] loss: 0.004\n",
      "[137,   150] loss: 0.005\n",
      "[137,   200] loss: 0.016\n",
      "[137,   250] loss: 0.005\n",
      "[138,    50] loss: 0.006\n",
      "[138,   100] loss: 0.042\n",
      "[138,   150] loss: 0.010\n",
      "[138,   200] loss: 0.008\n",
      "[138,   250] loss: 0.004\n",
      "[139,    50] loss: 0.006\n",
      "[139,   100] loss: 0.029\n",
      "[139,   150] loss: 0.007\n",
      "[139,   200] loss: 0.014\n",
      "[139,   250] loss: 0.003\n",
      "[140,    50] loss: 0.007\n",
      "[140,   100] loss: 0.007\n",
      "[140,   150] loss: 0.025\n",
      "[140,   200] loss: 0.011\n",
      "[140,   250] loss: 0.011\n",
      "[141,    50] loss: 0.016\n",
      "[141,   100] loss: 0.012\n",
      "[141,   150] loss: 0.010\n",
      "[141,   200] loss: 0.005\n",
      "[141,   250] loss: 0.008\n",
      "[142,    50] loss: 0.014\n",
      "[142,   100] loss: 0.011\n",
      "[142,   150] loss: 0.003\n",
      "[142,   200] loss: 0.005\n",
      "[142,   250] loss: 0.022\n",
      "[143,    50] loss: 0.025\n",
      "[143,   100] loss: 0.005\n",
      "[143,   150] loss: 0.016\n",
      "[143,   200] loss: 0.004\n",
      "[143,   250] loss: 0.007\n",
      "[144,    50] loss: 0.006\n",
      "[144,   100] loss: 0.005\n",
      "[144,   150] loss: 0.013\n",
      "[144,   200] loss: 0.009\n",
      "[144,   250] loss: 0.016\n",
      "[145,    50] loss: 0.006\n",
      "[145,   100] loss: 0.012\n",
      "[145,   150] loss: 0.016\n",
      "[145,   200] loss: 0.014\n",
      "[145,   250] loss: 0.011\n",
      "[146,    50] loss: 0.013\n",
      "[146,   100] loss: 0.008\n",
      "[146,   150] loss: 0.003\n",
      "[146,   200] loss: 0.011\n",
      "[146,   250] loss: 0.020\n",
      "[147,    50] loss: 0.009\n",
      "[147,   100] loss: 0.004\n",
      "[147,   150] loss: 0.014\n",
      "[147,   200] loss: 0.023\n",
      "[147,   250] loss: 0.010\n",
      "[148,    50] loss: 0.004\n",
      "[148,   100] loss: 0.004\n",
      "[148,   150] loss: 0.006\n",
      "[148,   200] loss: 0.018\n",
      "[148,   250] loss: 0.015\n",
      "[149,    50] loss: 0.020\n",
      "[149,   100] loss: 0.004\n",
      "[149,   150] loss: 0.004\n",
      "[149,   200] loss: 0.009\n",
      "[149,   250] loss: 0.011\n",
      "[150,    50] loss: 0.018\n",
      "[150,   100] loss: 0.012\n",
      "[150,   150] loss: 0.006\n",
      "[150,   200] loss: 0.006\n",
      "[150,   250] loss: 0.013\n",
      "[151,    50] loss: 0.008\n",
      "[151,   100] loss: 0.003\n",
      "[151,   150] loss: 0.023\n",
      "[151,   200] loss: 0.024\n",
      "[151,   250] loss: 0.013\n",
      "[152,    50] loss: 0.012\n",
      "[152,   100] loss: 0.007\n",
      "[152,   150] loss: 0.007\n",
      "[152,   200] loss: 0.015\n",
      "[152,   250] loss: 0.024\n",
      "[153,    50] loss: 0.007\n",
      "[153,   100] loss: 0.008\n",
      "[153,   150] loss: 0.032\n",
      "[153,   200] loss: 0.008\n",
      "[153,   250] loss: 0.003\n",
      "[154,    50] loss: 0.007\n",
      "[154,   100] loss: 0.005\n",
      "[154,   150] loss: 0.004\n",
      "[154,   200] loss: 0.003\n",
      "[154,   250] loss: 0.049\n",
      "[155,    50] loss: 0.021\n",
      "[155,   100] loss: 0.011\n",
      "[155,   150] loss: 0.007\n",
      "[155,   200] loss: 0.006\n",
      "[155,   250] loss: 0.030\n",
      "[156,    50] loss: 0.007\n",
      "[156,   100] loss: 0.001\n",
      "[156,   150] loss: 0.007\n",
      "[156,   200] loss: 0.012\n",
      "[156,   250] loss: 0.016\n",
      "[157,    50] loss: 0.005\n",
      "[157,   100] loss: 0.005\n",
      "[157,   150] loss: 0.011\n",
      "[157,   200] loss: 0.011\n",
      "[157,   250] loss: 0.023\n",
      "[158,    50] loss: 0.016\n",
      "[158,   100] loss: 0.010\n",
      "[158,   150] loss: 0.017\n",
      "[158,   200] loss: 0.005\n",
      "[158,   250] loss: 0.005\n",
      "[159,    50] loss: 0.011\n",
      "[159,   100] loss: 0.001\n",
      "[159,   150] loss: 0.005\n",
      "[159,   200] loss: 0.022\n",
      "[159,   250] loss: 0.022\n",
      "[160,    50] loss: 0.005\n",
      "[160,   100] loss: 0.001\n",
      "[160,   150] loss: 0.018\n",
      "[160,   200] loss: 0.027\n",
      "[160,   250] loss: 0.007\n",
      "[161,    50] loss: 0.003\n",
      "[161,   100] loss: 0.009\n",
      "[161,   150] loss: 0.029\n",
      "[161,   200] loss: 0.004\n",
      "[161,   250] loss: 0.013\n",
      "[162,    50] loss: 0.004\n",
      "[162,   100] loss: 0.003\n",
      "[162,   150] loss: 0.004\n",
      "[162,   200] loss: 0.010\n",
      "[162,   250] loss: 0.004\n",
      "[163,    50] loss: 0.002\n",
      "[163,   100] loss: 0.005\n",
      "[163,   150] loss: 0.017\n",
      "[163,   200] loss: 0.019\n",
      "[163,   250] loss: 0.008\n",
      "[164,    50] loss: 0.002\n",
      "[164,   100] loss: 0.023\n",
      "[164,   150] loss: 0.010\n",
      "[164,   200] loss: 0.017\n",
      "[164,   250] loss: 0.002\n",
      "[165,    50] loss: 0.002\n",
      "[165,   100] loss: 0.005\n",
      "[165,   150] loss: 0.006\n",
      "[165,   200] loss: 0.030\n",
      "[165,   250] loss: 0.004\n",
      "[166,    50] loss: 0.007\n",
      "[166,   100] loss: 0.006\n",
      "[166,   150] loss: 0.003\n",
      "[166,   200] loss: 0.022\n",
      "[166,   250] loss: 0.007\n",
      "[167,    50] loss: 0.011\n",
      "[167,   100] loss: 0.030\n",
      "[167,   150] loss: 0.036\n",
      "[167,   200] loss: 0.013\n",
      "[167,   250] loss: 0.008\n",
      "[168,    50] loss: 0.027\n",
      "[168,   100] loss: 0.005\n",
      "[168,   150] loss: 0.006\n",
      "[168,   200] loss: 0.004\n",
      "[168,   250] loss: 0.057\n",
      "[169,    50] loss: 0.011\n",
      "[169,   100] loss: 0.009\n",
      "[169,   150] loss: 0.007\n",
      "[169,   200] loss: 0.002\n",
      "[169,   250] loss: 0.015\n",
      "[170,    50] loss: 0.010\n",
      "[170,   100] loss: 0.007\n",
      "[170,   150] loss: 0.005\n",
      "[170,   200] loss: 0.003\n",
      "[170,   250] loss: 0.026\n",
      "[171,    50] loss: 0.004\n",
      "[171,   100] loss: 0.003\n",
      "[171,   150] loss: 0.004\n",
      "[171,   200] loss: 0.028\n",
      "[171,   250] loss: 0.005\n",
      "[172,    50] loss: 0.012\n",
      "[172,   100] loss: 0.002\n",
      "[172,   150] loss: 0.004\n",
      "[172,   200] loss: 0.005\n",
      "[172,   250] loss: 0.003\n",
      "[173,    50] loss: 0.007\n",
      "[173,   100] loss: 0.001\n",
      "[173,   150] loss: 0.007\n",
      "[173,   200] loss: 0.027\n",
      "[173,   250] loss: 0.004\n",
      "[174,    50] loss: 0.001\n",
      "[174,   100] loss: 0.003\n",
      "[174,   150] loss: 0.011\n",
      "[174,   200] loss: 0.016\n",
      "[174,   250] loss: 0.002\n",
      "[175,    50] loss: 0.009\n",
      "[175,   100] loss: 0.014\n",
      "[175,   150] loss: 0.003\n",
      "[175,   200] loss: 0.009\n",
      "[175,   250] loss: 0.016\n",
      "[176,    50] loss: 0.008\n",
      "[176,   100] loss: 0.049\n",
      "[176,   150] loss: 0.004\n",
      "[176,   200] loss: 0.002\n",
      "[176,   250] loss: 0.011\n",
      "[177,    50] loss: 0.003\n",
      "[177,   100] loss: 0.010\n",
      "[177,   150] loss: 0.024\n",
      "[177,   200] loss: 0.007\n",
      "[177,   250] loss: 0.005\n",
      "[178,    50] loss: 0.011\n",
      "[178,   100] loss: 0.005\n",
      "[178,   150] loss: 0.004\n",
      "[178,   200] loss: 0.003\n",
      "[178,   250] loss: 0.010\n",
      "[179,    50] loss: 0.008\n",
      "[179,   100] loss: 0.017\n",
      "[179,   150] loss: 0.011\n",
      "[179,   200] loss: 0.015\n",
      "[179,   250] loss: 0.015\n",
      "[180,    50] loss: 0.019\n",
      "[180,   100] loss: 0.006\n",
      "[180,   150] loss: 0.007\n",
      "[180,   200] loss: 0.013\n",
      "[180,   250] loss: 0.001\n",
      "[181,    50] loss: 0.004\n",
      "[181,   100] loss: 0.002\n",
      "[181,   150] loss: 0.021\n",
      "[181,   200] loss: 0.001\n",
      "[181,   250] loss: 0.017\n",
      "[182,    50] loss: 0.019\n",
      "[182,   100] loss: 0.017\n",
      "[182,   150] loss: 0.012\n",
      "[182,   200] loss: 0.004\n",
      "[182,   250] loss: 0.002\n",
      "[183,    50] loss: 0.003\n",
      "[183,   100] loss: 0.008\n",
      "[183,   150] loss: 0.011\n",
      "[183,   200] loss: 0.008\n",
      "[183,   250] loss: 0.008\n",
      "[184,    50] loss: 0.011\n",
      "[184,   100] loss: 0.002\n",
      "[184,   150] loss: 0.003\n",
      "[184,   200] loss: 0.005\n",
      "[184,   250] loss: 0.017\n",
      "[185,    50] loss: 0.005\n",
      "[185,   100] loss: 0.007\n",
      "[185,   150] loss: 0.017\n",
      "[185,   200] loss: 0.004\n",
      "[185,   250] loss: 0.013\n",
      "[186,    50] loss: 0.008\n",
      "[186,   100] loss: 0.000\n",
      "[186,   150] loss: 0.009\n",
      "[186,   200] loss: 0.008\n",
      "[186,   250] loss: 0.014\n",
      "[187,    50] loss: 0.004\n",
      "[187,   100] loss: 0.004\n",
      "[187,   150] loss: 0.007\n",
      "[187,   200] loss: 0.002\n",
      "[187,   250] loss: 0.032\n",
      "[188,    50] loss: 0.009\n",
      "[188,   100] loss: 0.007\n",
      "[188,   150] loss: 0.007\n",
      "[188,   200] loss: 0.011\n",
      "[188,   250] loss: 0.007\n",
      "[189,    50] loss: 0.003\n",
      "[189,   100] loss: 0.006\n",
      "[189,   150] loss: 0.003\n",
      "[189,   200] loss: 0.006\n",
      "[189,   250] loss: 0.023\n",
      "[190,    50] loss: 0.024\n",
      "[190,   100] loss: 0.013\n",
      "[190,   150] loss: 0.017\n",
      "[190,   200] loss: 0.004\n",
      "[190,   250] loss: 0.004\n",
      "[191,    50] loss: 0.007\n",
      "[191,   100] loss: 0.002\n",
      "[191,   150] loss: 0.003\n",
      "[191,   200] loss: 0.005\n",
      "[191,   250] loss: 0.018\n",
      "[192,    50] loss: 0.004\n",
      "[192,   100] loss: 0.001\n",
      "[192,   150] loss: 0.006\n",
      "[192,   200] loss: 0.018\n",
      "[192,   250] loss: 0.006\n",
      "[193,    50] loss: 0.010\n",
      "[193,   100] loss: 0.004\n",
      "[193,   150] loss: 0.002\n",
      "[193,   200] loss: 0.017\n",
      "[193,   250] loss: 0.006\n",
      "[194,    50] loss: 0.007\n",
      "[194,   100] loss: 0.003\n",
      "[194,   150] loss: 0.004\n",
      "[194,   200] loss: 0.022\n",
      "[194,   250] loss: 0.004\n",
      "[195,    50] loss: 0.005\n",
      "[195,   100] loss: 0.006\n",
      "[195,   150] loss: 0.004\n",
      "[195,   200] loss: 0.003\n",
      "[195,   250] loss: 0.007\n",
      "[196,    50] loss: 0.005\n",
      "[196,   100] loss: 0.001\n",
      "[196,   150] loss: 0.007\n",
      "[196,   200] loss: 0.006\n",
      "[196,   250] loss: 0.014\n",
      "[197,    50] loss: 0.006\n",
      "[197,   100] loss: 0.010\n",
      "[197,   150] loss: 0.008\n",
      "[197,   200] loss: 0.007\n",
      "[197,   250] loss: 0.001\n",
      "[198,    50] loss: 0.003\n",
      "[198,   100] loss: 0.008\n",
      "[198,   150] loss: 0.004\n",
      "[198,   200] loss: 0.017\n",
      "[198,   250] loss: 0.013\n",
      "[199,    50] loss: 0.009\n",
      "[199,   100] loss: 0.010\n",
      "[199,   150] loss: 0.003\n",
      "[199,   200] loss: 0.012\n",
      "[199,   250] loss: 0.005\n",
      "[200,    50] loss: 0.007\n",
      "[200,   100] loss: 0.002\n",
      "[200,   150] loss: 0.016\n",
      "[200,   200] loss: 0.005\n",
      "[200,   250] loss: 0.017\n",
      "[201,    50] loss: 0.004\n",
      "[201,   100] loss: 0.012\n",
      "[201,   150] loss: 0.002\n",
      "[201,   200] loss: 0.003\n",
      "[201,   250] loss: 0.020\n",
      "[202,    50] loss: 0.016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[202,   100] loss: 0.008\n",
      "[202,   150] loss: 0.004\n",
      "[202,   200] loss: 0.008\n",
      "[202,   250] loss: 0.002\n",
      "[203,    50] loss: 0.017\n",
      "[203,   100] loss: 0.002\n",
      "[203,   150] loss: 0.011\n",
      "[203,   200] loss: 0.009\n",
      "[203,   250] loss: 0.003\n",
      "[204,    50] loss: 0.004\n",
      "[204,   100] loss: 0.004\n",
      "[204,   150] loss: 0.023\n",
      "[204,   200] loss: 0.008\n",
      "[204,   250] loss: 0.001\n",
      "[205,    50] loss: 0.001\n",
      "[205,   100] loss: 0.003\n",
      "[205,   150] loss: 0.002\n",
      "[205,   200] loss: 0.013\n",
      "[205,   250] loss: 0.008\n",
      "[206,    50] loss: 0.008\n",
      "[206,   100] loss: 0.002\n",
      "[206,   150] loss: 0.005\n",
      "[206,   200] loss: 0.001\n",
      "[206,   250] loss: 0.006\n",
      "[207,    50] loss: 0.001\n",
      "[207,   100] loss: 0.007\n",
      "[207,   150] loss: 0.003\n",
      "[207,   200] loss: 0.007\n",
      "[207,   250] loss: 0.005\n",
      "[208,    50] loss: 0.006\n",
      "[208,   100] loss: 0.006\n",
      "[208,   150] loss: 0.007\n",
      "[208,   200] loss: 0.014\n",
      "[208,   250] loss: 0.002\n",
      "[209,    50] loss: 0.004\n",
      "[209,   100] loss: 0.002\n",
      "[209,   150] loss: 0.024\n",
      "[209,   200] loss: 0.004\n",
      "[209,   250] loss: 0.001\n",
      "[210,    50] loss: 0.002\n",
      "[210,   100] loss: 0.010\n",
      "[210,   150] loss: 0.006\n",
      "[210,   200] loss: 0.005\n",
      "[210,   250] loss: 0.008\n",
      "[211,    50] loss: 0.003\n",
      "[211,   100] loss: 0.013\n",
      "[211,   150] loss: 0.013\n",
      "[211,   200] loss: 0.003\n",
      "[211,   250] loss: 0.002\n",
      "[212,    50] loss: 0.003\n",
      "[212,   100] loss: 0.002\n",
      "[212,   150] loss: 0.004\n",
      "[212,   200] loss: 0.005\n",
      "[212,   250] loss: 0.004\n",
      "[213,    50] loss: 0.005\n",
      "[213,   100] loss: 0.003\n",
      "[213,   150] loss: 0.002\n",
      "[213,   200] loss: 0.009\n",
      "[213,   250] loss: 0.007\n",
      "[214,    50] loss: 0.012\n",
      "[214,   100] loss: 0.002\n",
      "[214,   150] loss: 0.002\n",
      "[214,   200] loss: 0.015\n",
      "[214,   250] loss: 0.005\n",
      "[215,    50] loss: 0.004\n",
      "[215,   100] loss: 0.014\n",
      "[215,   150] loss: 0.003\n",
      "[215,   200] loss: 0.006\n",
      "[215,   250] loss: 0.000\n",
      "[216,    50] loss: 0.004\n",
      "[216,   100] loss: 0.001\n",
      "[216,   150] loss: 0.001\n",
      "[216,   200] loss: 0.007\n",
      "[216,   250] loss: 0.004\n",
      "[217,    50] loss: 0.006\n",
      "[217,   100] loss: 0.007\n",
      "[217,   150] loss: 0.008\n",
      "[217,   200] loss: 0.003\n",
      "[217,   250] loss: 0.004\n",
      "[218,    50] loss: 0.002\n",
      "[218,   100] loss: 0.022\n",
      "[218,   150] loss: 0.002\n",
      "[218,   200] loss: 0.001\n",
      "[218,   250] loss: 0.008\n",
      "[219,    50] loss: 0.005\n",
      "[219,   100] loss: 0.011\n",
      "[219,   150] loss: 0.004\n",
      "[219,   200] loss: 0.003\n",
      "[219,   250] loss: 0.001\n",
      "[220,    50] loss: 0.002\n",
      "[220,   100] loss: 0.000\n",
      "[220,   150] loss: 0.002\n",
      "[220,   200] loss: 0.007\n",
      "[220,   250] loss: 0.002\n",
      "[221,    50] loss: 0.009\n",
      "[221,   100] loss: 0.002\n",
      "[221,   150] loss: 0.006\n",
      "[221,   200] loss: 0.014\n",
      "[221,   250] loss: 0.002\n",
      "[222,    50] loss: 0.003\n",
      "[222,   100] loss: 0.021\n",
      "[222,   150] loss: 0.005\n",
      "[222,   200] loss: 0.004\n",
      "[222,   250] loss: 0.003\n",
      "[223,    50] loss: 0.003\n",
      "[223,   100] loss: 0.021\n",
      "[223,   150] loss: 0.005\n",
      "[223,   200] loss: 0.003\n",
      "[223,   250] loss: 0.003\n",
      "[224,    50] loss: 0.002\n",
      "[224,   100] loss: 0.014\n",
      "[224,   150] loss: 0.009\n",
      "[224,   200] loss: 0.004\n",
      "[224,   250] loss: 0.001\n",
      "[225,    50] loss: 0.017\n",
      "[225,   100] loss: 0.010\n",
      "[225,   150] loss: 0.004\n",
      "[225,   200] loss: 0.002\n",
      "[225,   250] loss: 0.003\n",
      "[226,    50] loss: 0.007\n",
      "[226,   100] loss: 0.001\n",
      "[226,   150] loss: 0.002\n",
      "[226,   200] loss: 0.002\n",
      "[226,   250] loss: 0.014\n",
      "[227,    50] loss: 0.004\n",
      "[227,   100] loss: 0.004\n",
      "[227,   150] loss: 0.006\n",
      "[227,   200] loss: 0.003\n",
      "[227,   250] loss: 0.013\n",
      "[228,    50] loss: 0.000\n",
      "[228,   100] loss: 0.006\n",
      "[228,   150] loss: 0.008\n",
      "[228,   200] loss: 0.002\n",
      "[228,   250] loss: 0.004\n",
      "[229,    50] loss: 0.013\n",
      "[229,   100] loss: 0.002\n",
      "[229,   150] loss: 0.002\n",
      "[229,   200] loss: 0.006\n",
      "[229,   250] loss: 0.006\n",
      "[230,    50] loss: 0.003\n",
      "[230,   100] loss: 0.004\n",
      "[230,   150] loss: 0.002\n",
      "[230,   200] loss: 0.004\n",
      "[230,   250] loss: 0.020\n",
      "[231,    50] loss: 0.005\n",
      "[231,   100] loss: 0.003\n",
      "[231,   150] loss: 0.002\n",
      "[231,   200] loss: 0.003\n",
      "[231,   250] loss: 0.012\n",
      "[232,    50] loss: 0.001\n",
      "[232,   100] loss: 0.015\n",
      "[232,   150] loss: 0.002\n",
      "[232,   200] loss: 0.003\n",
      "[232,   250] loss: 0.005\n",
      "[233,    50] loss: 0.005\n",
      "[233,   100] loss: 0.005\n",
      "[233,   150] loss: 0.003\n",
      "[233,   200] loss: 0.004\n",
      "[233,   250] loss: 0.014\n",
      "[234,    50] loss: 0.010\n",
      "[234,   100] loss: 0.002\n",
      "[234,   150] loss: 0.003\n",
      "[234,   200] loss: 0.003\n",
      "[234,   250] loss: 0.003\n",
      "[235,    50] loss: 0.004\n",
      "[235,   100] loss: 0.001\n",
      "[235,   150] loss: 0.004\n",
      "[235,   200] loss: 0.001\n",
      "[235,   250] loss: 0.016\n",
      "[236,    50] loss: 0.003\n",
      "[236,   100] loss: 0.002\n",
      "[236,   150] loss: 0.019\n",
      "[236,   200] loss: 0.004\n",
      "[236,   250] loss: 0.002\n",
      "[237,    50] loss: 0.004\n",
      "[237,   100] loss: 0.002\n",
      "[237,   150] loss: 0.001\n",
      "[237,   200] loss: 0.005\n",
      "[237,   250] loss: 0.011\n",
      "[238,    50] loss: 0.002\n",
      "[238,   100] loss: 0.003\n",
      "[238,   150] loss: 0.002\n",
      "[238,   200] loss: 0.005\n",
      "[238,   250] loss: 0.013\n",
      "[239,    50] loss: 0.008\n",
      "[239,   100] loss: 0.004\n",
      "[239,   150] loss: 0.005\n",
      "[239,   200] loss: 0.001\n",
      "[239,   250] loss: 0.005\n",
      "[240,    50] loss: 0.001\n",
      "[240,   100] loss: 0.007\n",
      "[240,   150] loss: 0.001\n",
      "[240,   200] loss: 0.001\n",
      "[240,   250] loss: 0.001\n",
      "[241,    50] loss: 0.008\n",
      "[241,   100] loss: 0.002\n",
      "[241,   150] loss: 0.011\n",
      "[241,   200] loss: 0.002\n",
      "[241,   250] loss: 0.007\n",
      "[242,    50] loss: 0.001\n",
      "[242,   100] loss: 0.009\n",
      "[242,   150] loss: 0.006\n",
      "[242,   200] loss: 0.001\n",
      "[242,   250] loss: 0.002\n",
      "[243,    50] loss: 0.003\n",
      "[243,   100] loss: 0.003\n",
      "[243,   150] loss: 0.002\n",
      "[243,   200] loss: 0.004\n",
      "[243,   250] loss: 0.004\n",
      "[244,    50] loss: 0.018\n",
      "[244,   100] loss: 0.004\n",
      "[244,   150] loss: 0.004\n",
      "[244,   200] loss: 0.004\n",
      "[244,   250] loss: 0.001\n",
      "[245,    50] loss: 0.002\n",
      "[245,   100] loss: 0.013\n",
      "[245,   150] loss: 0.003\n",
      "[245,   200] loss: 0.002\n",
      "[245,   250] loss: 0.003\n",
      "[246,    50] loss: 0.002\n",
      "[246,   100] loss: 0.003\n",
      "[246,   150] loss: 0.002\n",
      "[246,   200] loss: 0.001\n",
      "[246,   250] loss: 0.011\n",
      "[247,    50] loss: 0.001\n",
      "[247,   100] loss: 0.002\n",
      "[247,   150] loss: 0.002\n",
      "[247,   200] loss: 0.012\n",
      "[247,   250] loss: 0.011\n",
      "[248,    50] loss: 0.002\n",
      "[248,   100] loss: 0.004\n",
      "[248,   150] loss: 0.015\n",
      "[248,   200] loss: 0.005\n",
      "[248,   250] loss: 0.003\n",
      "[249,    50] loss: 0.009\n",
      "[249,   100] loss: 0.003\n",
      "[249,   150] loss: 0.002\n",
      "[249,   200] loss: 0.006\n",
      "[249,   250] loss: 0.004\n",
      "[250,    50] loss: 0.003\n",
      "[250,   100] loss: 0.002\n",
      "[250,   150] loss: 0.003\n",
      "[250,   200] loss: 0.006\n",
      "[250,   250] loss: 0.007\n",
      "[251,    50] loss: 0.001\n",
      "[251,   100] loss: 0.005\n",
      "[251,   150] loss: 0.002\n",
      "[251,   200] loss: 0.002\n",
      "[251,   250] loss: 0.013\n",
      "[252,    50] loss: 0.005\n",
      "[252,   100] loss: 0.011\n",
      "[252,   150] loss: 0.006\n",
      "[252,   200] loss: 0.001\n",
      "[252,   250] loss: 0.002\n",
      "[253,    50] loss: 0.006\n",
      "[253,   100] loss: 0.002\n",
      "[253,   150] loss: 0.001\n",
      "[253,   200] loss: 0.009\n",
      "[253,   250] loss: 0.001\n",
      "[254,    50] loss: 0.005\n",
      "[254,   100] loss: 0.007\n",
      "[254,   150] loss: 0.002\n",
      "[254,   200] loss: 0.003\n",
      "[254,   250] loss: 0.003\n",
      "[255,    50] loss: 0.002\n",
      "[255,   100] loss: 0.003\n",
      "[255,   150] loss: 0.014\n",
      "[255,   200] loss: 0.007\n",
      "[255,   250] loss: 0.002\n",
      "[256,    50] loss: 0.001\n",
      "[256,   100] loss: 0.005\n",
      "[256,   150] loss: 0.003\n",
      "[256,   200] loss: 0.008\n",
      "[256,   250] loss: 0.004\n",
      "[257,    50] loss: 0.001\n",
      "[257,   100] loss: 0.001\n",
      "[257,   150] loss: 0.003\n",
      "[257,   200] loss: 0.009\n",
      "[257,   250] loss: 0.004\n",
      "[258,    50] loss: 0.001\n",
      "[258,   100] loss: 0.002\n",
      "[258,   150] loss: 0.006\n",
      "[258,   200] loss: 0.003\n",
      "[258,   250] loss: 0.001\n",
      "[259,    50] loss: 0.003\n",
      "[259,   100] loss: 0.011\n",
      "[259,   150] loss: 0.001\n",
      "[259,   200] loss: 0.003\n",
      "[259,   250] loss: 0.002\n",
      "[260,    50] loss: 0.004\n",
      "[260,   100] loss: 0.002\n",
      "[260,   150] loss: 0.002\n",
      "[260,   200] loss: 0.009\n",
      "[260,   250] loss: 0.001\n",
      "[261,    50] loss: 0.000\n",
      "[261,   100] loss: 0.002\n",
      "[261,   150] loss: 0.014\n",
      "[261,   200] loss: 0.003\n",
      "[261,   250] loss: 0.002\n",
      "[262,    50] loss: 0.001\n",
      "[262,   100] loss: 0.001\n",
      "[262,   150] loss: 0.001\n",
      "[262,   200] loss: 0.003\n",
      "[262,   250] loss: 0.006\n",
      "[263,    50] loss: 0.002\n",
      "[263,   100] loss: 0.002\n",
      "[263,   150] loss: 0.003\n",
      "[263,   200] loss: 0.013\n",
      "[263,   250] loss: 0.005\n",
      "[264,    50] loss: 0.004\n",
      "[264,   100] loss: 0.004\n",
      "[264,   150] loss: 0.002\n",
      "[264,   200] loss: 0.010\n",
      "[264,   250] loss: 0.001\n",
      "[265,    50] loss: 0.005\n",
      "[265,   100] loss: 0.001\n",
      "[265,   150] loss: 0.011\n",
      "[265,   200] loss: 0.002\n",
      "[265,   250] loss: 0.004\n",
      "[266,    50] loss: 0.013\n",
      "[266,   100] loss: 0.002\n",
      "[266,   150] loss: 0.004\n",
      "[266,   200] loss: 0.003\n",
      "[266,   250] loss: 0.000\n",
      "[267,    50] loss: 0.004\n",
      "[267,   100] loss: 0.006\n",
      "[267,   150] loss: 0.005\n",
      "[267,   200] loss: 0.002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[267,   250] loss: 0.003\n",
      "[268,    50] loss: 0.012\n",
      "[268,   100] loss: 0.000\n",
      "[268,   150] loss: 0.002\n",
      "[268,   200] loss: 0.006\n",
      "[268,   250] loss: 0.001\n",
      "[269,    50] loss: 0.002\n",
      "[269,   100] loss: 0.003\n",
      "[269,   150] loss: 0.002\n",
      "[269,   200] loss: 0.003\n",
      "[269,   250] loss: 0.009\n",
      "[270,    50] loss: 0.002\n",
      "[270,   100] loss: 0.002\n",
      "[270,   150] loss: 0.001\n",
      "[270,   200] loss: 0.013\n",
      "[270,   250] loss: 0.002\n",
      "[271,    50] loss: 0.002\n",
      "[271,   100] loss: 0.002\n",
      "[271,   150] loss: 0.009\n",
      "[271,   200] loss: 0.005\n",
      "[271,   250] loss: 0.004\n",
      "[272,    50] loss: 0.001\n",
      "[272,   100] loss: 0.002\n",
      "[272,   150] loss: 0.005\n",
      "[272,   200] loss: 0.006\n",
      "[272,   250] loss: 0.004\n",
      "[273,    50] loss: 0.001\n",
      "[273,   100] loss: 0.003\n",
      "[273,   150] loss: 0.001\n",
      "[273,   200] loss: 0.010\n",
      "[273,   250] loss: 0.003\n",
      "[274,    50] loss: 0.001\n",
      "[274,   100] loss: 0.012\n",
      "[274,   150] loss: 0.003\n",
      "[274,   200] loss: 0.001\n",
      "[274,   250] loss: 0.008\n",
      "[275,    50] loss: 0.000\n",
      "[275,   100] loss: 0.010\n",
      "[275,   150] loss: 0.004\n",
      "[275,   200] loss: 0.002\n",
      "[275,   250] loss: 0.007\n",
      "[276,    50] loss: 0.003\n",
      "[276,   100] loss: 0.003\n",
      "[276,   150] loss: 0.002\n",
      "[276,   200] loss: 0.001\n",
      "[276,   250] loss: 0.003\n",
      "[277,    50] loss: 0.003\n",
      "[277,   100] loss: 0.003\n",
      "[277,   150] loss: 0.001\n",
      "[277,   200] loss: 0.001\n",
      "[277,   250] loss: 0.008\n",
      "[278,    50] loss: 0.002\n",
      "[278,   100] loss: 0.004\n",
      "[278,   150] loss: 0.003\n",
      "[278,   200] loss: 0.001\n",
      "[278,   250] loss: 0.011\n",
      "[279,    50] loss: 0.003\n",
      "[279,   100] loss: 0.004\n",
      "[279,   150] loss: 0.004\n",
      "[279,   200] loss: 0.003\n",
      "[279,   250] loss: 0.002\n",
      "[280,    50] loss: 0.002\n",
      "[280,   100] loss: 0.002\n",
      "[280,   150] loss: 0.010\n",
      "[280,   200] loss: 0.001\n",
      "[280,   250] loss: 0.005\n",
      "[281,    50] loss: 0.003\n",
      "[281,   100] loss: 0.000\n",
      "[281,   150] loss: 0.006\n",
      "[281,   200] loss: 0.001\n",
      "[281,   250] loss: 0.004\n",
      "[282,    50] loss: 0.009\n",
      "[282,   100] loss: 0.004\n",
      "[282,   150] loss: 0.001\n",
      "[282,   200] loss: 0.001\n",
      "[282,   250] loss: 0.004\n",
      "[283,    50] loss: 0.002\n",
      "[283,   100] loss: 0.001\n",
      "[283,   150] loss: 0.003\n",
      "[283,   200] loss: 0.015\n",
      "[283,   250] loss: 0.001\n",
      "[284,    50] loss: 0.003\n",
      "[284,   100] loss: 0.001\n",
      "[284,   150] loss: 0.000\n",
      "[284,   200] loss: 0.001\n",
      "[284,   250] loss: 0.010\n",
      "[285,    50] loss: 0.002\n",
      "[285,   100] loss: 0.000\n",
      "[285,   150] loss: 0.002\n",
      "[285,   200] loss: 0.005\n",
      "[285,   250] loss: 0.012\n",
      "[286,    50] loss: 0.002\n",
      "[286,   100] loss: 0.005\n",
      "[286,   150] loss: 0.000\n",
      "[286,   200] loss: 0.001\n",
      "[286,   250] loss: 0.007\n",
      "[287,    50] loss: 0.001\n",
      "[287,   100] loss: 0.003\n",
      "[287,   150] loss: 0.003\n",
      "[287,   200] loss: 0.010\n",
      "[287,   250] loss: 0.001\n",
      "[288,    50] loss: 0.006\n",
      "[288,   100] loss: 0.002\n",
      "[288,   150] loss: 0.004\n",
      "[288,   200] loss: 0.003\n",
      "[288,   250] loss: 0.001\n",
      "[289,    50] loss: 0.000\n",
      "[289,   100] loss: 0.008\n",
      "[289,   150] loss: 0.006\n",
      "[289,   200] loss: 0.002\n",
      "[289,   250] loss: 0.002\n",
      "[290,    50] loss: 0.002\n",
      "[290,   100] loss: 0.001\n",
      "[290,   150] loss: 0.010\n",
      "[290,   200] loss: 0.001\n",
      "[290,   250] loss: 0.001\n",
      "[291,    50] loss: 0.001\n",
      "[291,   100] loss: 0.001\n",
      "[291,   150] loss: 0.012\n",
      "[291,   200] loss: 0.002\n",
      "[291,   250] loss: 0.002\n",
      "[292,    50] loss: 0.002\n",
      "[292,   100] loss: 0.002\n",
      "[292,   150] loss: 0.002\n",
      "[292,   200] loss: 0.002\n",
      "[292,   250] loss: 0.009\n",
      "[293,    50] loss: 0.001\n",
      "[293,   100] loss: 0.001\n",
      "[293,   150] loss: 0.004\n",
      "[293,   200] loss: 0.001\n",
      "[293,   250] loss: 0.008\n",
      "[294,    50] loss: 0.005\n",
      "[294,   100] loss: 0.000\n",
      "[294,   150] loss: 0.001\n",
      "[294,   200] loss: 0.002\n",
      "[294,   250] loss: 0.006\n",
      "[295,    50] loss: 0.005\n",
      "[295,   100] loss: 0.002\n",
      "[295,   150] loss: 0.001\n",
      "[295,   200] loss: 0.006\n",
      "[295,   250] loss: 0.005\n",
      "[296,    50] loss: 0.005\n",
      "[296,   100] loss: 0.000\n",
      "[296,   150] loss: 0.003\n",
      "[296,   200] loss: 0.002\n",
      "[296,   250] loss: 0.002\n",
      "[297,    50] loss: 0.002\n",
      "[297,   100] loss: 0.002\n",
      "[297,   150] loss: 0.000\n",
      "[297,   200] loss: 0.001\n",
      "[297,   250] loss: 0.002\n",
      "[298,    50] loss: 0.006\n",
      "[298,   100] loss: 0.001\n",
      "[298,   150] loss: 0.005\n",
      "[298,   200] loss: 0.004\n",
      "[298,   250] loss: 0.001\n",
      "[299,    50] loss: 0.001\n",
      "[299,   100] loss: 0.002\n",
      "[299,   150] loss: 0.001\n",
      "[299,   200] loss: 0.002\n",
      "[299,   250] loss: 0.008\n",
      "[300,    50] loss: 0.010\n",
      "[300,   100] loss: 0.001\n",
      "[300,   150] loss: 0.002\n",
      "[300,   200] loss: 0.001\n",
      "[300,   250] loss: 0.003\n",
      "[301,    50] loss: 0.000\n",
      "[301,   100] loss: 0.003\n",
      "[301,   150] loss: 0.001\n",
      "[301,   200] loss: 0.006\n",
      "[301,   250] loss: 0.004\n",
      "[302,    50] loss: 0.001\n",
      "[302,   100] loss: 0.001\n",
      "[302,   150] loss: 0.000\n",
      "[302,   200] loss: 0.009\n",
      "[302,   250] loss: 0.007\n",
      "[303,    50] loss: 0.000\n",
      "[303,   100] loss: 0.001\n",
      "[303,   150] loss: 0.003\n",
      "[303,   200] loss: 0.003\n",
      "[303,   250] loss: 0.006\n",
      "[304,    50] loss: 0.002\n",
      "[304,   100] loss: 0.014\n",
      "[304,   150] loss: 0.001\n",
      "[304,   200] loss: 0.002\n",
      "[304,   250] loss: 0.001\n",
      "[305,    50] loss: 0.000\n",
      "[305,   100] loss: 0.002\n",
      "[305,   150] loss: 0.002\n",
      "[305,   200] loss: 0.002\n",
      "[305,   250] loss: 0.001\n",
      "[306,    50] loss: 0.001\n",
      "[306,   100] loss: 0.006\n",
      "[306,   150] loss: 0.004\n",
      "[306,   200] loss: 0.000\n",
      "[306,   250] loss: 0.002\n",
      "[307,    50] loss: 0.001\n",
      "[307,   100] loss: 0.005\n",
      "[307,   150] loss: 0.001\n",
      "[307,   200] loss: 0.003\n",
      "[307,   250] loss: 0.003\n",
      "[308,    50] loss: 0.001\n",
      "[308,   100] loss: 0.009\n",
      "[308,   150] loss: 0.001\n",
      "[308,   200] loss: 0.003\n",
      "[308,   250] loss: 0.000\n",
      "[309,    50] loss: 0.001\n",
      "[309,   100] loss: 0.001\n",
      "[309,   150] loss: 0.002\n",
      "[309,   200] loss: 0.003\n",
      "[309,   250] loss: 0.004\n",
      "[310,    50] loss: 0.001\n",
      "[310,   100] loss: 0.001\n",
      "[310,   150] loss: 0.010\n",
      "[310,   200] loss: 0.001\n",
      "[310,   250] loss: 0.004\n",
      "[311,    50] loss: 0.005\n",
      "[311,   100] loss: 0.002\n",
      "[311,   150] loss: 0.002\n",
      "[311,   200] loss: 0.001\n",
      "[311,   250] loss: 0.003\n",
      "[312,    50] loss: 0.001\n",
      "[312,   100] loss: 0.002\n",
      "[312,   150] loss: 0.001\n",
      "[312,   200] loss: 0.001\n",
      "[312,   250] loss: 0.005\n",
      "[313,    50] loss: 0.002\n",
      "[313,   100] loss: 0.001\n",
      "[313,   150] loss: 0.002\n",
      "[313,   200] loss: 0.007\n",
      "[313,   250] loss: 0.001\n",
      "[314,    50] loss: 0.000\n",
      "[314,   100] loss: 0.001\n",
      "[314,   150] loss: 0.005\n",
      "[314,   200] loss: 0.001\n",
      "[314,   250] loss: 0.002\n",
      "[315,    50] loss: 0.001\n",
      "[315,   100] loss: 0.002\n",
      "[315,   150] loss: 0.001\n",
      "[315,   200] loss: 0.001\n",
      "[315,   250] loss: 0.007\n",
      "[316,    50] loss: 0.002\n",
      "[316,   100] loss: 0.002\n",
      "[316,   150] loss: 0.002\n",
      "[316,   200] loss: 0.000\n",
      "[316,   250] loss: 0.004\n",
      "[317,    50] loss: 0.002\n",
      "[317,   100] loss: 0.001\n",
      "[317,   150] loss: 0.001\n",
      "[317,   200] loss: 0.000\n",
      "[317,   250] loss: 0.008\n",
      "[318,    50] loss: 0.002\n",
      "[318,   100] loss: 0.003\n",
      "[318,   150] loss: 0.001\n",
      "[318,   200] loss: 0.004\n",
      "[318,   250] loss: 0.003\n",
      "[319,    50] loss: 0.004\n",
      "[319,   100] loss: 0.001\n",
      "[319,   150] loss: 0.005\n",
      "[319,   200] loss: 0.001\n",
      "[319,   250] loss: 0.003\n",
      "[320,    50] loss: 0.002\n",
      "[320,   100] loss: 0.002\n",
      "[320,   150] loss: 0.004\n",
      "[320,   200] loss: 0.003\n",
      "[320,   250] loss: 0.002\n",
      "[321,    50] loss: 0.002\n",
      "[321,   100] loss: 0.002\n",
      "[321,   150] loss: 0.002\n",
      "[321,   200] loss: 0.004\n",
      "[321,   250] loss: 0.002\n",
      "[322,    50] loss: 0.001\n",
      "[322,   100] loss: 0.003\n",
      "[322,   150] loss: 0.001\n",
      "[322,   200] loss: 0.005\n",
      "[322,   250] loss: 0.002\n",
      "[323,    50] loss: 0.002\n",
      "[323,   100] loss: 0.000\n",
      "[323,   150] loss: 0.000\n",
      "[323,   200] loss: 0.002\n",
      "[323,   250] loss: 0.004\n",
      "[324,    50] loss: 0.001\n",
      "[324,   100] loss: 0.006\n",
      "[324,   150] loss: 0.002\n",
      "[324,   200] loss: 0.003\n",
      "[324,   250] loss: 0.002\n",
      "[325,    50] loss: 0.001\n",
      "[325,   100] loss: 0.001\n",
      "[325,   150] loss: 0.001\n",
      "[325,   200] loss: 0.003\n",
      "[325,   250] loss: 0.004\n",
      "[326,    50] loss: 0.001\n",
      "[326,   100] loss: 0.001\n",
      "[326,   150] loss: 0.001\n",
      "[326,   200] loss: 0.007\n",
      "[326,   250] loss: 0.002\n",
      "[327,    50] loss: 0.000\n",
      "[327,   100] loss: 0.001\n",
      "[327,   150] loss: 0.002\n",
      "[327,   200] loss: 0.002\n",
      "[327,   250] loss: 0.005\n",
      "[328,    50] loss: 0.001\n",
      "[328,   100] loss: 0.001\n",
      "[328,   150] loss: 0.002\n",
      "[328,   200] loss: 0.002\n",
      "[328,   250] loss: 0.006\n",
      "[329,    50] loss: 0.001\n",
      "[329,   100] loss: 0.000\n",
      "[329,   150] loss: 0.003\n",
      "[329,   200] loss: 0.002\n",
      "[329,   250] loss: 0.005\n",
      "[330,    50] loss: 0.003\n",
      "[330,   100] loss: 0.001\n",
      "[330,   150] loss: 0.001\n",
      "[330,   200] loss: 0.004\n",
      "[330,   250] loss: 0.001\n",
      "[331,    50] loss: 0.001\n",
      "[331,   100] loss: 0.004\n",
      "[331,   150] loss: 0.000\n",
      "[331,   200] loss: 0.001\n",
      "[331,   250] loss: 0.002\n",
      "[332,    50] loss: 0.001\n",
      "[332,   100] loss: 0.005\n",
      "[332,   150] loss: 0.002\n",
      "[332,   200] loss: 0.001\n",
      "[332,   250] loss: 0.001\n",
      "[333,    50] loss: 0.000\n",
      "[333,   100] loss: 0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[333,   150] loss: 0.005\n",
      "[333,   200] loss: 0.003\n",
      "[333,   250] loss: 0.001\n",
      "[334,    50] loss: 0.004\n",
      "[334,   100] loss: 0.000\n",
      "[334,   150] loss: 0.004\n",
      "[334,   200] loss: 0.002\n",
      "[334,   250] loss: 0.001\n",
      "[335,    50] loss: 0.001\n",
      "[335,   100] loss: 0.001\n",
      "[335,   150] loss: 0.003\n",
      "[335,   200] loss: 0.001\n",
      "[335,   250] loss: 0.002\n",
      "[336,    50] loss: 0.002\n",
      "[336,   100] loss: 0.002\n",
      "[336,   150] loss: 0.004\n",
      "[336,   200] loss: 0.001\n",
      "[336,   250] loss: 0.003\n",
      "[337,    50] loss: 0.001\n",
      "[337,   100] loss: 0.001\n",
      "[337,   150] loss: 0.006\n",
      "[337,   200] loss: 0.003\n",
      "[337,   250] loss: 0.001\n",
      "[338,    50] loss: 0.000\n",
      "[338,   100] loss: 0.001\n",
      "[338,   150] loss: 0.001\n",
      "[338,   200] loss: 0.001\n",
      "[338,   250] loss: 0.004\n",
      "[339,    50] loss: 0.007\n",
      "[339,   100] loss: 0.002\n",
      "[339,   150] loss: 0.001\n",
      "[339,   200] loss: 0.001\n",
      "[339,   250] loss: 0.001\n",
      "[340,    50] loss: 0.003\n",
      "[340,   100] loss: 0.004\n",
      "[340,   150] loss: 0.001\n",
      "[340,   200] loss: 0.000\n",
      "[340,   250] loss: 0.002\n",
      "[341,    50] loss: 0.001\n",
      "[341,   100] loss: 0.001\n",
      "[341,   150] loss: 0.002\n",
      "[341,   200] loss: 0.004\n",
      "[341,   250] loss: 0.001\n",
      "[342,    50] loss: 0.002\n",
      "[342,   100] loss: 0.001\n",
      "[342,   150] loss: 0.004\n",
      "[342,   200] loss: 0.002\n",
      "[342,   250] loss: 0.000\n",
      "[343,    50] loss: 0.001\n",
      "[343,   100] loss: 0.008\n",
      "[343,   150] loss: 0.001\n",
      "[343,   200] loss: 0.001\n",
      "[343,   250] loss: 0.001\n",
      "[344,    50] loss: 0.001\n",
      "[344,   100] loss: 0.004\n",
      "[344,   150] loss: 0.000\n",
      "[344,   200] loss: 0.001\n",
      "[344,   250] loss: 0.003\n",
      "[345,    50] loss: 0.003\n",
      "[345,   100] loss: 0.001\n",
      "[345,   150] loss: 0.002\n",
      "[345,   200] loss: 0.001\n",
      "[345,   250] loss: 0.002\n",
      "[346,    50] loss: 0.001\n",
      "[346,   100] loss: 0.003\n",
      "[346,   150] loss: 0.002\n",
      "[346,   200] loss: 0.003\n",
      "[346,   250] loss: 0.001\n",
      "[347,    50] loss: 0.004\n",
      "[347,   100] loss: 0.002\n",
      "[347,   150] loss: 0.001\n",
      "[347,   200] loss: 0.001\n",
      "[347,   250] loss: 0.002\n",
      "[348,    50] loss: 0.001\n",
      "[348,   100] loss: 0.002\n",
      "[348,   150] loss: 0.003\n",
      "[348,   200] loss: 0.001\n",
      "[348,   250] loss: 0.001\n",
      "[349,    50] loss: 0.001\n",
      "[349,   100] loss: 0.001\n",
      "[349,   150] loss: 0.001\n",
      "[349,   200] loss: 0.005\n",
      "[349,   250] loss: 0.003\n",
      "[350,    50] loss: 0.000\n",
      "[350,   100] loss: 0.003\n",
      "[350,   150] loss: 0.000\n",
      "[350,   200] loss: 0.002\n",
      "[350,   250] loss: 0.004\n",
      "[351,    50] loss: 0.002\n",
      "[351,   100] loss: 0.002\n",
      "[351,   150] loss: 0.001\n",
      "[351,   200] loss: 0.000\n",
      "[351,   250] loss: 0.005\n",
      "[352,    50] loss: 0.002\n",
      "[352,   100] loss: 0.005\n",
      "[352,   150] loss: 0.001\n",
      "[352,   200] loss: 0.001\n",
      "[352,   250] loss: 0.003\n",
      "[353,    50] loss: 0.001\n",
      "[353,   100] loss: 0.003\n",
      "[353,   150] loss: 0.000\n",
      "[353,   200] loss: 0.002\n",
      "[353,   250] loss: 0.001\n",
      "[354,    50] loss: 0.000\n",
      "[354,   100] loss: 0.001\n",
      "[354,   150] loss: 0.002\n",
      "[354,   200] loss: 0.004\n",
      "[354,   250] loss: 0.000\n",
      "[355,    50] loss: 0.001\n",
      "[355,   100] loss: 0.011\n",
      "[355,   150] loss: 0.001\n",
      "[355,   200] loss: 0.001\n",
      "[355,   250] loss: 0.000\n",
      "[356,    50] loss: 0.001\n",
      "[356,   100] loss: 0.000\n",
      "[356,   150] loss: 0.001\n",
      "[356,   200] loss: 0.006\n",
      "[356,   250] loss: 0.000\n",
      "[357,    50] loss: 0.001\n",
      "[357,   100] loss: 0.001\n",
      "[357,   150] loss: 0.005\n",
      "[357,   200] loss: 0.002\n",
      "[357,   250] loss: 0.000\n",
      "[358,    50] loss: 0.001\n",
      "[358,   100] loss: 0.001\n",
      "[358,   150] loss: 0.001\n",
      "[358,   200] loss: 0.004\n",
      "[358,   250] loss: 0.001\n",
      "[359,    50] loss: 0.001\n",
      "[359,   100] loss: 0.001\n",
      "[359,   150] loss: 0.001\n",
      "[359,   200] loss: 0.001\n",
      "[359,   250] loss: 0.003\n",
      "[360,    50] loss: 0.001\n",
      "[360,   100] loss: 0.003\n",
      "[360,   150] loss: 0.003\n",
      "[360,   200] loss: 0.001\n",
      "[360,   250] loss: 0.001\n",
      "[361,    50] loss: 0.001\n",
      "[361,   100] loss: 0.001\n",
      "[361,   150] loss: 0.006\n",
      "[361,   200] loss: 0.001\n",
      "[361,   250] loss: 0.001\n",
      "[362,    50] loss: 0.001\n",
      "[362,   100] loss: 0.004\n",
      "[362,   150] loss: 0.001\n",
      "[362,   200] loss: 0.002\n",
      "[362,   250] loss: 0.000\n",
      "[363,    50] loss: 0.001\n",
      "[363,   100] loss: 0.004\n",
      "[363,   150] loss: 0.003\n",
      "[363,   200] loss: 0.001\n",
      "[363,   250] loss: 0.002\n",
      "[364,    50] loss: 0.004\n",
      "[364,   100] loss: 0.002\n",
      "[364,   150] loss: 0.001\n",
      "[364,   200] loss: 0.001\n",
      "[364,   250] loss: 0.002\n",
      "[365,    50] loss: 0.001\n",
      "[365,   100] loss: 0.003\n",
      "[365,   150] loss: 0.003\n",
      "[365,   200] loss: 0.000\n",
      "[365,   250] loss: 0.000\n",
      "[366,    50] loss: 0.002\n",
      "[366,   100] loss: 0.002\n",
      "[366,   150] loss: 0.003\n",
      "[366,   200] loss: 0.002\n",
      "[366,   250] loss: 0.001\n",
      "[367,    50] loss: 0.001\n",
      "[367,   100] loss: 0.001\n",
      "[367,   150] loss: 0.000\n",
      "[367,   200] loss: 0.002\n",
      "[367,   250] loss: 0.001\n",
      "[368,    50] loss: 0.000\n",
      "[368,   100] loss: 0.000\n",
      "[368,   150] loss: 0.002\n",
      "[368,   200] loss: 0.003\n",
      "[368,   250] loss: 0.001\n",
      "[369,    50] loss: 0.002\n",
      "[369,   100] loss: 0.002\n",
      "[369,   150] loss: 0.002\n",
      "[369,   200] loss: 0.001\n",
      "[369,   250] loss: 0.003\n",
      "[370,    50] loss: 0.000\n",
      "[370,   100] loss: 0.001\n",
      "[370,   150] loss: 0.003\n",
      "[370,   200] loss: 0.003\n",
      "[370,   250] loss: 0.003\n",
      "[371,    50] loss: 0.001\n",
      "[371,   100] loss: 0.000\n",
      "[371,   150] loss: 0.002\n",
      "[371,   200] loss: 0.003\n",
      "[371,   250] loss: 0.002\n",
      "[372,    50] loss: 0.001\n",
      "[372,   100] loss: 0.005\n",
      "[372,   150] loss: 0.000\n",
      "[372,   200] loss: 0.001\n",
      "[372,   250] loss: 0.001\n",
      "[373,    50] loss: 0.001\n",
      "[373,   100] loss: 0.001\n",
      "[373,   150] loss: 0.004\n",
      "[373,   200] loss: 0.001\n",
      "[373,   250] loss: 0.002\n",
      "[374,    50] loss: 0.005\n",
      "[374,   100] loss: 0.001\n",
      "[374,   150] loss: 0.001\n",
      "[374,   200] loss: 0.002\n",
      "[374,   250] loss: 0.001\n",
      "[375,    50] loss: 0.001\n",
      "[375,   100] loss: 0.000\n",
      "[375,   150] loss: 0.001\n",
      "[375,   200] loss: 0.001\n",
      "[375,   250] loss: 0.006\n",
      "[376,    50] loss: 0.004\n",
      "[376,   100] loss: 0.001\n",
      "[376,   150] loss: 0.001\n",
      "[376,   200] loss: 0.001\n",
      "[376,   250] loss: 0.000\n",
      "[377,    50] loss: 0.001\n",
      "[377,   100] loss: 0.001\n",
      "[377,   150] loss: 0.004\n",
      "[377,   200] loss: 0.000\n",
      "[377,   250] loss: 0.002\n",
      "[378,    50] loss: 0.000\n",
      "[378,   100] loss: 0.002\n",
      "[378,   150] loss: 0.001\n",
      "[378,   200] loss: 0.001\n",
      "[378,   250] loss: 0.004\n",
      "[379,    50] loss: 0.001\n",
      "[379,   100] loss: 0.002\n",
      "[379,   150] loss: 0.001\n",
      "[379,   200] loss: 0.001\n",
      "[379,   250] loss: 0.003\n",
      "[380,    50] loss: 0.000\n",
      "[380,   100] loss: 0.000\n",
      "[380,   150] loss: 0.006\n",
      "[380,   200] loss: 0.000\n",
      "[380,   250] loss: 0.002\n",
      "[381,    50] loss: 0.002\n",
      "[381,   100] loss: 0.000\n",
      "[381,   150] loss: 0.001\n",
      "[381,   200] loss: 0.001\n",
      "[381,   250] loss: 0.000\n",
      "[382,    50] loss: 0.001\n",
      "[382,   100] loss: 0.001\n",
      "[382,   150] loss: 0.002\n",
      "[382,   200] loss: 0.002\n",
      "[382,   250] loss: 0.002\n",
      "[383,    50] loss: 0.000\n",
      "[383,   100] loss: 0.003\n",
      "[383,   150] loss: 0.002\n",
      "[383,   200] loss: 0.000\n",
      "[383,   250] loss: 0.001\n",
      "[384,    50] loss: 0.001\n",
      "[384,   100] loss: 0.000\n",
      "[384,   150] loss: 0.001\n",
      "[384,   200] loss: 0.001\n",
      "[384,   250] loss: 0.004\n",
      "[385,    50] loss: 0.001\n",
      "[385,   100] loss: 0.001\n",
      "[385,   150] loss: 0.002\n",
      "[385,   200] loss: 0.000\n",
      "[385,   250] loss: 0.001\n",
      "[386,    50] loss: 0.001\n",
      "[386,   100] loss: 0.001\n",
      "[386,   150] loss: 0.003\n",
      "[386,   200] loss: 0.001\n",
      "[386,   250] loss: 0.000\n",
      "[387,    50] loss: 0.000\n",
      "[387,   100] loss: 0.002\n",
      "[387,   150] loss: 0.001\n",
      "[387,   200] loss: 0.003\n",
      "[387,   250] loss: 0.002\n",
      "[388,    50] loss: 0.000\n",
      "[388,   100] loss: 0.002\n",
      "[388,   150] loss: 0.000\n",
      "[388,   200] loss: 0.001\n",
      "[388,   250] loss: 0.003\n",
      "[389,    50] loss: 0.001\n",
      "[389,   100] loss: 0.001\n",
      "[389,   150] loss: 0.001\n",
      "[389,   200] loss: 0.001\n",
      "[389,   250] loss: 0.003\n",
      "[390,    50] loss: 0.002\n",
      "[390,   100] loss: 0.001\n",
      "[390,   150] loss: 0.001\n",
      "[390,   200] loss: 0.003\n",
      "[390,   250] loss: 0.001\n",
      "[391,    50] loss: 0.000\n",
      "[391,   100] loss: 0.001\n",
      "[391,   150] loss: 0.001\n",
      "[391,   200] loss: 0.003\n",
      "[391,   250] loss: 0.001\n",
      "[392,    50] loss: 0.001\n",
      "[392,   100] loss: 0.004\n",
      "[392,   150] loss: 0.001\n",
      "[392,   200] loss: 0.000\n",
      "[392,   250] loss: 0.001\n",
      "[393,    50] loss: 0.001\n",
      "[393,   100] loss: 0.000\n",
      "[393,   150] loss: 0.000\n",
      "[393,   200] loss: 0.004\n",
      "[393,   250] loss: 0.002\n",
      "[394,    50] loss: 0.001\n",
      "[394,   100] loss: 0.001\n",
      "[394,   150] loss: 0.001\n",
      "[394,   200] loss: 0.000\n",
      "[394,   250] loss: 0.003\n",
      "[395,    50] loss: 0.000\n",
      "[395,   100] loss: 0.001\n",
      "[395,   150] loss: 0.002\n",
      "[395,   200] loss: 0.000\n",
      "[395,   250] loss: 0.002\n",
      "[396,    50] loss: 0.000\n",
      "[396,   100] loss: 0.001\n",
      "[396,   150] loss: 0.003\n",
      "[396,   200] loss: 0.002\n",
      "[396,   250] loss: 0.002\n",
      "[397,    50] loss: 0.002\n",
      "[397,   100] loss: 0.001\n",
      "[397,   150] loss: 0.001\n",
      "[397,   200] loss: 0.001\n",
      "[397,   250] loss: 0.002\n",
      "[398,    50] loss: 0.000\n",
      "[398,   100] loss: 0.001\n",
      "[398,   150] loss: 0.001\n",
      "[398,   200] loss: 0.002\n",
      "[398,   250] loss: 0.002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[399,    50] loss: 0.000\n",
      "[399,   100] loss: 0.004\n",
      "[399,   150] loss: 0.001\n",
      "[399,   200] loss: 0.001\n",
      "[399,   250] loss: 0.002\n",
      "[400,    50] loss: 0.000\n",
      "[400,   100] loss: 0.000\n",
      "[400,   150] loss: 0.001\n",
      "[400,   200] loss: 0.001\n",
      "[400,   250] loss: 0.003\n",
      "[401,    50] loss: 0.001\n",
      "[401,   100] loss: 0.001\n",
      "[401,   150] loss: 0.003\n",
      "[401,   200] loss: 0.000\n",
      "[401,   250] loss: 0.000\n",
      "[402,    50] loss: 0.001\n",
      "[402,   100] loss: 0.001\n",
      "[402,   150] loss: 0.003\n",
      "[402,   200] loss: 0.000\n",
      "[402,   250] loss: 0.001\n",
      "[403,    50] loss: 0.001\n",
      "[403,   100] loss: 0.000\n",
      "[403,   150] loss: 0.001\n",
      "[403,   200] loss: 0.001\n",
      "[403,   250] loss: 0.001\n",
      "[404,    50] loss: 0.002\n",
      "[404,   100] loss: 0.000\n",
      "[404,   150] loss: 0.003\n",
      "[404,   200] loss: 0.001\n",
      "[404,   250] loss: 0.001\n",
      "[405,    50] loss: 0.001\n",
      "[405,   100] loss: 0.000\n",
      "[405,   150] loss: 0.000\n",
      "[405,   200] loss: 0.001\n",
      "[405,   250] loss: 0.004\n",
      "[406,    50] loss: 0.001\n",
      "[406,   100] loss: 0.002\n",
      "[406,   150] loss: 0.000\n",
      "[406,   200] loss: 0.002\n",
      "[406,   250] loss: 0.002\n",
      "[407,    50] loss: 0.001\n",
      "[407,   100] loss: 0.000\n",
      "[407,   150] loss: 0.003\n",
      "[407,   200] loss: 0.001\n",
      "[407,   250] loss: 0.000\n",
      "[408,    50] loss: 0.000\n",
      "[408,   100] loss: 0.000\n",
      "[408,   150] loss: 0.001\n",
      "[408,   200] loss: 0.001\n",
      "[408,   250] loss: 0.000\n",
      "[409,    50] loss: 0.001\n",
      "[409,   100] loss: 0.001\n",
      "[409,   150] loss: 0.001\n",
      "[409,   200] loss: 0.001\n",
      "[409,   250] loss: 0.001\n",
      "[410,    50] loss: 0.000\n",
      "[410,   100] loss: 0.002\n",
      "[410,   150] loss: 0.001\n",
      "[410,   200] loss: 0.001\n",
      "[410,   250] loss: 0.000\n",
      "[411,    50] loss: 0.001\n",
      "[411,   100] loss: 0.001\n",
      "[411,   150] loss: 0.001\n",
      "[411,   200] loss: 0.004\n",
      "[411,   250] loss: 0.000\n",
      "[412,    50] loss: 0.000\n",
      "[412,   100] loss: 0.000\n",
      "[412,   150] loss: 0.001\n",
      "[412,   200] loss: 0.003\n",
      "[412,   250] loss: 0.001\n",
      "[413,    50] loss: 0.000\n",
      "[413,   100] loss: 0.002\n",
      "[413,   150] loss: 0.001\n",
      "[413,   200] loss: 0.002\n",
      "[413,   250] loss: 0.001\n",
      "[414,    50] loss: 0.001\n",
      "[414,   100] loss: 0.001\n",
      "[414,   150] loss: 0.002\n",
      "[414,   200] loss: 0.002\n",
      "[414,   250] loss: 0.001\n",
      "[415,    50] loss: 0.003\n",
      "[415,   100] loss: 0.001\n",
      "[415,   150] loss: 0.001\n",
      "[415,   200] loss: 0.001\n",
      "[415,   250] loss: 0.000\n",
      "[416,    50] loss: 0.001\n",
      "[416,   100] loss: 0.001\n",
      "[416,   150] loss: 0.002\n",
      "[416,   200] loss: 0.002\n",
      "[416,   250] loss: 0.001\n",
      "[417,    50] loss: 0.000\n",
      "[417,   100] loss: 0.002\n",
      "[417,   150] loss: 0.002\n",
      "[417,   200] loss: 0.001\n",
      "[417,   250] loss: 0.000\n",
      "[418,    50] loss: 0.002\n",
      "[418,   100] loss: 0.001\n",
      "[418,   150] loss: 0.002\n",
      "[418,   200] loss: 0.000\n",
      "[418,   250] loss: 0.001\n",
      "[419,    50] loss: 0.001\n",
      "[419,   100] loss: 0.001\n",
      "[419,   150] loss: 0.000\n",
      "[419,   200] loss: 0.004\n",
      "[419,   250] loss: 0.001\n",
      "[420,    50] loss: 0.002\n",
      "[420,   100] loss: 0.001\n",
      "[420,   150] loss: 0.000\n",
      "[420,   200] loss: 0.000\n",
      "[420,   250] loss: 0.002\n",
      "[421,    50] loss: 0.001\n",
      "[421,   100] loss: 0.000\n",
      "[421,   150] loss: 0.001\n",
      "[421,   200] loss: 0.002\n",
      "[421,   250] loss: 0.002\n",
      "[422,    50] loss: 0.001\n",
      "[422,   100] loss: 0.001\n",
      "[422,   150] loss: 0.000\n",
      "[422,   200] loss: 0.001\n",
      "[422,   250] loss: 0.001\n",
      "[423,    50] loss: 0.001\n",
      "[423,   100] loss: 0.002\n",
      "[423,   150] loss: 0.001\n",
      "[423,   200] loss: 0.001\n",
      "[423,   250] loss: 0.001\n",
      "[424,    50] loss: 0.000\n",
      "[424,   100] loss: 0.001\n",
      "[424,   150] loss: 0.000\n",
      "[424,   200] loss: 0.001\n",
      "[424,   250] loss: 0.003\n",
      "[425,    50] loss: 0.000\n",
      "[425,   100] loss: 0.003\n",
      "[425,   150] loss: 0.001\n",
      "[425,   200] loss: 0.001\n",
      "[425,   250] loss: 0.000\n",
      "[426,    50] loss: 0.000\n",
      "[426,   100] loss: 0.000\n",
      "[426,   150] loss: 0.002\n",
      "[426,   200] loss: 0.002\n",
      "[426,   250] loss: 0.001\n",
      "[427,    50] loss: 0.002\n",
      "[427,   100] loss: 0.000\n",
      "[427,   150] loss: 0.000\n",
      "[427,   200] loss: 0.001\n",
      "[427,   250] loss: 0.002\n",
      "[428,    50] loss: 0.001\n",
      "[428,   100] loss: 0.000\n",
      "[428,   150] loss: 0.001\n",
      "[428,   200] loss: 0.003\n",
      "[428,   250] loss: 0.000\n",
      "[429,    50] loss: 0.002\n",
      "[429,   100] loss: 0.001\n",
      "[429,   150] loss: 0.000\n",
      "[429,   200] loss: 0.002\n",
      "[429,   250] loss: 0.001\n",
      "[430,    50] loss: 0.000\n",
      "[430,   100] loss: 0.003\n",
      "[430,   150] loss: 0.000\n",
      "[430,   200] loss: 0.000\n",
      "[430,   250] loss: 0.001\n",
      "[431,    50] loss: 0.001\n",
      "[431,   100] loss: 0.001\n",
      "[431,   150] loss: 0.000\n",
      "[431,   200] loss: 0.000\n",
      "[431,   250] loss: 0.004\n",
      "[432,    50] loss: 0.001\n",
      "[432,   100] loss: 0.000\n",
      "[432,   150] loss: 0.001\n",
      "[432,   200] loss: 0.001\n",
      "[432,   250] loss: 0.002\n",
      "[433,    50] loss: 0.000\n",
      "[433,   100] loss: 0.001\n",
      "[433,   150] loss: 0.002\n",
      "[433,   200] loss: 0.000\n",
      "[433,   250] loss: 0.002\n",
      "[434,    50] loss: 0.001\n",
      "[434,   100] loss: 0.001\n",
      "[434,   150] loss: 0.000\n",
      "[434,   200] loss: 0.002\n",
      "[434,   250] loss: 0.000\n",
      "[435,    50] loss: 0.001\n",
      "[435,   100] loss: 0.002\n",
      "[435,   150] loss: 0.002\n",
      "[435,   200] loss: 0.001\n",
      "[435,   250] loss: 0.000\n",
      "[436,    50] loss: 0.001\n",
      "[436,   100] loss: 0.001\n",
      "[436,   150] loss: 0.001\n",
      "[436,   200] loss: 0.001\n",
      "[436,   250] loss: 0.002\n",
      "[437,    50] loss: 0.001\n",
      "[437,   100] loss: 0.001\n",
      "[437,   150] loss: 0.001\n",
      "[437,   200] loss: 0.002\n",
      "[437,   250] loss: 0.001\n",
      "[438,    50] loss: 0.001\n",
      "[438,   100] loss: 0.001\n",
      "[438,   150] loss: 0.000\n",
      "[438,   200] loss: 0.001\n",
      "[438,   250] loss: 0.003\n",
      "[439,    50] loss: 0.002\n",
      "[439,   100] loss: 0.001\n",
      "[439,   150] loss: 0.001\n",
      "[439,   200] loss: 0.000\n",
      "[439,   250] loss: 0.001\n",
      "[440,    50] loss: 0.001\n",
      "[440,   100] loss: 0.003\n",
      "[440,   150] loss: 0.001\n",
      "[440,   200] loss: 0.000\n",
      "[440,   250] loss: 0.000\n",
      "[441,    50] loss: 0.001\n",
      "[441,   100] loss: 0.001\n",
      "[441,   150] loss: 0.002\n",
      "[441,   200] loss: 0.001\n",
      "[441,   250] loss: 0.000\n",
      "[442,    50] loss: 0.000\n",
      "[442,   100] loss: 0.001\n",
      "[442,   150] loss: 0.002\n",
      "[442,   200] loss: 0.000\n",
      "[442,   250] loss: 0.001\n",
      "[443,    50] loss: 0.001\n",
      "[443,   100] loss: 0.000\n",
      "[443,   150] loss: 0.002\n",
      "[443,   200] loss: 0.001\n",
      "[443,   250] loss: 0.001\n",
      "[444,    50] loss: 0.000\n",
      "[444,   100] loss: 0.003\n",
      "[444,   150] loss: 0.000\n",
      "[444,   200] loss: 0.001\n",
      "[444,   250] loss: 0.001\n",
      "[445,    50] loss: 0.001\n",
      "[445,   100] loss: 0.000\n",
      "[445,   150] loss: 0.000\n",
      "[445,   200] loss: 0.002\n",
      "[445,   250] loss: 0.000\n",
      "[446,    50] loss: 0.001\n",
      "[446,   100] loss: 0.000\n",
      "[446,   150] loss: 0.002\n",
      "[446,   200] loss: 0.001\n",
      "[446,   250] loss: 0.001\n",
      "[447,    50] loss: 0.000\n",
      "[447,   100] loss: 0.000\n",
      "[447,   150] loss: 0.001\n",
      "[447,   200] loss: 0.001\n",
      "[447,   250] loss: 0.002\n",
      "[448,    50] loss: 0.001\n",
      "[448,   100] loss: 0.001\n",
      "[448,   150] loss: 0.000\n",
      "[448,   200] loss: 0.002\n",
      "[448,   250] loss: 0.001\n",
      "[449,    50] loss: 0.002\n",
      "[449,   100] loss: 0.001\n",
      "[449,   150] loss: 0.001\n",
      "[449,   200] loss: 0.000\n",
      "[449,   250] loss: 0.001\n",
      "[450,    50] loss: 0.000\n",
      "[450,   100] loss: 0.003\n",
      "[450,   150] loss: 0.000\n",
      "[450,   200] loss: 0.000\n",
      "[450,   250] loss: 0.001\n",
      "[451,    50] loss: 0.000\n",
      "[451,   100] loss: 0.003\n",
      "[451,   150] loss: 0.000\n",
      "[451,   200] loss: 0.000\n",
      "[451,   250] loss: 0.001\n",
      "[452,    50] loss: 0.002\n",
      "[452,   100] loss: 0.000\n",
      "[452,   150] loss: 0.001\n",
      "[452,   200] loss: 0.001\n",
      "[452,   250] loss: 0.000\n",
      "[453,    50] loss: 0.002\n",
      "[453,   100] loss: 0.000\n",
      "[453,   150] loss: 0.001\n",
      "[453,   200] loss: 0.001\n",
      "[453,   250] loss: 0.001\n",
      "[454,    50] loss: 0.000\n",
      "[454,   100] loss: 0.001\n",
      "[454,   150] loss: 0.001\n",
      "[454,   200] loss: 0.000\n",
      "[454,   250] loss: 0.001\n",
      "[455,    50] loss: 0.001\n",
      "[455,   100] loss: 0.002\n",
      "[455,   150] loss: 0.000\n",
      "[455,   200] loss: 0.001\n",
      "[455,   250] loss: 0.001\n",
      "[456,    50] loss: 0.000\n",
      "[456,   100] loss: 0.000\n",
      "[456,   150] loss: 0.000\n",
      "[456,   200] loss: 0.001\n",
      "[456,   250] loss: 0.002\n",
      "[457,    50] loss: 0.001\n",
      "[457,   100] loss: 0.000\n",
      "[457,   150] loss: 0.003\n",
      "[457,   200] loss: 0.001\n",
      "[457,   250] loss: 0.000\n",
      "[458,    50] loss: 0.000\n",
      "[458,   100] loss: 0.001\n",
      "[458,   150] loss: 0.001\n",
      "[458,   200] loss: 0.002\n",
      "[458,   250] loss: 0.001\n",
      "[459,    50] loss: 0.001\n",
      "[459,   100] loss: 0.001\n",
      "[459,   150] loss: 0.002\n",
      "[459,   200] loss: 0.000\n",
      "[459,   250] loss: 0.001\n",
      "[460,    50] loss: 0.001\n",
      "[460,   100] loss: 0.000\n",
      "[460,   150] loss: 0.001\n",
      "[460,   200] loss: 0.002\n",
      "[460,   250] loss: 0.000\n",
      "[461,    50] loss: 0.001\n",
      "[461,   100] loss: 0.001\n",
      "[461,   150] loss: 0.000\n",
      "[461,   200] loss: 0.002\n",
      "[461,   250] loss: 0.000\n",
      "[462,    50] loss: 0.001\n",
      "[462,   100] loss: 0.001\n",
      "[462,   150] loss: 0.000\n",
      "[462,   200] loss: 0.001\n",
      "[462,   250] loss: 0.002\n",
      "[463,    50] loss: 0.001\n",
      "[463,   100] loss: 0.002\n",
      "[463,   150] loss: 0.000\n",
      "[463,   200] loss: 0.000\n",
      "[463,   250] loss: 0.002\n",
      "[464,    50] loss: 0.001\n",
      "[464,   100] loss: 0.001\n",
      "[464,   150] loss: 0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[464,   200] loss: 0.000\n",
      "[464,   250] loss: 0.001\n",
      "[465,    50] loss: 0.001\n",
      "[465,   100] loss: 0.002\n",
      "[465,   150] loss: 0.001\n",
      "[465,   200] loss: 0.000\n",
      "[465,   250] loss: 0.000\n",
      "[466,    50] loss: 0.001\n",
      "[466,   100] loss: 0.000\n",
      "[466,   150] loss: 0.000\n",
      "[466,   200] loss: 0.001\n",
      "[466,   250] loss: 0.001\n",
      "[467,    50] loss: 0.000\n",
      "[467,   100] loss: 0.000\n",
      "[467,   150] loss: 0.002\n",
      "[467,   200] loss: 0.001\n",
      "[467,   250] loss: 0.001\n",
      "[468,    50] loss: 0.000\n",
      "[468,   100] loss: 0.002\n",
      "[468,   150] loss: 0.000\n",
      "[468,   200] loss: 0.001\n",
      "[468,   250] loss: 0.001\n",
      "[469,    50] loss: 0.001\n",
      "[469,   100] loss: 0.001\n",
      "[469,   150] loss: 0.000\n",
      "[469,   200] loss: 0.000\n",
      "[469,   250] loss: 0.000\n",
      "[470,    50] loss: 0.001\n",
      "[470,   100] loss: 0.000\n",
      "[470,   150] loss: 0.000\n",
      "[470,   200] loss: 0.000\n",
      "[470,   250] loss: 0.002\n",
      "[471,    50] loss: 0.001\n",
      "[471,   100] loss: 0.001\n",
      "[471,   150] loss: 0.001\n",
      "[471,   200] loss: 0.000\n",
      "[471,   250] loss: 0.000\n",
      "[472,    50] loss: 0.001\n",
      "[472,   100] loss: 0.000\n",
      "[472,   150] loss: 0.000\n",
      "[472,   200] loss: 0.001\n",
      "[472,   250] loss: 0.002\n",
      "[473,    50] loss: 0.001\n",
      "[473,   100] loss: 0.000\n",
      "[473,   150] loss: 0.001\n",
      "[473,   200] loss: 0.000\n",
      "[473,   250] loss: 0.002\n",
      "[474,    50] loss: 0.002\n",
      "[474,   100] loss: 0.001\n",
      "[474,   150] loss: 0.000\n",
      "[474,   200] loss: 0.000\n",
      "[474,   250] loss: 0.001\n",
      "[475,    50] loss: 0.001\n",
      "[475,   100] loss: 0.002\n",
      "[475,   150] loss: 0.000\n",
      "[475,   200] loss: 0.001\n",
      "[475,   250] loss: 0.000\n",
      "[476,    50] loss: 0.000\n",
      "[476,   100] loss: 0.002\n",
      "[476,   150] loss: 0.001\n",
      "[476,   200] loss: 0.000\n",
      "[476,   250] loss: 0.000\n",
      "[477,    50] loss: 0.003\n",
      "[477,   100] loss: 0.000\n",
      "[477,   150] loss: 0.000\n",
      "[477,   200] loss: 0.000\n",
      "[477,   250] loss: 0.000\n",
      "[478,    50] loss: 0.000\n",
      "[478,   100] loss: 0.001\n",
      "[478,   150] loss: 0.001\n",
      "[478,   200] loss: 0.002\n",
      "[478,   250] loss: 0.000\n",
      "[479,    50] loss: 0.002\n",
      "[479,   100] loss: 0.001\n",
      "[479,   150] loss: 0.000\n",
      "[479,   200] loss: 0.000\n",
      "[479,   250] loss: 0.001\n",
      "[480,    50] loss: 0.000\n",
      "[480,   100] loss: 0.001\n",
      "[480,   150] loss: 0.000\n",
      "[480,   200] loss: 0.001\n",
      "[480,   250] loss: 0.000\n",
      "[481,    50] loss: 0.001\n",
      "[481,   100] loss: 0.000\n",
      "[481,   150] loss: 0.002\n",
      "[481,   200] loss: 0.000\n",
      "[481,   250] loss: 0.000\n",
      "[482,    50] loss: 0.001\n",
      "[482,   100] loss: 0.001\n",
      "[482,   150] loss: 0.000\n",
      "[482,   200] loss: 0.000\n",
      "[482,   250] loss: 0.001\n",
      "[483,    50] loss: 0.001\n",
      "[483,   100] loss: 0.002\n",
      "[483,   150] loss: 0.001\n",
      "[483,   200] loss: 0.001\n",
      "[483,   250] loss: 0.000\n",
      "[484,    50] loss: 0.000\n",
      "[484,   100] loss: 0.001\n",
      "[484,   150] loss: 0.000\n",
      "[484,   200] loss: 0.000\n",
      "[484,   250] loss: 0.001\n",
      "[485,    50] loss: 0.000\n",
      "[485,   100] loss: 0.002\n",
      "[485,   150] loss: 0.001\n",
      "[485,   200] loss: 0.001\n",
      "[485,   250] loss: 0.000\n",
      "[486,    50] loss: 0.000\n",
      "[486,   100] loss: 0.000\n",
      "[486,   150] loss: 0.001\n",
      "[486,   200] loss: 0.001\n",
      "[486,   250] loss: 0.002\n",
      "[487,    50] loss: 0.001\n",
      "[487,   100] loss: 0.003\n",
      "[487,   150] loss: 0.000\n",
      "[487,   200] loss: 0.000\n",
      "[487,   250] loss: 0.000\n",
      "[488,    50] loss: 0.001\n",
      "[488,   100] loss: 0.000\n",
      "[488,   150] loss: 0.002\n",
      "[488,   200] loss: 0.000\n",
      "[488,   250] loss: 0.001\n",
      "[489,    50] loss: 0.000\n",
      "[489,   100] loss: 0.002\n",
      "[489,   150] loss: 0.000\n",
      "[489,   200] loss: 0.000\n",
      "[489,   250] loss: 0.001\n",
      "[490,    50] loss: 0.001\n",
      "[490,   100] loss: 0.001\n",
      "[490,   150] loss: 0.000\n",
      "[490,   200] loss: 0.001\n",
      "[490,   250] loss: 0.000\n",
      "[491,    50] loss: 0.001\n",
      "[491,   100] loss: 0.000\n",
      "[491,   150] loss: 0.001\n",
      "[491,   200] loss: 0.000\n",
      "[491,   250] loss: 0.002\n",
      "[492,    50] loss: 0.001\n",
      "[492,   100] loss: 0.001\n",
      "[492,   150] loss: 0.000\n",
      "[492,   200] loss: 0.001\n",
      "[492,   250] loss: 0.000\n",
      "[493,    50] loss: 0.001\n",
      "[493,   100] loss: 0.002\n",
      "[493,   150] loss: 0.000\n",
      "[493,   200] loss: 0.000\n",
      "[493,   250] loss: 0.001\n",
      "[494,    50] loss: 0.001\n",
      "[494,   100] loss: 0.000\n",
      "[494,   150] loss: 0.001\n",
      "[494,   200] loss: 0.000\n",
      "[494,   250] loss: 0.002\n",
      "[495,    50] loss: 0.000\n",
      "[495,   100] loss: 0.001\n",
      "[495,   150] loss: 0.002\n",
      "[495,   200] loss: 0.001\n",
      "[495,   250] loss: 0.000\n",
      "[496,    50] loss: 0.001\n",
      "[496,   100] loss: 0.000\n",
      "[496,   150] loss: 0.000\n",
      "[496,   200] loss: 0.001\n",
      "[496,   250] loss: 0.001\n",
      "[497,    50] loss: 0.001\n",
      "[497,   100] loss: 0.000\n",
      "[497,   150] loss: 0.000\n",
      "[497,   200] loss: 0.001\n",
      "[497,   250] loss: 0.001\n",
      "[498,    50] loss: 0.001\n",
      "[498,   100] loss: 0.001\n",
      "[498,   150] loss: 0.000\n",
      "[498,   200] loss: 0.001\n",
      "[498,   250] loss: 0.000\n",
      "[499,    50] loss: 0.000\n",
      "[499,   100] loss: 0.002\n",
      "[499,   150] loss: 0.000\n",
      "[499,   200] loss: 0.001\n",
      "[499,   250] loss: 0.000\n",
      "[500,    50] loss: 0.000\n",
      "[500,   100] loss: 0.001\n",
      "[500,   150] loss: 0.000\n",
      "[500,   200] loss: 0.000\n",
      "[500,   250] loss: 0.002\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "nepoch = 500  # number of epochs\n",
    "print_num = 50\n",
    "for epoch in range(nepoch):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer2.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer2.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % print_num == (print_num-1):    # print every 50 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / print_num))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroundTruth:   FRII   FRI\n"
     ]
    }
   ],
   "source": [
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(batch_size_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 150, 150])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f72c06e1390>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_tensor = torch.tensor(np.zeros((1,1,150,150)))\n",
    "img_tensor[0,0,25:125,25:125]=sample1[num].reshape(100,100)\n",
    "plt.imshow(img_tensor[0,0,:,:].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net(img_tensor.float())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predicted = torch.max(outputs, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRII\n"
     ]
    }
   ],
   "source": [
    "print(classes[predicted[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]], dtype=torch.float64,\n",
       "       grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',\n",
       "       dtype=torch.float64, grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_tensor.cuda().double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = img_tensor.type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',\n",
       "       grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
