{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Variational Auto Encoder - Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.datasets as dset\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO, TraceEnum_ELBO, config_enumerate\n",
    "from pyro.optim import Adam\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.1'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyro.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.enable_validation(True)\n",
    "pyro.distributions.enable_validation(False)\n",
    "pyro.set_rng_seed(0)\n",
    "# Enable smoke test - run the notebook cells on CI.\n",
    "smoke_test = 'CI' in os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loading and batching MNIST dataset\n",
    "def setup_data_loaders(batch_size=128, use_cuda=True):\n",
    "    root = './data'\n",
    "    download = True\n",
    "    trans = transforms.ToTensor()\n",
    "    train_set = dset.MNIST(root=root, train=True, transform=trans,\n",
    "                           download=download)\n",
    "    test_set = dset.MNIST(root=root, train=False, transform=trans)\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': use_cuda}\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_set,\n",
    "        batch_size=batch_size, shuffle=False, **kwargs)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderZ(nn.Module):\n",
    "    #def __init__(self, z_dim, hidden_dim):\n",
    "    def __init__(self, x_dim, y_dim, h_dim, z_dim):\n",
    "        super(EncoderZ, self).__init__()\n",
    "        # setup the three linear transformations used\n",
    "        #have to here define the fully connected layers - 784 to 400\n",
    "        #400 to 2\n",
    "        \n",
    "        # setup the non-linearities\n",
    "        self.fc1 = nn.Linear(x_dim+y_dim, h_dim)  \n",
    "        self.fc21 = nn.Linear(h_dim, z_dim) \n",
    "        self.fc22 = nn.Linear(h_dim, z_dim)\n",
    "        \n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, x_y_2):\n",
    "        [x,y]=x_y_2\n",
    "        x = x.reshape(-1, 784) #@David Change this to reshape if something fucks up\n",
    "        y = y.reshape(-1, 10) #@David Change this to reshape if something fucks up\n",
    "        x_y_1 = torch.cat((x,y), dim=1) #I think that this should concatenate the two inputs if this does work then test it independenlty\n",
    "        x_y_1 = x_y_1.view(x_y_1.size(0), -1)\n",
    "        \n",
    "        hidden = self.softplus(self.fc1(x_y_1))\n",
    "        \n",
    "        z_loc = self.fc21(hidden)\n",
    "        z_scale = torch.exp(self.fc22(hidden)) # mu, log_var\n",
    "        return z_loc, z_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, x_dim, y_dim, h_dim, z_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        # setup the two linear transformations used\n",
    "        self.fc1 = nn.Linear(z_dim+y_dim, h_dim)\n",
    "        self.fc21 = nn.Linear(h_dim, x_dim)\n",
    "        # setup the non-linearities\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,z_y_2):\n",
    "        # define the forward computation on the latent z\n",
    "        # first compute the hidden units\n",
    "        \n",
    "        [z,y]=z_y_2\n",
    "        \n",
    "        z = z.reshape(-1, 10) #@David Change this to reshape if something fucks up\n",
    "        y = y.reshape(-1, 10)\n",
    "        z_y_1 = torch.cat((z,y), dim=1)\n",
    "        z_y_1 = z_y_1.view(z_y_1.size(0), -1)\n",
    "        hidden = self.softplus(self.fc1(z_y_1))\n",
    "        # return the parameter for the output Bernoulli\n",
    "        # each is of size batch_size x 784\n",
    "        loc_img = self.sigmoid(self.fc21(hidden))\n",
    "        return loc_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    # by default our latent space is 50-dimensional\n",
    "    # and we use 400 hidden units\n",
    "    def __init__(self, x_dim = 784, y_dim = 10, h_dim = 500, z_dim = 10,use_cuda=True):\n",
    "        super(VAE, self).__init__()\n",
    "    \n",
    "        # create the encoder and decoder networks\n",
    "        # a split in the final layer's size is used for multiple outputs\n",
    "        # and potentially applying separate activation functions on them\n",
    "        # e.g. in this network the final output is of size [z_dim,z_dim]\n",
    "        # to produce loc and scale, and apply different activations [None,Exp] on them\n",
    "              \n",
    "        self.encoder_z = EncoderZ(x_dim, y_dim, h_dim, z_dim)\n",
    "        \n",
    "        self.decoder = Decoder(x_dim, y_dim, h_dim, z_dim)\n",
    "\n",
    "        if use_cuda:\n",
    "            # calling cuda() here will put all the parameters of\n",
    "            # the encoder and decoder networks into gpu memory\n",
    "            self.cuda()\n",
    "            \n",
    "            \n",
    "        self.use_cuda = use_cuda\n",
    "        self.z_dim = z_dim\n",
    "        self.output_size = y_dim\n",
    "        \n",
    "        \n",
    "    # define the model p(x|z)p(z)\n",
    "    def model(self, xs, ys):\n",
    "        # register this pytorch module and all of its sub-modules with pyro\n",
    "        pyro.module(\"ss_vae\", self)\n",
    "        batch_size = xs.size(0)\n",
    "\n",
    "        # inform Pyro that the variables in the batch of xs, ys are conditionally independent\n",
    "        with pyro.plate(\"data\"):\n",
    "\n",
    "            # sample the handwriting style from the constant prior distribution\n",
    "            prior_loc = xs.new_zeros([batch_size, self.z_dim])\n",
    "            prior_scale = xs.new_ones([batch_size, self.z_dim])\n",
    "            zs = pyro.sample(\"z\", dist.Normal(prior_loc, prior_scale).to_event(1))\n",
    "\n",
    "            # if the label y (which digit to write) is supervised, sample from the\n",
    "            # constant prior, otherwise, observe the value (i.e. score it against the constant prior)\n",
    "            alpha_prior = xs.new_ones([batch_size, self.output_size]) / (1.0 * self.output_size)\n",
    "            ys = pyro.sample(\"y\", dist.OneHotCategorical(alpha_prior), obs=ys)\n",
    "            \n",
    "            # finally, score the image (x) using the handwriting style (z) and\n",
    "            # the class label y (which digit to write) against the\n",
    "            # parametrized distribution p(x|y,z) = bernoulli(decoder(y,z))\n",
    "            # where `decoder` is a neural network\n",
    "            loc = self.decoder.forward([zs, ys])\n",
    "            pyro.sample(\"x\", dist.Bernoulli(loc).to_event(1), obs=xs)\n",
    "            \n",
    "    def guide(self, xs, ys):\n",
    "        with pyro.plate(\"data\"):\n",
    "           # if the class label (the digit) is not supervised, sample\n",
    "           # (and score) the digit with the variational distribution\n",
    "           # q(y|x) = categorical(alpha(x))\n",
    "           \n",
    "            #-------------------REMOVED THIS PART FOR THE CLASSIFIER ASSUME ALL DATA ARE LABELLED---------\n",
    "\n",
    "           # sample (and score) the latent handwriting-style with the variational\n",
    "           # distribution q(z|x,y) = normal(loc(x,y),scale(x,y))\n",
    "           loc, scale = self.encoder_z.forward([xs, ys])\n",
    "           pyro.sample(\"z\", dist.Normal(loc, scale).to_event(1))\n",
    "\n",
    "    # define a helper function for reconstructing images\n",
    "    def reconstruct_img(self, xs, ys):\n",
    "        # encode image x\n",
    "        z_loc, z_scale = self.encoder_z.forward([xs,ys])\n",
    "        # sample in latent space\n",
    "        z = dist.Normal(z_loc, z_scale).sample()\n",
    "        # decode the image (note we don't sample in image space)\n",
    "        loc_img = self.decoder.forward([zs,ys])\n",
    "        \n",
    "        return loc_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(svi, train_loader, use_cuda=True):\n",
    "    # initialize loss accumulator\n",
    "    epoch_loss = 0.\n",
    "    # do a training epoch over each mini-batch x returned\n",
    "    # by the data loader\n",
    "    for x, y in train_loader:\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "        # do ELBO gradient and accumulate loss\n",
    "        labels_y = torch.tensor(np.zeros((y.shape[0],10)))\n",
    "        for j in range (0,y.shape[0]):\n",
    "            labels_y[j,int(y[j].numpy())] = 1\n",
    "        epoch_loss += svi.step(x.reshape(-1,784),labels_y.float())\n",
    "\n",
    "    # return epoch loss\n",
    "    normalizer_train = len(train_loader.dataset)\n",
    "    total_epoch_loss_train = epoch_loss / normalizer_train\n",
    "    return total_epoch_loss_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_image_sampler(z,num):\n",
    "    labels_y = torch.tensor(np.zeros((100,10)))\n",
    "    labels_y[:,num] = 1\n",
    "    single_sample_image = vae.decoder([z,labels_y.float()])\n",
    "    image_array_single =single_sample_image.reshape(100,28,28).cpu().detach().numpy()\n",
    "    temp_array=image_array_single\n",
    "  #  plt.figure(figsize = (10,10))\n",
    "  #  plt.imshow(image_array_single)\n",
    " #   plt.colorbar()\n",
    " #   plt.show() \n",
    "    return image_array_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(num):\n",
    "    z = torch.rand(100,10)\n",
    "    z[:,0] = torch.from_numpy(np.random.uniform(-2,2,size=(100))).float().to('cpu') \n",
    "    z[:,1] = torch.from_numpy(np.random.uniform(-2,2,size=(100))).float().to('cpu') \n",
    "    z[:,2] = torch.from_numpy(np.random.uniform(-0.5,0.5,size=(100))).float().to('cpu') \n",
    "    z[:,3] = torch.from_numpy(np.random.uniform(-0.2,0.4,size=(100))).float().to('cpu') \n",
    "    z[:,4] = torch.from_numpy(np.random.uniform(-1,1,size=(100))).float().to('cpu') \n",
    "    z[:,5] = torch.from_numpy(np.random.uniform(-0.1,0.1,size=(100))).float().to('cpu') \n",
    "    z[:,6] = torch.from_numpy(np.random.uniform(-0.1,0.2,size=(100))).float().to('cpu') \n",
    "    z[:,7] = torch.from_numpy(np.random.uniform(-2.3,2.5,size=(100))).float().to('cpu') \n",
    "    z[:,8] = torch.from_numpy(np.random.uniform(-0.2,0.2,size=(100))).float().to('cpu') \n",
    "    z[:,9] = torch.from_numpy(np.random.uniform(-0.2,0.2,size=(100))).float().to('cpu')\n",
    "    image = single_image_sampler(z,num)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_image = generate_images(7)\n",
    "plt.imshow(temp_image[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = MNIST('./data', train=True, download=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(), # ToTensor does min-max normalization. \n",
    "]), )\n",
    "\n",
    "test = MNIST('./data', train=False, download=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(), # ToTensor does min-max normalization. \n",
    "]), )\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader_args = dict(shuffle=True, batch_size=64,num_workers=1, pin_memory=True)\n",
    "train_loader = dataloader.DataLoader(train, **dataloader_args)\n",
    "test_loader = dataloader.DataLoader(test, **dataloader_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda-python-3.6/lib/python3.6/site-packages/torchvision/datasets/mnist.py:53: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train]\n",
      " - Numpy Shape: (60000, 28, 28)\n",
      " - Tensor Shape: torch.Size([60000, 28, 28])\n",
      " - Transformed Shape: torch.Size([28, 60000, 28])\n",
      " - min: tensor(0.)\n",
      " - max: tensor(1.)\n",
      " - mean: tensor(0.1307)\n",
      " - std: tensor(0.3081)\n",
      " - var: tensor(0.0949)\n"
     ]
    }
   ],
   "source": [
    "train_data = train.train_data\n",
    "train_data = train.transform(train_data.numpy())\n",
    "\n",
    "print('[Train]')\n",
    "print(' - Numpy Shape:', train.train_data.cpu().numpy().shape)\n",
    "print(' - Tensor Shape:', train.train_data.size())\n",
    "print(' - Transformed Shape:', train_data.size())\n",
    "print(' - min:', torch.min(train_data))\n",
    "print(' - max:', torch.max(train_data))\n",
    "print(' - mean:', torch.mean(train_data))\n",
    "print(' - std:', torch.std(train_data))\n",
    "print(' - var:', torch.var(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(784, 548)\n",
    "        self.bc1 = nn.BatchNorm1d(548)\n",
    "        \n",
    "        self.fc2 = nn.Linear(548, 252)\n",
    "        self.bc2 = nn.BatchNorm1d(252)\n",
    "        \n",
    "        self.fc3 = nn.Linear(252, 10)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view((-1, 784))\n",
    "        h = self.fc1(x)\n",
    "        h = self.bc1(h)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        \n",
    "        h = self.fc2(h)\n",
    "        h = self.bc2(h)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=0.2, training=self.training)\n",
    "        \n",
    "        h = self.fc3(h)\n",
    "        out = F.log_softmax(h)\n",
    "        return out\n",
    "\n",
    "model = Model()\n",
    "model.cuda() # CUDA!\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda-python-3.6/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train Epoch: 0 [57664/60000 (96%)]\tLoss: 0.129184\n",
      " Train Epoch: 1 [57664/60000 (96%)]\tLoss: 0.102641\n",
      " Train Epoch: 2 [57664/60000 (96%)]\tLoss: 0.079456\n",
      " Train Epoch: 3 [57664/60000 (96%)]\tLoss: 0.039955\n",
      " Train Epoch: 4 [57664/60000 (96%)]\tLoss: 0.048989\n",
      " Train Epoch: 5 [57664/60000 (96%)]\tLoss: 0.059902\n",
      " Train Epoch: 6 [57664/60000 (96%)]\tLoss: 0.052282\n",
      " Train Epoch: 7 [57664/60000 (96%)]\tLoss: 0.087476\n",
      " Train Epoch: 8 [57664/60000 (96%)]\tLoss: 0.048985\n",
      " Train Epoch: 9 [57664/60000 (96%)]\tLoss: 0.138384\n",
      " Train Epoch: 10 [57664/60000 (96%)]\tLoss: 0.123397\n",
      " Train Epoch: 11 [57664/60000 (96%)]\tLoss: 0.073971\n",
      " Train Epoch: 12 [57664/60000 (96%)]\tLoss: 0.059660\n",
      " Train Epoch: 13 [57664/60000 (96%)]\tLoss: 0.025509\n",
      " Train Epoch: 14 [57664/60000 (96%)]\tLoss: 0.045962\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "losses = []\n",
    "for epoch in range(15):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Get Samples\n",
    "        data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        \n",
    "        # Init\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Predict\n",
    "        y_pred = model(data) \n",
    "\n",
    "        # Calculate loss\n",
    "        loss = F.cross_entropy(y_pred, target)\n",
    "        losses.append(loss.data)\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        # Display\n",
    "        if batch_idx % 100 == 1:\n",
    "            print('\\r Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, \n",
    "                batch_idx * len(data), \n",
    "                len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), \n",
    "                loss.data), \n",
    "                end='')\n",
    "            \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f78b9440b38>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda-python-3.6/lib/python3.6/site-packages/torchvision/datasets/mnist.py:58: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "/usr/local/anaconda-python-3.6/lib/python3.6/site-packages/torchvision/datasets/mnist.py:48: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n",
      "/usr/local/anaconda-python-3.6/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/usr/local/anaconda-python-3.6/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "evaluate_x = Variable(test_loader.dataset.test_data.type_as(torch.FloatTensor())).cuda()\n",
    "evaluate_y = Variable(test_loader.dataset.test_labels).cuda()\n",
    "\n",
    "\n",
    "output = model(evaluate_x)\n",
    "pred = output.data.max(1)[1]\n",
    "d = pred.eq(evaluate_y.data).cpu()\n",
    "accuracy = np.asscalar(d.sum().cpu().detach().numpy().astype(np.float))/d.size()[0]\n",
    "\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_x = torch.from_numpy(image).float().to('cpu') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda-python-3.6/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "output = model(evaluate_x.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = output.data.max(1)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a procedure that:\n",
    "   1. Generate the Images of each class (hence first looping scructure of i from 0 to 9)\n",
    "   2. Input each result in the neural network and generate classification)\n",
    "   3. Concatenate all the results\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 000]  average training loss: 164.3112 average testing loss: 132.3595 Classification Accuracy: 0.7070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda-python-3.6/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 001]  average training loss: 126.0701 average testing loss: 120.7438 Classification Accuracy: 0.8030\n",
      "[epoch 002]  average training loss: 118.9444 average testing loss: 116.8044 Classification Accuracy: 0.8640\n",
      "[epoch 003]  average training loss: 115.2021 average testing loss: 114.0697 Classification Accuracy: 0.9070\n",
      "[epoch 004]  average training loss: 112.8356 average testing loss: 111.4765 Classification Accuracy: 0.9320\n",
      "[epoch 005]  average training loss: 111.2251 average testing loss: 110.1591 Classification Accuracy: 0.9390\n",
      "[epoch 006]  average training loss: 109.8835 average testing loss: 108.9575 Classification Accuracy: 0.9540\n",
      "[epoch 007]  average training loss: 108.9108 average testing loss: 108.4142 Classification Accuracy: 0.9700\n",
      "[epoch 008]  average training loss: 108.1024 average testing loss: 107.7121 Classification Accuracy: 0.9750\n",
      "[epoch 009]  average training loss: 107.3393 average testing loss: 106.8514 Classification Accuracy: 0.9750\n",
      "[epoch 010]  average training loss: 106.8041 average testing loss: 106.5010 Classification Accuracy: 0.9860\n",
      "[epoch 011]  average training loss: 106.1948 average testing loss: 105.9720 Classification Accuracy: 0.9740\n",
      "[epoch 012]  average training loss: 105.7042 average testing loss: 105.6877 Classification Accuracy: 0.9870\n",
      "[epoch 013]  average training loss: 105.2869 average testing loss: 105.2241 Classification Accuracy: 0.9790\n",
      "[epoch 014]  average training loss: 104.8830 average testing loss: 104.8552 Classification Accuracy: 0.9880\n",
      "[epoch 015]  average training loss: 104.4828 average testing loss: 104.4339 Classification Accuracy: 0.9910\n",
      "[epoch 016]  average training loss: 104.1239 average testing loss: 104.0805 Classification Accuracy: 0.9810\n",
      "[epoch 017]  average training loss: 103.8362 average testing loss: 103.6865 Classification Accuracy: 0.9920\n",
      "[epoch 018]  average training loss: 103.4364 average testing loss: 103.4676 Classification Accuracy: 0.9900\n",
      "[epoch 019]  average training loss: 103.1488 average testing loss: 103.0392 Classification Accuracy: 0.9850\n",
      "[epoch 020]  average training loss: 102.9326 average testing loss: 102.8762 Classification Accuracy: 0.9850\n",
      "[epoch 021]  average training loss: 102.6815 average testing loss: 103.0367 Classification Accuracy: 0.9910\n",
      "[epoch 022]  average training loss: 102.3744 average testing loss: 102.4689 Classification Accuracy: 0.9850\n",
      "[epoch 023]  average training loss: 102.2013 average testing loss: 103.1202 Classification Accuracy: 0.9900\n",
      "[epoch 024]  average training loss: 101.9332 average testing loss: 102.3511 Classification Accuracy: 0.9890\n",
      "[epoch 025]  average training loss: 101.7131 average testing loss: 102.3472 Classification Accuracy: 0.9890\n",
      "[epoch 026]  average training loss: 101.5150 average testing loss: 102.0159 Classification Accuracy: 0.9850\n",
      "[epoch 027]  average training loss: 101.3976 average testing loss: 101.7621 Classification Accuracy: 0.9910\n",
      "[epoch 028]  average training loss: 101.2185 average testing loss: 101.9352 Classification Accuracy: 0.9880\n",
      "[epoch 029]  average training loss: 101.0631 average testing loss: 101.6344 Classification Accuracy: 0.9940\n",
      "[epoch 030]  average training loss: 100.8695 average testing loss: 101.2359 Classification Accuracy: 0.9850\n",
      "[epoch 031]  average training loss: 100.7199 average testing loss: 101.4438 Classification Accuracy: 0.9880\n",
      "[epoch 032]  average training loss: 100.6079 average testing loss: 101.5469 Classification Accuracy: 0.9890\n",
      "[epoch 033]  average training loss: 100.4610 average testing loss: 100.9911 Classification Accuracy: 0.9940\n",
      "[epoch 034]  average training loss: 100.3643 average testing loss: 101.5807 Classification Accuracy: 0.9890\n",
      "[epoch 035]  average training loss: 100.2583 average testing loss: 101.1489 Classification Accuracy: 0.9920\n",
      "[epoch 036]  average training loss: 100.0952 average testing loss: 100.9902 Classification Accuracy: 0.9930\n",
      "[epoch 037]  average training loss: 99.9919 average testing loss: 101.2592 Classification Accuracy: 0.9920\n",
      "[epoch 038]  average training loss: 99.8758 average testing loss: 100.8662 Classification Accuracy: 0.9880\n",
      "[epoch 039]  average training loss: 99.7572 average testing loss: 100.5900 Classification Accuracy: 0.9910\n",
      "[epoch 040]  average training loss: 99.6933 average testing loss: 100.6509 Classification Accuracy: 0.9950\n",
      "[epoch 041]  average training loss: 99.5802 average testing loss: 100.3040 Classification Accuracy: 0.9950\n",
      "[epoch 042]  average training loss: 99.4851 average testing loss: 100.4526 Classification Accuracy: 0.9950\n",
      "[epoch 043]  average training loss: 99.4612 average testing loss: 100.7113 Classification Accuracy: 0.9950\n",
      "[epoch 044]  average training loss: 99.3675 average testing loss: 100.4672 Classification Accuracy: 0.9960\n",
      "[epoch 045]  average training loss: 99.2784 average testing loss: 100.3291 Classification Accuracy: 0.9910\n",
      "[epoch 046]  average training loss: 99.1606 average testing loss: 100.9057 Classification Accuracy: 0.9990\n",
      "[epoch 047]  average training loss: 99.1251 average testing loss: 100.0666 Classification Accuracy: 0.9970\n",
      "[epoch 048]  average training loss: 99.0194 average testing loss: 100.2473 Classification Accuracy: 0.9960\n",
      "[epoch 049]  average training loss: 98.9582 average testing loss: 99.8264 Classification Accuracy: 0.9990\n",
      "[epoch 050]  average training loss: 98.8605 average testing loss: 100.1002 Classification Accuracy: 0.9910\n",
      "[epoch 051]  average training loss: 98.8216 average testing loss: 100.5050 Classification Accuracy: 0.9920\n",
      "[epoch 052]  average training loss: 98.7392 average testing loss: 100.1114 Classification Accuracy: 0.9960\n",
      "[epoch 053]  average training loss: 98.6905 average testing loss: 99.8909 Classification Accuracy: 0.9950\n",
      "[epoch 054]  average training loss: 98.6368 average testing loss: 99.8105 Classification Accuracy: 0.9970\n",
      "[epoch 055]  average training loss: 98.5397 average testing loss: 99.9213 Classification Accuracy: 0.9930\n",
      "[epoch 056]  average training loss: 98.5233 average testing loss: 100.0894 Classification Accuracy: 0.9940\n",
      "[epoch 057]  average training loss: 98.4494 average testing loss: 99.8813 Classification Accuracy: 0.9960\n",
      "[epoch 058]  average training loss: 98.3890 average testing loss: 99.5905 Classification Accuracy: 0.9960\n",
      "[epoch 059]  average training loss: 98.3266 average testing loss: 99.8756 Classification Accuracy: 0.9960\n",
      "[epoch 060]  average training loss: 98.2758 average testing loss: 99.5660 Classification Accuracy: 1.0000\n",
      "[epoch 061]  average training loss: 98.2425 average testing loss: 99.9109 Classification Accuracy: 0.9980\n",
      "[epoch 062]  average training loss: 98.2046 average testing loss: 99.3365 Classification Accuracy: 0.9940\n",
      "[epoch 063]  average training loss: 98.1103 average testing loss: 99.7593 Classification Accuracy: 0.9960\n",
      "[epoch 064]  average training loss: 98.0865 average testing loss: 99.7937 Classification Accuracy: 0.9970\n",
      "[epoch 065]  average training loss: 98.0466 average testing loss: 99.2517 Classification Accuracy: 0.9950\n",
      "[epoch 066]  average training loss: 98.0144 average testing loss: 99.5309 Classification Accuracy: 0.9990\n",
      "[epoch 067]  average training loss: 97.9629 average testing loss: 99.1986 Classification Accuracy: 0.9980\n",
      "[epoch 068]  average training loss: 97.8501 average testing loss: 99.7839 Classification Accuracy: 0.9990\n",
      "[epoch 069]  average training loss: 97.8662 average testing loss: 99.7957 Classification Accuracy: 0.9960\n",
      "[epoch 070]  average training loss: 97.8405 average testing loss: 99.4972 Classification Accuracy: 0.9920\n",
      "[epoch 071]  average training loss: 97.7628 average testing loss: 99.3356 Classification Accuracy: 0.9960\n",
      "[epoch 072]  average training loss: 97.7598 average testing loss: 99.6019 Classification Accuracy: 0.9950\n",
      "[epoch 073]  average training loss: 97.7007 average testing loss: 99.3784 Classification Accuracy: 0.9930\n",
      "[epoch 074]  average training loss: 97.7254 average testing loss: 99.0911 Classification Accuracy: 0.9950\n",
      "[epoch 075]  average training loss: 97.6535 average testing loss: 99.1196 Classification Accuracy: 0.9970\n",
      "[epoch 076]  average training loss: 97.5818 average testing loss: 99.3074 Classification Accuracy: 0.9940\n",
      "[epoch 077]  average training loss: 97.5636 average testing loss: 99.3514 Classification Accuracy: 0.9970\n",
      "[epoch 078]  average training loss: 97.4981 average testing loss: 99.2616 Classification Accuracy: 0.9980\n",
      "[epoch 079]  average training loss: 97.4885 average testing loss: 99.3318 Classification Accuracy: 0.9980\n",
      "[epoch 080]  average training loss: 97.4897 average testing loss: 99.5785 Classification Accuracy: 0.9960\n",
      "[epoch 081]  average training loss: 97.4558 average testing loss: 99.3105 Classification Accuracy: 1.0000\n",
      "[epoch 082]  average training loss: 97.4009 average testing loss: 99.4775 Classification Accuracy: 0.9960\n",
      "[epoch 083]  average training loss: 97.3590 average testing loss: 98.9468 Classification Accuracy: 0.9970\n",
      "[epoch 084]  average training loss: 97.3231 average testing loss: 98.8648 Classification Accuracy: 0.9980\n",
      "[epoch 085]  average training loss: 97.3097 average testing loss: 98.8303 Classification Accuracy: 0.9940\n",
      "[epoch 086]  average training loss: 97.2446 average testing loss: 98.9894 Classification Accuracy: 0.9990\n",
      "[epoch 087]  average training loss: 97.2653 average testing loss: 98.9370 Classification Accuracy: 0.9950\n",
      "[epoch 088]  average training loss: 97.1950 average testing loss: 99.0790 Classification Accuracy: 0.9930\n",
      "[epoch 089]  average training loss: 97.1867 average testing loss: 99.1348 Classification Accuracy: 0.9990\n",
      "[epoch 090]  average training loss: 97.1370 average testing loss: 98.8306 Classification Accuracy: 0.9960\n",
      "[epoch 091]  average training loss: 97.1786 average testing loss: 99.0786 Classification Accuracy: 0.9980\n",
      "[epoch 092]  average training loss: 97.1061 average testing loss: 99.1767 Classification Accuracy: 0.9950\n",
      "[epoch 093]  average training loss: 97.0342 average testing loss: 98.9216 Classification Accuracy: 0.9930\n",
      "[epoch 094]  average training loss: 97.0796 average testing loss: 98.9600 Classification Accuracy: 0.9980\n",
      "[epoch 095]  average training loss: 97.0242 average testing loss: 99.0885 Classification Accuracy: 0.9960\n",
      "[epoch 096]  average training loss: 97.0194 average testing loss: 99.0998 Classification Accuracy: 0.9960\n",
      "[epoch 097]  average training loss: 96.9701 average testing loss: 99.1656 Classification Accuracy: 0.9940\n",
      "[epoch 098]  average training loss: 96.9485 average testing loss: 98.7957 Classification Accuracy: 0.9950\n",
      "[epoch 099]  average training loss: 96.9205 average testing loss: 99.1977 Classification Accuracy: 0.9970\n",
      "[epoch 100]  average training loss: 96.9103 average testing loss: 98.7895 Classification Accuracy: 0.9920\n",
      "[epoch 101]  average training loss: 96.8953 average testing loss: 98.8046 Classification Accuracy: 0.9970\n",
      "[epoch 102]  average training loss: 96.8658 average testing loss: 98.8199 Classification Accuracy: 0.9990\n",
      "[epoch 103]  average training loss: 96.8627 average testing loss: 98.8237 Classification Accuracy: 0.9990\n",
      "[epoch 104]  average training loss: 96.8341 average testing loss: 98.5712 Classification Accuracy: 0.9930\n",
      "[epoch 105]  average training loss: 96.7968 average testing loss: 98.9899 Classification Accuracy: 0.9950\n",
      "[epoch 106]  average training loss: 96.7721 average testing loss: 98.9925 Classification Accuracy: 0.9950\n",
      "[epoch 107]  average training loss: 96.7692 average testing loss: 98.6071 Classification Accuracy: 0.9950\n",
      "[epoch 108]  average training loss: 96.7150 average testing loss: 98.6401 Classification Accuracy: 0.9980\n",
      "[epoch 109]  average training loss: 96.6854 average testing loss: 98.9398 Classification Accuracy: 0.9980\n",
      "[epoch 110]  average training loss: 96.7402 average testing loss: 98.6616 Classification Accuracy: 0.9980\n",
      "[epoch 111]  average training loss: 96.6573 average testing loss: 98.6750 Classification Accuracy: 0.9970\n",
      "[epoch 112]  average training loss: 96.6752 average testing loss: 99.0318 Classification Accuracy: 0.9980\n",
      "[epoch 113]  average training loss: 96.6269 average testing loss: 98.7605 Classification Accuracy: 0.9980\n",
      "[epoch 114]  average training loss: 96.6457 average testing loss: 98.7422 Classification Accuracy: 0.9960\n",
      "[epoch 115]  average training loss: 96.5887 average testing loss: 98.9078 Classification Accuracy: 0.9970\n",
      "[epoch 116]  average training loss: 96.5946 average testing loss: 98.9310 Classification Accuracy: 0.9980\n",
      "[epoch 117]  average training loss: 96.5480 average testing loss: 98.9644 Classification Accuracy: 0.9930\n",
      "[epoch 118]  average training loss: 96.5532 average testing loss: 98.8272 Classification Accuracy: 0.9970\n",
      "[epoch 119]  average training loss: 96.5033 average testing loss: 98.6406 Classification Accuracy: 0.9990\n",
      "[epoch 120]  average training loss: 96.4536 average testing loss: 98.7568 Classification Accuracy: 0.9940\n",
      "[epoch 121]  average training loss: 96.4277 average testing loss: 98.5241 Classification Accuracy: 0.9990\n",
      "[epoch 122]  average training loss: 96.5012 average testing loss: 98.9151 Classification Accuracy: 0.9960\n",
      "[epoch 123]  average training loss: 96.4625 average testing loss: 99.0218 Classification Accuracy: 0.9980\n",
      "[epoch 124]  average training loss: 96.4832 average testing loss: 98.8764 Classification Accuracy: 1.0000\n",
      "[epoch 125]  average training loss: 96.4487 average testing loss: 98.6223 Classification Accuracy: 0.9970\n",
      "[epoch 126]  average training loss: 96.3881 average testing loss: 99.0283 Classification Accuracy: 0.9980\n",
      "[epoch 127]  average training loss: 96.3913 average testing loss: 98.6266 Classification Accuracy: 0.9950\n",
      "[epoch 128]  average training loss: 96.3889 average testing loss: 98.4791 Classification Accuracy: 0.9960\n",
      "[epoch 129]  average training loss: 96.3606 average testing loss: 98.6681 Classification Accuracy: 0.9980\n",
      "[epoch 130]  average training loss: 96.3622 average testing loss: 98.7896 Classification Accuracy: 0.9950\n",
      "[epoch 131]  average training loss: 96.3512 average testing loss: 98.5358 Classification Accuracy: 0.9980\n",
      "[epoch 132]  average training loss: 96.2724 average testing loss: 98.8300 Classification Accuracy: 0.9970\n",
      "[epoch 133]  average training loss: 96.2714 average testing loss: 98.5697 Classification Accuracy: 0.9940\n",
      "[epoch 134]  average training loss: 96.2867 average testing loss: 98.4241 Classification Accuracy: 0.9980\n",
      "[epoch 135]  average training loss: 96.2330 average testing loss: 98.7107 Classification Accuracy: 0.9970\n",
      "[epoch 136]  average training loss: 96.2824 average testing loss: 98.7227 Classification Accuracy: 0.9950\n",
      "[epoch 137]  average training loss: 96.2449 average testing loss: 98.8752 Classification Accuracy: 0.9980\n",
      "[epoch 138]  average training loss: 96.2054 average testing loss: 99.0140 Classification Accuracy: 0.9990\n",
      "[epoch 139]  average training loss: 96.2370 average testing loss: 98.7097 Classification Accuracy: 0.9950\n",
      "[epoch 140]  average training loss: 96.2482 average testing loss: 98.5940 Classification Accuracy: 0.9970\n",
      "[epoch 141]  average training loss: 96.1955 average testing loss: 98.6499 Classification Accuracy: 0.9970\n",
      "[epoch 142]  average training loss: 96.1686 average testing loss: 98.6509 Classification Accuracy: 0.9990\n",
      "[epoch 143]  average training loss: 96.1464 average testing loss: 98.5292 Classification Accuracy: 0.9980\n",
      "[epoch 144]  average training loss: 96.1408 average testing loss: 98.8817 Classification Accuracy: 1.0000\n",
      "[epoch 145]  average training loss: 96.1101 average testing loss: 98.6015 Classification Accuracy: 0.9970\n",
      "[epoch 146]  average training loss: 96.0900 average testing loss: 98.4596 Classification Accuracy: 0.9960\n",
      "[epoch 147]  average training loss: 96.1221 average testing loss: 98.5312 Classification Accuracy: 0.9970\n",
      "[epoch 148]  average training loss: 96.0409 average testing loss: 98.5799 Classification Accuracy: 0.9970\n",
      "[epoch 149]  average training loss: 96.0774 average testing loss: 98.6589 Classification Accuracy: 0.9960\n",
      "[epoch 150]  average training loss: 96.0675 average testing loss: 98.5760 Classification Accuracy: 0.9930\n",
      "[epoch 151]  average training loss: 96.0692 average testing loss: 98.5118 Classification Accuracy: 0.9990\n",
      "[epoch 152]  average training loss: 96.0323 average testing loss: 98.8854 Classification Accuracy: 0.9960\n",
      "[epoch 153]  average training loss: 96.0821 average testing loss: 99.0038 Classification Accuracy: 0.9950\n",
      "[epoch 154]  average training loss: 96.0627 average testing loss: 98.6207 Classification Accuracy: 0.9970\n",
      "[epoch 155]  average training loss: 96.0192 average testing loss: 98.7032 Classification Accuracy: 0.9980\n",
      "[epoch 156]  average training loss: 95.9657 average testing loss: 98.7965 Classification Accuracy: 0.9970\n",
      "[epoch 157]  average training loss: 95.9640 average testing loss: 98.6556 Classification Accuracy: 0.9990\n",
      "[epoch 158]  average training loss: 95.9882 average testing loss: 98.6244 Classification Accuracy: 0.9980\n",
      "[epoch 159]  average training loss: 95.9745 average testing loss: 98.7255 Classification Accuracy: 0.9970\n",
      "[epoch 160]  average training loss: 95.9860 average testing loss: 98.4240 Classification Accuracy: 0.9950\n",
      "[epoch 161]  average training loss: 95.9907 average testing loss: 98.5786 Classification Accuracy: 0.9970\n",
      "[epoch 162]  average training loss: 95.9745 average testing loss: 98.4997 Classification Accuracy: 0.9960\n",
      "[epoch 163]  average training loss: 95.9535 average testing loss: 98.8470 Classification Accuracy: 0.9910\n",
      "[epoch 164]  average training loss: 95.9253 average testing loss: 98.9403 Classification Accuracy: 0.9950\n",
      "[epoch 165]  average training loss: 95.9174 average testing loss: 98.6442 Classification Accuracy: 0.9970\n",
      "[epoch 166]  average training loss: 95.9157 average testing loss: 98.7825 Classification Accuracy: 0.9970\n",
      "[epoch 167]  average training loss: 95.8925 average testing loss: 98.5218 Classification Accuracy: 0.9960\n",
      "[epoch 168]  average training loss: 95.8739 average testing loss: 98.7819 Classification Accuracy: 0.9960\n",
      "[epoch 169]  average training loss: 95.8820 average testing loss: 98.7631 Classification Accuracy: 0.9990\n",
      "[epoch 170]  average training loss: 95.8814 average testing loss: 98.6541 Classification Accuracy: 0.9990\n",
      "[epoch 171]  average training loss: 95.8380 average testing loss: 99.0533 Classification Accuracy: 0.9990\n",
      "[epoch 172]  average training loss: 95.8630 average testing loss: 98.3869 Classification Accuracy: 0.9970\n",
      "[epoch 173]  average training loss: 95.8361 average testing loss: 98.6544 Classification Accuracy: 0.9970\n",
      "[epoch 174]  average training loss: 95.8226 average testing loss: 98.3704 Classification Accuracy: 0.9970\n",
      "[epoch 175]  average training loss: 95.8456 average testing loss: 98.4357 Classification Accuracy: 0.9980\n",
      "[epoch 176]  average training loss: 95.7569 average testing loss: 98.4749 Classification Accuracy: 0.9950\n",
      "[epoch 177]  average training loss: 95.7645 average testing loss: 98.3993 Classification Accuracy: 0.9940\n",
      "[epoch 178]  average training loss: 95.7575 average testing loss: 98.5565 Classification Accuracy: 0.9970\n",
      "[epoch 179]  average training loss: 95.7476 average testing loss: 98.3518 Classification Accuracy: 0.9960\n",
      "[epoch 180]  average training loss: 95.7912 average testing loss: 98.6076 Classification Accuracy: 0.9970\n",
      "[epoch 181]  average training loss: 95.7764 average testing loss: 98.3370 Classification Accuracy: 0.9970\n",
      "[epoch 182]  average training loss: 95.7126 average testing loss: 98.6285 Classification Accuracy: 0.9970\n",
      "[epoch 183]  average training loss: 95.7483 average testing loss: 98.8740 Classification Accuracy: 0.9940\n",
      "[epoch 184]  average training loss: 95.7514 average testing loss: 98.3716 Classification Accuracy: 0.9980\n",
      "[epoch 185]  average training loss: 95.7209 average testing loss: 98.4814 Classification Accuracy: 0.9960\n",
      "[epoch 186]  average training loss: 95.7145 average testing loss: 98.4108 Classification Accuracy: 0.9960\n",
      "[epoch 187]  average training loss: 95.6844 average testing loss: 98.7695 Classification Accuracy: 0.9950\n",
      "[epoch 188]  average training loss: 95.6979 average testing loss: 98.4786 Classification Accuracy: 0.9970\n",
      "[epoch 189]  average training loss: 95.6731 average testing loss: 98.6599 Classification Accuracy: 0.9940\n",
      "[epoch 190]  average training loss: 95.7053 average testing loss: 98.5124 Classification Accuracy: 0.9940\n",
      "[epoch 191]  average training loss: 95.6716 average testing loss: 98.7865 Classification Accuracy: 0.9970\n",
      "[epoch 192]  average training loss: 95.6528 average testing loss: 98.3974 Classification Accuracy: 0.9960\n",
      "[epoch 193]  average training loss: 95.6751 average testing loss: 98.5046 Classification Accuracy: 0.9980\n",
      "[epoch 194]  average training loss: 95.6117 average testing loss: 98.4717 Classification Accuracy: 0.9980\n",
      "[epoch 195]  average training loss: 95.6327 average testing loss: 98.5797 Classification Accuracy: 0.9960\n",
      "[epoch 196]  average training loss: 95.6156 average testing loss: 98.4519 Classification Accuracy: 0.9910\n",
      "[epoch 197]  average training loss: 95.6022 average testing loss: 98.4410 Classification Accuracy: 0.9970\n",
      "[epoch 198]  average training loss: 95.5954 average testing loss: 98.5003 Classification Accuracy: 0.9970\n",
      "[epoch 199]  average training loss: 95.6030 average testing loss: 98.6884 Classification Accuracy: 0.9970\n",
      "[epoch 200]  average training loss: 95.6110 average testing loss: 98.5675 Classification Accuracy: 0.9960\n",
      "[epoch 201]  average training loss: 95.5880 average testing loss: 98.3567 Classification Accuracy: 0.9980\n",
      "[epoch 202]  average training loss: 95.5749 average testing loss: 98.8316 Classification Accuracy: 0.9910\n",
      "[epoch 203]  average training loss: 95.5626 average testing loss: 98.5769 Classification Accuracy: 0.9990\n",
      "[epoch 204]  average training loss: 95.5508 average testing loss: 98.5676 Classification Accuracy: 0.9980\n",
      "[epoch 205]  average training loss: 95.5778 average testing loss: 98.5444 Classification Accuracy: 0.9980\n",
      "[epoch 206]  average training loss: 95.5594 average testing loss: 98.8111 Classification Accuracy: 0.9990\n",
      "[epoch 207]  average training loss: 95.5105 average testing loss: 98.5355 Classification Accuracy: 0.9970\n",
      "[epoch 208]  average training loss: 95.5295 average testing loss: 98.3575 Classification Accuracy: 0.9980\n",
      "[epoch 209]  average training loss: 95.5113 average testing loss: 98.4268 Classification Accuracy: 0.9960\n",
      "[epoch 210]  average training loss: 95.5293 average testing loss: 98.4199 Classification Accuracy: 0.9970\n",
      "[epoch 211]  average training loss: 95.5184 average testing loss: 98.4901 Classification Accuracy: 0.9980\n",
      "[epoch 212]  average training loss: 95.4995 average testing loss: 98.6413 Classification Accuracy: 0.9980\n",
      "[epoch 213]  average training loss: 95.5236 average testing loss: 98.6004 Classification Accuracy: 0.9990\n",
      "[epoch 214]  average training loss: 95.5241 average testing loss: 98.5348 Classification Accuracy: 0.9950\n",
      "[epoch 215]  average training loss: 95.4642 average testing loss: 98.2776 Classification Accuracy: 0.9950\n",
      "[epoch 216]  average training loss: 95.4891 average testing loss: 98.4547 Classification Accuracy: 0.9970\n",
      "[epoch 217]  average training loss: 95.4647 average testing loss: 98.3312 Classification Accuracy: 0.9980\n",
      "[epoch 218]  average training loss: 95.4331 average testing loss: 98.5533 Classification Accuracy: 0.9950\n",
      "[epoch 219]  average training loss: 95.4237 average testing loss: 98.8534 Classification Accuracy: 0.9960\n",
      "[epoch 220]  average training loss: 95.4533 average testing loss: 98.5572 Classification Accuracy: 0.9940\n",
      "[epoch 221]  average training loss: 95.4422 average testing loss: 98.4967 Classification Accuracy: 0.9980\n",
      "[epoch 222]  average training loss: 95.4486 average testing loss: 98.6348 Classification Accuracy: 0.9970\n",
      "[epoch 223]  average training loss: 95.4768 average testing loss: 98.6638 Classification Accuracy: 0.9950\n",
      "[epoch 224]  average training loss: 95.4112 average testing loss: 98.8273 Classification Accuracy: 0.9970\n",
      "[epoch 225]  average training loss: 95.4105 average testing loss: 98.4126 Classification Accuracy: 0.9980\n",
      "[epoch 226]  average training loss: 95.4284 average testing loss: 98.2888 Classification Accuracy: 0.9950\n",
      "[epoch 227]  average training loss: 95.4280 average testing loss: 98.4163 Classification Accuracy: 0.9990\n",
      "[epoch 228]  average training loss: 95.4455 average testing loss: 98.4575 Classification Accuracy: 0.9990\n",
      "[epoch 229]  average training loss: 95.3878 average testing loss: 98.4416 Classification Accuracy: 0.9980\n",
      "[epoch 230]  average training loss: 95.3957 average testing loss: 98.5172 Classification Accuracy: 0.9980\n",
      "[epoch 231]  average training loss: 95.3547 average testing loss: 98.4206 Classification Accuracy: 0.9990\n",
      "[epoch 232]  average training loss: 95.3963 average testing loss: 98.4322 Classification Accuracy: 0.9990\n",
      "[epoch 233]  average training loss: 95.3477 average testing loss: 98.5799 Classification Accuracy: 0.9980\n",
      "[epoch 234]  average training loss: 95.3904 average testing loss: 98.4504 Classification Accuracy: 0.9980\n",
      "[epoch 235]  average training loss: 95.3822 average testing loss: 98.6823 Classification Accuracy: 0.9930\n",
      "[epoch 236]  average training loss: 95.3847 average testing loss: 98.5313 Classification Accuracy: 0.9970\n",
      "[epoch 237]  average training loss: 95.3036 average testing loss: 98.7522 Classification Accuracy: 0.9970\n",
      "[epoch 238]  average training loss: 95.3562 average testing loss: 98.6402 Classification Accuracy: 0.9940\n",
      "[epoch 239]  average training loss: 95.3641 average testing loss: 98.7832 Classification Accuracy: 0.9970\n",
      "[epoch 240]  average training loss: 95.3251 average testing loss: 98.5584 Classification Accuracy: 0.9950\n",
      "[epoch 241]  average training loss: 95.3444 average testing loss: 98.5195 Classification Accuracy: 0.9960\n",
      "[epoch 242]  average training loss: 95.2777 average testing loss: 98.2703 Classification Accuracy: 0.9990\n",
      "[epoch 243]  average training loss: 95.3317 average testing loss: 98.5266 Classification Accuracy: 0.9970\n",
      "[epoch 244]  average training loss: 95.2989 average testing loss: 98.6273 Classification Accuracy: 0.9950\n",
      "[epoch 245]  average training loss: 95.3117 average testing loss: 98.6671 Classification Accuracy: 0.9980\n",
      "[epoch 246]  average training loss: 95.3332 average testing loss: 98.4766 Classification Accuracy: 0.9970\n",
      "[epoch 247]  average training loss: 95.2336 average testing loss: 98.7589 Classification Accuracy: 0.9970\n",
      "[epoch 248]  average training loss: 95.2918 average testing loss: 98.5202 Classification Accuracy: 0.9960\n",
      "[epoch 249]  average training loss: 95.3000 average testing loss: 98.4901 Classification Accuracy: 0.9980\n",
      "[epoch 250]  average training loss: 95.2760 average testing loss: 98.4777 Classification Accuracy: 0.9950\n",
      "[epoch 251]  average training loss: 95.2518 average testing loss: 98.4815 Classification Accuracy: 0.9970\n",
      "[epoch 252]  average training loss: 95.3022 average testing loss: 98.4074 Classification Accuracy: 0.9970\n",
      "[epoch 253]  average training loss: 95.2541 average testing loss: 98.7186 Classification Accuracy: 0.9990\n",
      "[epoch 254]  average training loss: 95.2456 average testing loss: 98.4621 Classification Accuracy: 0.9980\n",
      "[epoch 255]  average training loss: 95.2475 average testing loss: 98.6272 Classification Accuracy: 0.9990\n",
      "[epoch 256]  average training loss: 95.2661 average testing loss: 98.5217 Classification Accuracy: 0.9980\n",
      "[epoch 257]  average training loss: 95.2231 average testing loss: 98.5618 Classification Accuracy: 0.9970\n",
      "[epoch 258]  average training loss: 95.2314 average testing loss: 98.5487 Classification Accuracy: 0.9960\n",
      "[epoch 259]  average training loss: 95.2306 average testing loss: 98.4681 Classification Accuracy: 0.9960\n",
      "[epoch 260]  average training loss: 95.2197 average testing loss: 98.5514 Classification Accuracy: 0.9990\n",
      "[epoch 261]  average training loss: 95.2601 average testing loss: 98.6311 Classification Accuracy: 0.9940\n",
      "[epoch 262]  average training loss: 95.1279 average testing loss: 98.6301 Classification Accuracy: 0.9980\n",
      "[epoch 263]  average training loss: 95.2099 average testing loss: 98.6693 Classification Accuracy: 0.9970\n",
      "[epoch 264]  average training loss: 95.2327 average testing loss: 98.4799 Classification Accuracy: 0.9970\n",
      "[epoch 265]  average training loss: 95.2049 average testing loss: 98.4772 Classification Accuracy: 0.9960\n",
      "[epoch 266]  average training loss: 95.1965 average testing loss: 98.5252 Classification Accuracy: 0.9970\n",
      "[epoch 267]  average training loss: 95.1758 average testing loss: 98.3156 Classification Accuracy: 0.9970\n",
      "[epoch 268]  average training loss: 95.1916 average testing loss: 98.4660 Classification Accuracy: 0.9970\n",
      "[epoch 269]  average training loss: 95.1969 average testing loss: 99.1740 Classification Accuracy: 0.9940\n",
      "[epoch 270]  average training loss: 95.1686 average testing loss: 98.4938 Classification Accuracy: 1.0000\n",
      "[epoch 271]  average training loss: 95.1803 average testing loss: 98.4717 Classification Accuracy: 0.9950\n",
      "[epoch 272]  average training loss: 95.1550 average testing loss: 98.3912 Classification Accuracy: 0.9970\n",
      "[epoch 273]  average training loss: 95.1852 average testing loss: 98.7440 Classification Accuracy: 0.9980\n",
      "[epoch 274]  average training loss: 95.1561 average testing loss: 98.3514 Classification Accuracy: 0.9970\n",
      "[epoch 275]  average training loss: 95.1265 average testing loss: 98.4122 Classification Accuracy: 0.9980\n",
      "[epoch 276]  average training loss: 95.1109 average testing loss: 98.5982 Classification Accuracy: 0.9950\n",
      "[epoch 277]  average training loss: 95.1402 average testing loss: 98.5992 Classification Accuracy: 0.9950\n",
      "[epoch 278]  average training loss: 95.1514 average testing loss: 98.7016 Classification Accuracy: 0.9970\n",
      "[epoch 279]  average training loss: 95.1549 average testing loss: 98.8879 Classification Accuracy: 0.9980\n",
      "[epoch 280]  average training loss: 95.1585 average testing loss: 98.4587 Classification Accuracy: 0.9940\n",
      "[epoch 281]  average training loss: 95.1330 average testing loss: 98.5642 Classification Accuracy: 0.9970\n",
      "[epoch 282]  average training loss: 95.1181 average testing loss: 98.4606 Classification Accuracy: 0.9970\n",
      "[epoch 283]  average training loss: 95.1113 average testing loss: 98.7274 Classification Accuracy: 0.9970\n",
      "[epoch 284]  average training loss: 95.1401 average testing loss: 98.6158 Classification Accuracy: 0.9970\n",
      "[epoch 285]  average training loss: 95.0876 average testing loss: 98.6187 Classification Accuracy: 0.9970\n",
      "[epoch 286]  average training loss: 95.1119 average testing loss: 98.5896 Classification Accuracy: 0.9960\n",
      "[epoch 287]  average training loss: 95.0660 average testing loss: 98.7536 Classification Accuracy: 0.9970\n",
      "[epoch 288]  average training loss: 95.1332 average testing loss: 98.4993 Classification Accuracy: 0.9960\n",
      "[epoch 289]  average training loss: 95.0960 average testing loss: 98.6430 Classification Accuracy: 0.9980\n",
      "[epoch 290]  average training loss: 95.1339 average testing loss: 98.6299 Classification Accuracy: 0.9960\n",
      "[epoch 291]  average training loss: 95.1044 average testing loss: 99.0494 Classification Accuracy: 0.9970\n",
      "[epoch 292]  average training loss: 95.0763 average testing loss: 98.6701 Classification Accuracy: 0.9970\n",
      "[epoch 293]  average training loss: 95.0727 average testing loss: 98.6524 Classification Accuracy: 0.9960\n",
      "[epoch 294]  average training loss: 95.0742 average testing loss: 98.4385 Classification Accuracy: 0.9970\n",
      "[epoch 295]  average training loss: 95.0484 average testing loss: 98.6499 Classification Accuracy: 0.9950\n",
      "[epoch 296]  average training loss: 95.0630 average testing loss: 98.3930 Classification Accuracy: 0.9960\n",
      "[epoch 297]  average training loss: 95.0339 average testing loss: 98.6590 Classification Accuracy: 0.9970\n",
      "[epoch 298]  average training loss: 95.0456 average testing loss: 98.5253 Classification Accuracy: 0.9990\n",
      "[epoch 299]  average training loss: 95.0838 average testing loss: 98.7123 Classification Accuracy: 0.9960\n",
      "[epoch 300]  average training loss: 95.0646 average testing loss: 98.7883 Classification Accuracy: 0.9980\n",
      "[epoch 301]  average training loss: 95.0138 average testing loss: 98.5960 Classification Accuracy: 0.9940\n",
      "[epoch 302]  average training loss: 95.0470 average testing loss: 98.4811 Classification Accuracy: 0.9990\n",
      "[epoch 303]  average training loss: 95.0414 average testing loss: 98.6709 Classification Accuracy: 0.9980\n",
      "[epoch 304]  average training loss: 95.0371 average testing loss: 98.7362 Classification Accuracy: 0.9970\n",
      "[epoch 305]  average training loss: 95.0322 average testing loss: 98.5586 Classification Accuracy: 0.9980\n",
      "[epoch 306]  average training loss: 94.9977 average testing loss: 98.5864 Classification Accuracy: 0.9970\n",
      "[epoch 307]  average training loss: 95.0353 average testing loss: 98.7598 Classification Accuracy: 0.9940\n",
      "[epoch 308]  average training loss: 95.0311 average testing loss: 98.5140 Classification Accuracy: 0.9960\n",
      "[epoch 309]  average training loss: 95.0053 average testing loss: 98.6971 Classification Accuracy: 0.9960\n",
      "[epoch 310]  average training loss: 95.0504 average testing loss: 98.6940 Classification Accuracy: 0.9990\n",
      "[epoch 311]  average training loss: 94.9983 average testing loss: 98.4745 Classification Accuracy: 0.9980\n",
      "[epoch 312]  average training loss: 95.0029 average testing loss: 98.4744 Classification Accuracy: 0.9980\n",
      "[epoch 313]  average training loss: 95.0108 average testing loss: 98.4349 Classification Accuracy: 0.9960\n",
      "[epoch 314]  average training loss: 95.0313 average testing loss: 98.4025 Classification Accuracy: 0.9990\n",
      "[epoch 315]  average training loss: 94.9619 average testing loss: 98.7262 Classification Accuracy: 0.9960\n",
      "[epoch 316]  average training loss: 94.9696 average testing loss: 98.5113 Classification Accuracy: 0.9970\n",
      "[epoch 317]  average training loss: 94.9301 average testing loss: 98.4570 Classification Accuracy: 0.9970\n",
      "[epoch 318]  average training loss: 94.9495 average testing loss: 98.6623 Classification Accuracy: 1.0000\n",
      "[epoch 319]  average training loss: 95.0260 average testing loss: 99.0867 Classification Accuracy: 0.9970\n",
      "[epoch 320]  average training loss: 94.9556 average testing loss: 98.6589 Classification Accuracy: 0.9960\n",
      "[epoch 321]  average training loss: 94.9298 average testing loss: 98.7735 Classification Accuracy: 0.9970\n",
      "[epoch 322]  average training loss: 94.9787 average testing loss: 98.5286 Classification Accuracy: 0.9940\n",
      "[epoch 323]  average training loss: 94.9849 average testing loss: 98.6639 Classification Accuracy: 1.0000\n",
      "[epoch 324]  average training loss: 94.9662 average testing loss: 98.8685 Classification Accuracy: 0.9970\n",
      "[epoch 325]  average training loss: 94.9713 average testing loss: 98.7094 Classification Accuracy: 0.9990\n",
      "[epoch 326]  average training loss: 94.9121 average testing loss: 98.8239 Classification Accuracy: 0.9980\n",
      "[epoch 327]  average training loss: 94.9486 average testing loss: 98.5671 Classification Accuracy: 0.9980\n",
      "[epoch 328]  average training loss: 94.9384 average testing loss: 98.5084 Classification Accuracy: 0.9970\n",
      "[epoch 329]  average training loss: 94.9311 average testing loss: 98.6028 Classification Accuracy: 0.9950\n",
      "[epoch 330]  average training loss: 94.9480 average testing loss: 98.7538 Classification Accuracy: 0.9990\n",
      "[epoch 331]  average training loss: 94.9158 average testing loss: 98.6375 Classification Accuracy: 0.9960\n",
      "[epoch 332]  average training loss: 94.9676 average testing loss: 98.7223 Classification Accuracy: 0.9990\n",
      "[epoch 333]  average training loss: 94.9027 average testing loss: 98.5658 Classification Accuracy: 0.9970\n",
      "[epoch 334]  average training loss: 94.8806 average testing loss: 98.9004 Classification Accuracy: 0.9990\n",
      "[epoch 335]  average training loss: 94.9234 average testing loss: 98.7116 Classification Accuracy: 0.9950\n",
      "[epoch 336]  average training loss: 94.9134 average testing loss: 98.7926 Classification Accuracy: 0.9950\n",
      "[epoch 337]  average training loss: 94.9284 average testing loss: 98.6817 Classification Accuracy: 0.9960\n",
      "[epoch 338]  average training loss: 94.8965 average testing loss: 98.8924 Classification Accuracy: 0.9960\n",
      "[epoch 339]  average training loss: 94.9393 average testing loss: 98.6688 Classification Accuracy: 0.9970\n",
      "[epoch 340]  average training loss: 94.9164 average testing loss: 98.6170 Classification Accuracy: 0.9950\n",
      "[epoch 341]  average training loss: 94.8932 average testing loss: 98.6647 Classification Accuracy: 0.9950\n",
      "[epoch 342]  average training loss: 94.9244 average testing loss: 98.5425 Classification Accuracy: 0.9960\n",
      "[epoch 343]  average training loss: 94.8897 average testing loss: 98.7361 Classification Accuracy: 0.9940\n",
      "[epoch 344]  average training loss: 94.8993 average testing loss: 98.7020 Classification Accuracy: 0.9980\n",
      "[epoch 345]  average training loss: 94.8814 average testing loss: 98.7378 Classification Accuracy: 0.9980\n",
      "[epoch 346]  average training loss: 94.8676 average testing loss: 98.6268 Classification Accuracy: 0.9960\n",
      "[epoch 347]  average training loss: 94.8760 average testing loss: 98.7220 Classification Accuracy: 0.9940\n",
      "[epoch 348]  average training loss: 94.9021 average testing loss: 98.7705 Classification Accuracy: 0.9970\n",
      "[epoch 349]  average training loss: 94.8646 average testing loss: 98.7284 Classification Accuracy: 0.9970\n",
      "[epoch 350]  average training loss: 94.8776 average testing loss: 98.4957 Classification Accuracy: 0.9970\n",
      "[epoch 351]  average training loss: 94.9047 average testing loss: 98.5739 Classification Accuracy: 0.9940\n",
      "[epoch 352]  average training loss: 94.8550 average testing loss: 98.9607 Classification Accuracy: 0.9960\n",
      "[epoch 353]  average training loss: 94.8827 average testing loss: 98.8619 Classification Accuracy: 0.9970\n",
      "[epoch 354]  average training loss: 94.8557 average testing loss: 98.7374 Classification Accuracy: 0.9910\n",
      "[epoch 355]  average training loss: 94.8180 average testing loss: 98.6675 Classification Accuracy: 0.9980\n",
      "[epoch 356]  average training loss: 94.8691 average testing loss: 98.7100 Classification Accuracy: 0.9970\n",
      "[epoch 357]  average training loss: 94.8361 average testing loss: 98.9772 Classification Accuracy: 0.9970\n",
      "[epoch 358]  average training loss: 94.8656 average testing loss: 98.8239 Classification Accuracy: 0.9960\n",
      "[epoch 359]  average training loss: 94.8945 average testing loss: 99.0783 Classification Accuracy: 1.0000\n",
      "[epoch 360]  average training loss: 94.8363 average testing loss: 98.4764 Classification Accuracy: 1.0000\n",
      "[epoch 361]  average training loss: 94.8050 average testing loss: 98.9459 Classification Accuracy: 0.9940\n",
      "[epoch 362]  average training loss: 94.7926 average testing loss: 98.8166 Classification Accuracy: 0.9970\n",
      "[epoch 363]  average training loss: 94.8044 average testing loss: 99.0381 Classification Accuracy: 0.9960\n",
      "[epoch 364]  average training loss: 94.8366 average testing loss: 98.7807 Classification Accuracy: 0.9990\n",
      "[epoch 365]  average training loss: 94.8159 average testing loss: 98.7897 Classification Accuracy: 0.9960\n",
      "[epoch 366]  average training loss: 94.7899 average testing loss: 98.8919 Classification Accuracy: 0.9980\n",
      "[epoch 367]  average training loss: 94.8222 average testing loss: 98.5466 Classification Accuracy: 0.9970\n",
      "[epoch 368]  average training loss: 94.8075 average testing loss: 98.8543 Classification Accuracy: 0.9970\n",
      "[epoch 369]  average training loss: 94.8033 average testing loss: 98.8192 Classification Accuracy: 0.9980\n",
      "[epoch 370]  average training loss: 94.8211 average testing loss: 98.4502 Classification Accuracy: 0.9960\n",
      "[epoch 371]  average training loss: 94.8559 average testing loss: 98.7772 Classification Accuracy: 0.9960\n",
      "[epoch 372]  average training loss: 94.7758 average testing loss: 98.6844 Classification Accuracy: 0.9970\n",
      "[epoch 373]  average training loss: 94.7823 average testing loss: 98.9036 Classification Accuracy: 0.9970\n",
      "[epoch 374]  average training loss: 94.7901 average testing loss: 98.7283 Classification Accuracy: 0.9980\n",
      "[epoch 375]  average training loss: 94.8284 average testing loss: 98.6187 Classification Accuracy: 0.9980\n",
      "[epoch 376]  average training loss: 94.8027 average testing loss: 98.7942 Classification Accuracy: 0.9960\n",
      "[epoch 377]  average training loss: 94.7168 average testing loss: 98.8517 Classification Accuracy: 0.9960\n",
      "[epoch 378]  average training loss: 94.7833 average testing loss: 98.7615 Classification Accuracy: 0.9930\n",
      "[epoch 379]  average training loss: 94.7616 average testing loss: 98.7044 Classification Accuracy: 0.9940\n",
      "[epoch 380]  average training loss: 94.8236 average testing loss: 98.6177 Classification Accuracy: 0.9960\n",
      "[epoch 381]  average training loss: 94.7520 average testing loss: 99.0719 Classification Accuracy: 0.9950\n",
      "[epoch 382]  average training loss: 94.7687 average testing loss: 99.0297 Classification Accuracy: 0.9980\n",
      "[epoch 383]  average training loss: 94.8007 average testing loss: 98.6165 Classification Accuracy: 0.9950\n",
      "[epoch 384]  average training loss: 94.7912 average testing loss: 98.6441 Classification Accuracy: 0.9960\n",
      "[epoch 385]  average training loss: 94.7374 average testing loss: 99.2042 Classification Accuracy: 0.9970\n",
      "[epoch 386]  average training loss: 94.7772 average testing loss: 98.6182 Classification Accuracy: 0.9980\n",
      "[epoch 387]  average training loss: 94.7273 average testing loss: 98.9008 Classification Accuracy: 0.9970\n",
      "[epoch 388]  average training loss: 94.7588 average testing loss: 99.1914 Classification Accuracy: 0.9990\n",
      "[epoch 389]  average training loss: 94.7360 average testing loss: 99.4354 Classification Accuracy: 0.9980\n",
      "[epoch 390]  average training loss: 94.7500 average testing loss: 98.7732 Classification Accuracy: 0.9950\n",
      "[epoch 391]  average training loss: 94.7495 average testing loss: 99.1964 Classification Accuracy: 0.9960\n",
      "[epoch 392]  average training loss: 94.7322 average testing loss: 98.8619 Classification Accuracy: 0.9920\n",
      "[epoch 393]  average training loss: 94.7180 average testing loss: 98.8544 Classification Accuracy: 0.9970\n",
      "[epoch 394]  average training loss: 94.7256 average testing loss: 98.7899 Classification Accuracy: 0.9910\n",
      "[epoch 395]  average training loss: 94.7224 average testing loss: 98.7270 Classification Accuracy: 0.9980\n",
      "[epoch 396]  average training loss: 94.7162 average testing loss: 98.8979 Classification Accuracy: 0.9990\n",
      "[epoch 397]  average training loss: 94.7067 average testing loss: 99.0201 Classification Accuracy: 0.9980\n",
      "[epoch 398]  average training loss: 94.6975 average testing loss: 98.8242 Classification Accuracy: 0.9980\n",
      "[epoch 399]  average training loss: 94.7424 average testing loss: 98.5939 Classification Accuracy: 0.9950\n",
      "[epoch 400]  average training loss: 94.7045 average testing loss: 98.6784 Classification Accuracy: 0.9930\n",
      "[epoch 401]  average training loss: 94.7272 average testing loss: 98.9359 Classification Accuracy: 0.9950\n",
      "[epoch 402]  average training loss: 94.6976 average testing loss: 99.2601 Classification Accuracy: 0.9960\n",
      "[epoch 403]  average training loss: 94.7426 average testing loss: 99.0319 Classification Accuracy: 0.9930\n",
      "[epoch 404]  average training loss: 94.7082 average testing loss: 98.7248 Classification Accuracy: 0.9970\n",
      "[epoch 405]  average training loss: 94.6983 average testing loss: 99.0478 Classification Accuracy: 0.9940\n",
      "[epoch 406]  average training loss: 94.7289 average testing loss: 98.7662 Classification Accuracy: 0.9970\n",
      "[epoch 407]  average training loss: 94.6667 average testing loss: 98.7228 Classification Accuracy: 0.9960\n",
      "[epoch 408]  average training loss: 94.6811 average testing loss: 98.7511 Classification Accuracy: 0.9960\n",
      "[epoch 409]  average training loss: 94.6758 average testing loss: 98.9588 Classification Accuracy: 0.9980\n",
      "[epoch 410]  average training loss: 94.6893 average testing loss: 98.9010 Classification Accuracy: 0.9970\n",
      "[epoch 411]  average training loss: 94.7270 average testing loss: 99.1784 Classification Accuracy: 0.9960\n",
      "[epoch 412]  average training loss: 94.6797 average testing loss: 99.3248 Classification Accuracy: 0.9980\n",
      "[epoch 413]  average training loss: 94.6916 average testing loss: 99.5716 Classification Accuracy: 0.9940\n",
      "[epoch 414]  average training loss: 94.7179 average testing loss: 98.7861 Classification Accuracy: 0.9920\n",
      "[epoch 415]  average training loss: 94.6412 average testing loss: 98.8454 Classification Accuracy: 0.9970\n",
      "[epoch 416]  average training loss: 94.6675 average testing loss: 98.9589 Classification Accuracy: 0.9960\n",
      "[epoch 417]  average training loss: 94.6878 average testing loss: 98.8546 Classification Accuracy: 0.9930\n",
      "[epoch 418]  average training loss: 94.6516 average testing loss: 98.9308 Classification Accuracy: 0.9990\n",
      "[epoch 419]  average training loss: 94.6854 average testing loss: 98.9317 Classification Accuracy: 0.9970\n",
      "[epoch 420]  average training loss: 94.7439 average testing loss: 98.7633 Classification Accuracy: 0.9970\n",
      "[epoch 421]  average training loss: 94.6794 average testing loss: 99.1114 Classification Accuracy: 0.9980\n",
      "[epoch 422]  average training loss: 94.6537 average testing loss: 99.0339 Classification Accuracy: 0.9990\n",
      "[epoch 423]  average training loss: 94.6447 average testing loss: 98.9110 Classification Accuracy: 0.9980\n",
      "[epoch 424]  average training loss: 94.6713 average testing loss: 99.0414 Classification Accuracy: 0.9960\n",
      "[epoch 425]  average training loss: 94.6541 average testing loss: 99.1615 Classification Accuracy: 0.9940\n",
      "[epoch 426]  average training loss: 94.6616 average testing loss: 98.8179 Classification Accuracy: 0.9960\n",
      "[epoch 427]  average training loss: 94.6902 average testing loss: 99.0657 Classification Accuracy: 0.9980\n",
      "[epoch 428]  average training loss: 94.6665 average testing loss: 99.1318 Classification Accuracy: 0.9960\n",
      "[epoch 429]  average training loss: 94.6468 average testing loss: 98.8473 Classification Accuracy: 0.9980\n",
      "[epoch 430]  average training loss: 94.6293 average testing loss: 98.7290 Classification Accuracy: 0.9970\n",
      "[epoch 431]  average training loss: 94.6138 average testing loss: 98.9372 Classification Accuracy: 0.9950\n",
      "[epoch 432]  average training loss: 94.6457 average testing loss: 98.7136 Classification Accuracy: 0.9950\n",
      "[epoch 433]  average training loss: 94.6327 average testing loss: 98.9840 Classification Accuracy: 0.9990\n",
      "[epoch 434]  average training loss: 94.5987 average testing loss: 99.0996 Classification Accuracy: 0.9960\n",
      "[epoch 435]  average training loss: 94.6516 average testing loss: 99.1766 Classification Accuracy: 0.9950\n",
      "[epoch 436]  average training loss: 94.6190 average testing loss: 98.9076 Classification Accuracy: 0.9970\n",
      "[epoch 437]  average training loss: 94.6462 average testing loss: 98.7948 Classification Accuracy: 0.9960\n",
      "[epoch 438]  average training loss: 94.6065 average testing loss: 99.2408 Classification Accuracy: 0.9960\n",
      "[epoch 439]  average training loss: 94.6220 average testing loss: 99.0063 Classification Accuracy: 0.9950\n",
      "[epoch 440]  average training loss: 94.6080 average testing loss: 98.8257 Classification Accuracy: 0.9980\n",
      "[epoch 441]  average training loss: 94.6067 average testing loss: 99.0573 Classification Accuracy: 0.9960\n",
      "[epoch 442]  average training loss: 94.5928 average testing loss: 98.7759 Classification Accuracy: 0.9990\n",
      "[epoch 443]  average training loss: 94.6198 average testing loss: 99.3354 Classification Accuracy: 0.9990\n",
      "[epoch 444]  average training loss: 94.6083 average testing loss: 98.7904 Classification Accuracy: 0.9980\n",
      "[epoch 445]  average training loss: 94.6065 average testing loss: 98.8399 Classification Accuracy: 0.9940\n",
      "[epoch 446]  average training loss: 94.6064 average testing loss: 99.0666 Classification Accuracy: 0.9910\n",
      "[epoch 447]  average training loss: 94.6223 average testing loss: 99.0977 Classification Accuracy: 0.9960\n",
      "[epoch 448]  average training loss: 94.5561 average testing loss: 99.0602 Classification Accuracy: 0.9950\n",
      "[epoch 449]  average training loss: 94.5319 average testing loss: 98.9243 Classification Accuracy: 0.9960\n",
      "[epoch 450]  average training loss: 94.5979 average testing loss: 99.0183 Classification Accuracy: 0.9980\n",
      "[epoch 451]  average training loss: 94.5886 average testing loss: 99.0458 Classification Accuracy: 0.9920\n",
      "[epoch 452]  average training loss: 94.6181 average testing loss: 98.6353 Classification Accuracy: 0.9980\n",
      "[epoch 453]  average training loss: 94.5842 average testing loss: 98.8604 Classification Accuracy: 0.9990\n",
      "[epoch 454]  average training loss: 94.5551 average testing loss: 99.2465 Classification Accuracy: 0.9940\n",
      "[epoch 455]  average training loss: 94.5716 average testing loss: 99.0913 Classification Accuracy: 0.9950\n",
      "[epoch 456]  average training loss: 94.5853 average testing loss: 99.0406 Classification Accuracy: 0.9980\n",
      "[epoch 457]  average training loss: 94.5664 average testing loss: 98.9592 Classification Accuracy: 0.9990\n",
      "[epoch 458]  average training loss: 94.5503 average testing loss: 98.9249 Classification Accuracy: 0.9930\n",
      "[epoch 459]  average training loss: 94.6011 average testing loss: 98.9949 Classification Accuracy: 0.9950\n",
      "[epoch 460]  average training loss: 94.5671 average testing loss: 99.0866 Classification Accuracy: 0.9970\n",
      "[epoch 461]  average training loss: 94.5795 average testing loss: 99.1603 Classification Accuracy: 0.9970\n",
      "[epoch 462]  average training loss: 94.5777 average testing loss: 98.8915 Classification Accuracy: 0.9940\n",
      "[epoch 463]  average training loss: 94.4993 average testing loss: 98.8355 Classification Accuracy: 0.9960\n",
      "[epoch 464]  average training loss: 94.5842 average testing loss: 98.9533 Classification Accuracy: 0.9960\n",
      "[epoch 465]  average training loss: 94.5925 average testing loss: 98.9118 Classification Accuracy: 0.9960\n",
      "[epoch 466]  average training loss: 94.6321 average testing loss: 98.9217 Classification Accuracy: 0.9950\n",
      "[epoch 467]  average training loss: 94.5185 average testing loss: 99.0476 Classification Accuracy: 0.9950\n",
      "[epoch 468]  average training loss: 94.5714 average testing loss: 99.0333 Classification Accuracy: 0.9980\n",
      "[epoch 469]  average training loss: 94.5373 average testing loss: 98.8434 Classification Accuracy: 0.9980\n",
      "[epoch 470]  average training loss: 94.5175 average testing loss: 98.9347 Classification Accuracy: 0.9960\n",
      "[epoch 471]  average training loss: 94.5526 average testing loss: 98.7800 Classification Accuracy: 0.9980\n",
      "[epoch 472]  average training loss: 94.5567 average testing loss: 98.8770 Classification Accuracy: 0.9930\n",
      "[epoch 473]  average training loss: 94.5147 average testing loss: 99.1023 Classification Accuracy: 0.9980\n",
      "[epoch 474]  average training loss: 94.5702 average testing loss: 99.0340 Classification Accuracy: 0.9980\n",
      "[epoch 475]  average training loss: 94.5488 average testing loss: 99.0630 Classification Accuracy: 0.9980\n",
      "[epoch 476]  average training loss: 94.5675 average testing loss: 98.8858 Classification Accuracy: 0.9950\n",
      "[epoch 477]  average training loss: 94.5497 average testing loss: 99.0117 Classification Accuracy: 0.9980\n",
      "[epoch 478]  average training loss: 94.5075 average testing loss: 99.2368 Classification Accuracy: 0.9980\n",
      "[epoch 479]  average training loss: 94.4989 average testing loss: 98.9118 Classification Accuracy: 0.9940\n",
      "[epoch 480]  average training loss: 94.5262 average testing loss: 98.8131 Classification Accuracy: 0.9950\n",
      "[epoch 481]  average training loss: 94.4966 average testing loss: 99.0008 Classification Accuracy: 0.9960\n",
      "[epoch 482]  average training loss: 94.5333 average testing loss: 99.0125 Classification Accuracy: 1.0000\n",
      "[epoch 483]  average training loss: 94.5291 average testing loss: 99.2644 Classification Accuracy: 0.9960\n",
      "[epoch 484]  average training loss: 94.6725 average testing loss: 99.3730 Classification Accuracy: 0.9970\n",
      "[epoch 485]  average training loss: 94.5303 average testing loss: 99.1164 Classification Accuracy: 0.9910\n",
      "[epoch 486]  average training loss: 94.5162 average testing loss: 99.0890 Classification Accuracy: 0.9960\n",
      "[epoch 487]  average training loss: 94.5115 average testing loss: 99.0181 Classification Accuracy: 0.9990\n",
      "[epoch 488]  average training loss: 94.5534 average testing loss: 98.9579 Classification Accuracy: 0.9960\n",
      "[epoch 489]  average training loss: 94.4855 average testing loss: 99.0088 Classification Accuracy: 0.9930\n",
      "[epoch 490]  average training loss: 94.4755 average testing loss: 99.1685 Classification Accuracy: 0.9940\n",
      "[epoch 491]  average training loss: 94.4963 average testing loss: 98.9561 Classification Accuracy: 0.9980\n",
      "[epoch 492]  average training loss: 94.4702 average testing loss: 99.1955 Classification Accuracy: 0.9980\n",
      "[epoch 493]  average training loss: 94.5088 average testing loss: 99.1375 Classification Accuracy: 0.9970\n",
      "[epoch 494]  average training loss: 94.5461 average testing loss: 99.0771 Classification Accuracy: 0.9930\n",
      "[epoch 495]  average training loss: 94.4894 average testing loss: 98.9334 Classification Accuracy: 0.9950\n",
      "[epoch 496]  average training loss: 94.5131 average testing loss: 99.2159 Classification Accuracy: 0.9970\n",
      "[epoch 497]  average training loss: 94.5169 average testing loss: 99.4426 Classification Accuracy: 0.9970\n",
      "[epoch 498]  average training loss: 94.4681 average testing loss: 99.0022 Classification Accuracy: 0.9960\n",
      "[epoch 499]  average training loss: 94.5256 average testing loss: 99.0994 Classification Accuracy: 0.9960\n"
     ]
    }
   ],
   "source": [
    "# Run options\n",
    "LEARNING_RATE = 1.0e-3\n",
    "USE_CUDA = False\n",
    "\n",
    "# Run only for a single iteration for testing\n",
    "NUM_EPOCHS = 1 if smoke_test else 500\n",
    "TEST_FREQUENCY = 5\n",
    "\n",
    "\n",
    "\n",
    "train_loader, test_loader = setup_data_loaders(batch_size=256, use_cuda=USE_CUDA)\n",
    "\n",
    "# clear param store\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# setup the VAE\n",
    "vae = VAE(use_cuda=USE_CUDA)\n",
    "\n",
    "# setup the optimizer\n",
    "adam_params = {\"lr\": 0.003}\n",
    "optimizer = Adam(adam_params)\n",
    "\n",
    "svi = SVI(vae.model, vae.guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "\n",
    "train_elbo = []\n",
    "test_elbo = []\n",
    "# training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_epoch_loss_train = train(svi, train_loader, use_cuda=USE_CUDA)\n",
    "    \n",
    "    train_elbo.append(-total_epoch_loss_train)\n",
    "    \n",
    "    # --------------------------- VAE Testing Sequence ------------------------------\n",
    "\n",
    "    test_loss = 0.\n",
    "        # compute the loss over the entire test set\n",
    "    for x_test,y_test in test_loader:\n",
    "            # compute ELBO estimate and accumulate loss\n",
    "            labels_y_test = torch.tensor(np.zeros((y_test.shape[0],2)))\n",
    "            y_test_2=torch.Tensor.cpu(y_test.reshape(1,y_test.size()[0])[0]).numpy().astype(int)  \n",
    "            labels_y_test=np.eye(10)[y_test_2]\n",
    "            labels_y_test = torch.from_numpy(labels_y_test)\n",
    "        \n",
    "            test_loss += svi.evaluate_loss(x_test.reshape(-1,784),labels_y_test.float()) \n",
    "            \n",
    "            \n",
    "    normalizer_test = len(test_loader.dataset)\n",
    "    total_epoch_loss_test = test_loss / normalizer_test\n",
    "    \n",
    "    \n",
    "    #-------------------------- CNN Validation Procedure -----------------------------\n",
    "    \n",
    "    all_generated_images_data = np.zeros((1,28,28))\n",
    "    all_generated_images_labels = np.zeros((1))\n",
    "    temp_labels = np.zeros((100))\n",
    "    for j in range (0,10):\n",
    "        temp_labels[:] = j\n",
    "        generated_data=generate_images(j)\n",
    "        all_generated_images_data=np.concatenate((all_generated_images_data,generated_data[:,:]),axis=0)\n",
    "        all_generated_images_labels=np.concatenate((all_generated_images_labels,temp_labels),axis=0)\n",
    "    \n",
    "    evaluate_x=torch.from_numpy(all_generated_images_data).float().to('cpu')\n",
    "    \n",
    "    output = model(evaluate_x.cuda())\n",
    "    \n",
    "    pred = output.data.max(1)[1]\n",
    "    \n",
    "    y_pred=pred[1:1001].cpu().detach().numpy().astype(np.float)\n",
    "    \n",
    "    y_class=all_generated_images_labels[1:1001]\n",
    "    \n",
    "    classification_accuracy_score = accuracy_score(y_class, y_pred)\n",
    "    \n",
    "    print(\"[epoch %03d]  average training loss: %.4f average testing loss: %.4f Classification Accuracy: %.4f\" % (epoch, total_epoch_loss_train,total_epoch_loss_test,classification_accuracy_score))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
