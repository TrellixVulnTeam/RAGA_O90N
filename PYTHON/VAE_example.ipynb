{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is adapted from: https://github.com/pytorch/examples/tree/master/vae which uses ReLUs and the adam optimizer, instead of sigmoids and adagrad as in the original paper. This gives faster convergence.\n",
    "\n",
    "The notebook is set up to receive images that are 28x28 pixels in size and encode them into 2 latent variables. It uses the MNIST dataset by default. If you don't have it already the notebook will automatically download it for you.\n",
    "\n",
    "You need to create a directory called \"results\" in the same directory that you run the notebook from. \n",
    "\n",
    "The outputs are: \n",
    "\n",
    "* **reconstruction_NN.png** - the top row is a random selection of 8 samples from the test dataset and the lower row is the reconstruction using the model based on their auto-encoded latent variables.\n",
    "\n",
    "* **sample_NN.png** - this is a selection of 64 randomly created samples based on a random selection of latent variable values from the model.\n",
    "\n",
    "* **manifold.png** - this is the equivalent of Fig4(b) in Kingma & Welling.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.distributions import Normal\n",
    "from torchsummary import summary\n",
    "from VAE import VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First set the parameters of the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "epochs=10\n",
    "no_cuda=True\n",
    "seed=1\n",
    "log_interval=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decide whether you can use a GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a random seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10a34d710>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "                            datasets.MNIST('../data', train=True, download=True,\n",
    "                            transform=transforms.ToTensor()),\n",
    "                            batch_size=batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "                            datasets.MNIST('../data', train=False, \n",
    "                            transform=transforms.ToTensor()),\n",
    "                            batch_size=batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the different layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 400]         314,000\n",
      "            Linear-2                    [-1, 2]             802\n",
      "            Linear-3                    [-1, 2]             802\n",
      "            Linear-4                  [-1, 400]           1,200\n",
      "            Linear-5                  [-1, 784]         314,384\n",
      "================================================================\n",
      "Total params: 631,188\n",
      "Trainable params: 631,188\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 2.41\n",
      "Estimated Total Size (MB): 2.42\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model,(1,28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the optimizer and the learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the loss function from the Kingma & Welling paper (Appendix B):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    \n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1,784), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch.view(batch_size, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(),\n",
    "                         'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over epochs, updating the training each time and testing the updated model against the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 552.574097\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 190.118073\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 182.392120\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 177.693863\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 173.205551\n",
      "====> Epoch: 1 Average loss: 191.6132\n",
      "====> Test set loss: 172.1348\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 174.518036\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 163.312073\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 169.693390\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 165.049606\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 164.099014\n",
      "====> Epoch: 2 Average loss: 168.1606\n",
      "====> Test set loss: 164.8987\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 166.214996\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 165.291962\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 162.137985\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 162.485703\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 161.068802\n",
      "====> Epoch: 3 Average loss: 163.3929\n",
      "====> Test set loss: 161.6210\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 168.281952\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 158.934402\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 157.654922\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 158.411606\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 162.921478\n",
      "====> Epoch: 4 Average loss: 160.7454\n",
      "====> Test set loss: 159.5269\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 162.687408\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 161.346756\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 163.431412\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 150.559570\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 153.009842\n",
      "====> Epoch: 5 Average loss: 158.9231\n",
      "====> Test set loss: 158.1781\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 162.297470\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 153.966858\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 160.255569\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 161.055710\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 159.556686\n",
      "====> Epoch: 6 Average loss: 157.7054\n",
      "====> Test set loss: 157.0081\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 160.624451\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 156.321228\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 158.579498\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 153.326385\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 163.269196\n",
      "====> Epoch: 7 Average loss: 156.6586\n",
      "====> Test set loss: 156.1518\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 154.692474\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 157.904526\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 150.966904\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 151.901184\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 157.630173\n",
      "====> Epoch: 8 Average loss: 155.7604\n",
      "====> Test set loss: 155.4686\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 159.507584\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 159.401215\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 151.258957\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 159.082535\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 150.480255\n",
      "====> Epoch: 9 Average loss: 155.0102\n",
      "====> Test set loss: 154.8375\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 153.255264\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 157.450500\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 159.667236\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 146.405243\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 149.031296\n",
      "====> Epoch: 10 Average loss: 154.3146\n",
      "====> Test set loss: 154.1702\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "        train(epoch)\n",
    "        test(epoch)\n",
    "        with torch.no_grad():\n",
    "            sample = torch.randn(64, 2).to(device)\n",
    "            sample = model.decode(sample).cpu()\n",
    "            save_image(sample.view(64, 1, 28, 28),\n",
    "                       'results/sample_' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recreate Figure 4(b) from Kingma & Welling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nside = 20\n",
    "x, y = torch.meshgrid([torch.linspace(0.,1.,nside), torch.linspace(0.,1.,nside)])\n",
    "m = Normal(torch.tensor([0.0]), torch.tensor([1.0]))\n",
    "x = m.icdf(x).view(-1,nside**2)\n",
    "y = m.icdf(y).view(-1,nside**2)\n",
    "z = torch.cat((x,y),0).t()\n",
    "with torch.no_grad():\n",
    "    sample = z.to(device)\n",
    "    sample = model.decode(sample).cpu()\n",
    "    save_image(sample.view(nside**2, 1, 28, 28),\n",
    "                   'results/manifold.png', nrow=nside)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
