{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Variational Auto Encoder - Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.datasets as dset\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO, TraceEnum_ELBO, config_enumerate\n",
    "from pyro.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.1'"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyro.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.enable_validation(True)\n",
    "pyro.distributions.enable_validation(False)\n",
    "pyro.set_rng_seed(0)\n",
    "# Enable smoke test - run the notebook cells on CI.\n",
    "smoke_test = 'CI' in os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loading and batching MNIST dataset\n",
    "def setup_data_loaders(batch_size=128, use_cuda=True):\n",
    "    root = './data'\n",
    "    download = True\n",
    "    trans = transforms.ToTensor()\n",
    "    train_set = dset.MNIST(root=root, train=True, transform=trans,\n",
    "                           download=download)\n",
    "    test_set = dset.MNIST(root=root, train=False, transform=trans)\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': use_cuda}\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_set,\n",
    "        batch_size=batch_size, shuffle=False, **kwargs)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderZ(nn.Module):\n",
    "    #def __init__(self, z_dim, hidden_dim):\n",
    "    def __init__(self, x_dim, y_dim, h_dim, z_dim):\n",
    "        super(EncoderZ, self).__init__()\n",
    "        # setup the three linear transformations used\n",
    "        #have to here define the fully connected layers - 784 to 400\n",
    "        #400 to 2\n",
    "        \n",
    "        # setup the non-linearities\n",
    "        self.fc1 = nn.Linear(x_dim+y_dim, h_dim)  \n",
    "        self.fc21 = nn.Linear(h_dim, z_dim) \n",
    "        self.fc22 = nn.Linear(h_dim, z_dim)\n",
    "        \n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, x_y_2):\n",
    "        [x,y]=x_y_2\n",
    "        x = x.reshape(-1, 784) #@David Change this to reshape if something fucks up\n",
    "        y = y.reshape(-1, 10) #@David Change this to reshape if something fucks up\n",
    "        x_y_1 = torch.cat((x,y), dim=1) #I think that this should concatenate the two inputs if this does work then test it independenlty\n",
    "        x_y_1 = x_y_1.view(x_y_1.size(0), -1)\n",
    "        \n",
    "        hidden = self.softplus(self.fc1(x_y_1))\n",
    "        \n",
    "        z_loc = self.fc21(hidden)\n",
    "        z_scale = torch.exp(self.fc22(hidden)) # mu, log_var\n",
    "        return z_loc, z_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, x_dim, y_dim, h_dim, z_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        # setup the two linear transformations used\n",
    "        self.fc1 = nn.Linear(z_dim+y_dim, h_dim)\n",
    "        self.fc21 = nn.Linear(h_dim, x_dim)\n",
    "        # setup the non-linearities\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,z_y_2):\n",
    "        # define the forward computation on the latent z\n",
    "        # first compute the hidden units\n",
    "        \n",
    "        [z,y]=z_y_2\n",
    "        \n",
    "        z = z.reshape(-1, 2) #@David Change this to reshape if something fucks up\n",
    "        y = y.reshape(-1, 10)\n",
    "        z_y_1 = torch.cat((z,y), dim=1)\n",
    "        z_y_1 = z_y_1.view(z_y_1.size(0), -1)\n",
    "        hidden = self.softplus(self.fc1(z_y_1))\n",
    "        # return the parameter for the output Bernoulli\n",
    "        # each is of size batch_size x 784\n",
    "        loc_img = self.sigmoid(self.fc21(hidden))\n",
    "        return loc_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    # by default our latent space is 50-dimensional\n",
    "    # and we use 400 hidden units\n",
    "    def __init__(self, x_dim = 784, y_dim = 10, h_dim = 500, z_dim = 2,use_cuda=True):\n",
    "        super(VAE, self).__init__()\n",
    "    \n",
    "        # create the encoder and decoder networks\n",
    "        # a split in the final layer's size is used for multiple outputs\n",
    "        # and potentially applying separate activation functions on them\n",
    "        # e.g. in this network the final output is of size [z_dim,z_dim]\n",
    "        # to produce loc and scale, and apply different activations [None,Exp] on them\n",
    "              \n",
    "        self.encoder_z = EncoderZ(x_dim, y_dim, h_dim, z_dim)\n",
    "        \n",
    "        self.decoder = Decoder(x_dim, y_dim, h_dim, z_dim)\n",
    "\n",
    "        if use_cuda:\n",
    "            # calling cuda() here will put all the parameters of\n",
    "            # the encoder and decoder networks into gpu memory\n",
    "            self.cuda()\n",
    "            \n",
    "            \n",
    "        self.use_cuda = use_cuda\n",
    "        self.z_dim = z_dim\n",
    "        self.output_size = y_dim\n",
    "        \n",
    "        \n",
    "    # define the model p(x|z)p(z)\n",
    "    def model(self, xs, ys):\n",
    "        # register this pytorch module and all of its sub-modules with pyro\n",
    "        pyro.module(\"ss_vae\", self)\n",
    "        batch_size = xs.size(0)\n",
    "\n",
    "        # inform Pyro that the variables in the batch of xs, ys are conditionally independent\n",
    "        with pyro.plate(\"data\"):\n",
    "\n",
    "            # sample the handwriting style from the constant prior distribution\n",
    "            prior_loc = xs.new_zeros([batch_size, self.z_dim])\n",
    "            prior_scale = xs.new_ones([batch_size, self.z_dim])\n",
    "            zs = pyro.sample(\"z\", dist.Normal(prior_loc, prior_scale).to_event(1))\n",
    "\n",
    "            # if the label y (which digit to write) is supervised, sample from the\n",
    "            # constant prior, otherwise, observe the value (i.e. score it against the constant prior)\n",
    "            alpha_prior = xs.new_ones([batch_size, self.output_size]) / (1.0 * self.output_size)\n",
    "            ys = pyro.sample(\"y\", dist.OneHotCategorical(alpha_prior), obs=ys)\n",
    "            \n",
    "            # finally, score the image (x) using the handwriting style (z) and\n",
    "            # the class label y (which digit to write) against the\n",
    "            # parametrized distribution p(x|y,z) = bernoulli(decoder(y,z))\n",
    "            # where `decoder` is a neural network\n",
    "            loc = self.decoder.forward([zs, ys])\n",
    "            pyro.sample(\"x\", dist.Bernoulli(loc).to_event(1), obs=xs)\n",
    "            \n",
    "    def guide(self, xs, ys):\n",
    "        with pyro.plate(\"data\"):\n",
    "           # if the class label (the digit) is not supervised, sample\n",
    "           # (and score) the digit with the variational distribution\n",
    "           # q(y|x) = categorical(alpha(x))\n",
    "           \n",
    "            #-------------------REMOVED THIS PART FOR THE CLASSIFIER ASSUME ALL DATA ARE LABELLED---------\n",
    "\n",
    "           # sample (and score) the latent handwriting-style with the variational\n",
    "           # distribution q(z|x,y) = normal(loc(x,y),scale(x,y))\n",
    "           loc, scale = self.encoder_z.forward([xs, ys])\n",
    "           pyro.sample(\"z\", dist.Normal(loc, scale).to_event(1))\n",
    "\n",
    "    # define a helper function for reconstructing images\n",
    "    def reconstruct_img(self, xs, ys):\n",
    "        # encode image x\n",
    "        z_loc, z_scale = self.encoder_z.forward([xs,ys])\n",
    "        # sample in latent space\n",
    "        z = dist.Normal(z_loc, z_scale).sample()\n",
    "        # decode the image (note we don't sample in image space)\n",
    "        loc_img = self.decoder.forward([zs,ys])\n",
    "        \n",
    "        return loc_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(svi, train_loader, use_cuda=True):\n",
    "    # initialize loss accumulator\n",
    "    epoch_loss = 0.\n",
    "    # do a training epoch over each mini-batch x returned\n",
    "    # by the data loader\n",
    "    for x, y in train_loader:\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "        # do ELBO gradient and accumulate loss\n",
    "        labels_y = torch.tensor(np.zeros((y.shape[0],10)))\n",
    "        for j in range (0,y.shape[0]):\n",
    "            labels_y[j,int(y[j].numpy())] = 1\n",
    "        epoch_loss += svi.step(x.reshape(-1,784),labels_y.float())\n",
    "\n",
    "    # return epoch loss\n",
    "    normalizer_train = len(train_loader.dataset)\n",
    "    total_epoch_loss_train = epoch_loss / normalizer_train\n",
    "    return total_epoch_loss_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run options\n",
    "LEARNING_RATE = 1.0e-3\n",
    "USE_CUDA = False\n",
    "\n",
    "# Run only for a single iteration for testing\n",
    "NUM_EPOCHS = 1 if smoke_test else 50\n",
    "TEST_FREQUENCY = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 000]  average training loss: 216.9260\n",
      "[epoch 001]  average training loss: 179.4542\n",
      "[epoch 002]  average training loss: 171.1694\n",
      "[epoch 003]  average training loss: 165.3247\n",
      "[epoch 004]  average training loss: 161.0235\n",
      "[epoch 005]  average training loss: 158.2552\n",
      "[epoch 006]  average training loss: 156.2076\n",
      "[epoch 007]  average training loss: 154.4524\n",
      "[epoch 008]  average training loss: 152.7730\n",
      "[epoch 009]  average training loss: 151.2435\n",
      "[epoch 010]  average training loss: 149.9963\n",
      "[epoch 011]  average training loss: 148.9709\n",
      "[epoch 012]  average training loss: 148.1016\n",
      "[epoch 013]  average training loss: 147.3774\n",
      "[epoch 014]  average training loss: 146.7372\n",
      "[epoch 015]  average training loss: 146.1686\n",
      "[epoch 016]  average training loss: 145.6654\n",
      "[epoch 017]  average training loss: 145.2398\n",
      "[epoch 018]  average training loss: 144.8295\n",
      "[epoch 019]  average training loss: 144.4458\n",
      "[epoch 020]  average training loss: 144.1104\n",
      "[epoch 021]  average training loss: 143.7935\n",
      "[epoch 022]  average training loss: 143.5292\n",
      "[epoch 023]  average training loss: 143.2726\n",
      "[epoch 024]  average training loss: 143.0337\n",
      "[epoch 025]  average training loss: 142.8110\n",
      "[epoch 026]  average training loss: 142.6219\n",
      "[epoch 027]  average training loss: 142.4242\n",
      "[epoch 028]  average training loss: 142.2509\n",
      "[epoch 029]  average training loss: 142.0754\n",
      "[epoch 030]  average training loss: 141.9210\n",
      "[epoch 031]  average training loss: 141.7773\n",
      "[epoch 032]  average training loss: 141.6488\n",
      "[epoch 033]  average training loss: 141.5120\n",
      "[epoch 034]  average training loss: 141.3749\n",
      "[epoch 035]  average training loss: 141.2448\n",
      "[epoch 036]  average training loss: 141.1336\n",
      "[epoch 037]  average training loss: 141.0024\n",
      "[epoch 038]  average training loss: 140.8954\n",
      "[epoch 039]  average training loss: 140.7813\n",
      "[epoch 040]  average training loss: 140.6685\n",
      "[epoch 041]  average training loss: 140.5561\n",
      "[epoch 042]  average training loss: 140.4593\n",
      "[epoch 043]  average training loss: 140.3443\n",
      "[epoch 044]  average training loss: 140.2357\n",
      "[epoch 045]  average training loss: 140.1423\n",
      "[epoch 046]  average training loss: 140.0199\n",
      "[epoch 047]  average training loss: 139.9174\n",
      "[epoch 048]  average training loss: 139.8358\n",
      "[epoch 049]  average training loss: 139.7196\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = setup_data_loaders(batch_size=256, use_cuda=USE_CUDA)\n",
    "\n",
    "# clear param store\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# setup the VAE\n",
    "vae = VAE(use_cuda=USE_CUDA)\n",
    "\n",
    "# setup the optimizer\n",
    "adam_params = {\"lr\": 0.0003}\n",
    "optimizer = Adam(adam_params)\n",
    "\n",
    "# setup the inference algorithm\n",
    "svi_aux = SVI(model_classify, guide_classify, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "svi = SVI(vae.model, vae.guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "\n",
    "train_elbo = []\n",
    "test_elbo = []\n",
    "# training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_epoch_loss_train = train(svi, train_loader, use_cuda=USE_CUDA)\n",
    "    \n",
    "    train_elbo.append(-total_epoch_loss_train)\n",
    "    print(\"[epoch %03d]  average training loss: %.4f\" % (epoch, total_epoch_loss_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_image_sampler(z0=0.0,z1=0.0, z2=0.0, z3=0.0, z4=0.0, z5=0.0, z6=0.0, z7=0.0, z8=0.0, z9=0.0):\n",
    "    z = torch.rand(1,10)\n",
    "    z[0,0]=z0\n",
    "    z[0,1]=z1\n",
    "    z[0,2]=z2\n",
    "    z[0,3]=z3\n",
    "    z[0,4]=z4\n",
    "    z[0,5]=z5   \n",
    "    z[0,6]=z6\n",
    "    z[0,7]=z7    \n",
    "    z[0,8]=z8\n",
    "    z[0,9]=z9\n",
    "    labels_y = torch.tensor(np.zeros((10)))\n",
    "    labels_y[0] = 1\n",
    "    single_sample_image = vae.decoder([z,labels_y.float()])\n",
    "    image_array_single =single_sample_image.reshape(28,28).cpu().detach().numpy()\n",
    "    temp_array=image_array_single\n",
    "  #  plt.figure(figsize = (10,10))\n",
    "  #  plt.imshow(image_array_single)\n",
    " #   plt.colorbar()\n",
    " #   plt.show() \n",
    "    return image_array_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rand_0 = np.random.uniform(-2,2)\n",
    "rand_1 = np.random.uniform(-2,2)\n",
    "rand_2 = np.random.uniform(-0.5,0.5)\n",
    "rand_3 = np.random.uniform(-0.2,0.4)\n",
    "rand_4 = np.random.uniform(-1,1)\n",
    "rand_5 = np.random.uniform(-0.1,0.1)\n",
    "rand_6 = np.random.uniform(-0.1,0.2)\n",
    "rand_7 = np.random.uniform(-2.3,2.5)\n",
    "rand_8 = np.random.uniform(-0.2,0.2)\n",
    "rand_9 = np.random.uniform(-0.2,0.2)\n",
    "image = single_image_sampler(rand_0,rand_1,rand_2,rand_3,rand_4,rand_5,rand_6,rand_7,rand_8,rand_9)\n",
    "img_max = image.max() \n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "number=9\n",
    "with torch.no_grad():\n",
    "    count = 0\n",
    "    z = torch.randn(100, 2)\n",
    "    labels_y = torch.tensor(np.zeros((100,10)))\n",
    "    for i in range (0,10):\n",
    "        for j in range (0,10):\n",
    "            z[count,0]=np.random.uniform(-2,2)\n",
    "            z[count,1]=np.random.uniform(-2,2)\n",
    "            labels_y[count,8] = 1\n",
    "            labels_y[count,0] = 1\n",
    "            count = count +1 \n",
    "        \n",
    "    sample = vae.decoder([z,labels_y.float()])\n",
    "    \n",
    "    save_image(sample.view(100, 1, 28, 28), str(number)+'_sample_z_space_' +str(epoch)+'.png',nrow=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
