{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Auto Encoder Notebook\n",
    "Generative Algorithm that has been trained using FIRST Radio Sources. Makes use of the FRDEEP dataset of FIRST Radio Sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports the images from the FRDEEP, we make use of the FIRST radio sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FRDEEP import FRDEEPF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize([0.5],[0.5])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloads the training dataset. Two datasets are downloaded 1. Training 2. Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = FRDEEPF(root='./FIRST_data', train=True, download=True, transform=transform)  \n",
    "trainloader = torch.utils.data.DataLoader(trainset, shuffle=True, num_workers=2, batch_size=len(trainset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "testset = FRDEEPF(root='./FIRST_data', train=False, download=True, transform=transform) \n",
    "testloader = torch.utils.data.DataLoader(testset, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('FRI', 'FRII')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training dataset is loaded in a numpy array for data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_train= next(iter(trainloader))[0].numpy() # Training Datasets is loaded in numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_label= next(iter(trainloader))[1].numpy() # Training Datasets labels is loaded in seperate numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plots a sample radio image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f33159645f8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(array_train[102,0,:,:]) # Prints sample image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates array augmented_data to host the Augmented Data. \n",
    "- Each image is rotated at steps of 10 deg making up 19800 images for training\n",
    "- The images are cut at dimension 100 pixels by 100 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data=np.zeros((19800,1,100,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "count=0\n",
    "for j in range(0,550):\n",
    "    image_object=Image.fromarray(array_train[j,0,:,:])\n",
    "    for i in range(0,36):\n",
    "        rotated=image_object.rotate(i*10)\n",
    "        imgarr = np.array(rotated)\n",
    "        temp_img_array=imgarr[25:125,25:125]\n",
    "        augmented_data[count,0,:,:]=temp_img_array\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Normalizes the imagae arrayfrom 0.0 to 1.0. Minimum parameter used for normalization is -1.0 while maximum parameter used for normalization is +1.0. If image array is not normalized between 0 and +1.0 the loss function will result runtime error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data=(augmented_data-np.min(augmented_data))/(np.max(augmented_data)-np.min(augmented_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=augmented_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Shuffles images in the data array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.take(X,np.random.permutation(X.shape[0]),axis=0,out=X);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Auto Encoder Neural Network \n",
    "The Variational Auto Encoder consists of two neural network: i. An encoder and ii.a decoder.  \n",
    "- The Encoder\\\n",
    "The enncoder consists of 5 layers and an input layer. The 100 by 100 pizels are reshaped to 10000 input features that are then reduced to 4096 in the first connected layer, then to 2048 features, then to 1024 features, to 512 features, 256 features and finally to the final 2 dimensional latent space z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim1, h_dim2, h_dim3, h_dim4, h_dim5, z_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # encoder part\n",
    "        self.fc1 = nn.Linear(x_dim, h_dim1) #x_dim=10000 to h_dim1=4096 \n",
    "        self.fc2 = nn.Linear(h_dim1, h_dim2) #h_dim1=4096 to h_dim2=2048\n",
    "        self.fc3 = nn.Linear(h_dim2, h_dim3) #h_dim2=2048 to h_dim3=1024\n",
    "        self.fc4 = nn.Linear(h_dim3, h_dim4) #h_dim3=1024 to h_dim4=512\n",
    "        self.fc5 = nn.Linear(h_dim4, h_dim5) #h_dim4=512 to h_dim5=256\n",
    "        self.fc61 = nn.Linear(h_dim5, z_dim) #h_dim5=256 to z_dim=2\n",
    "        self.fc62 = nn.Linear(h_dim5, z_dim) #h_dim5=256 to z_dim=2\n",
    "        \n",
    "        # decoder part\n",
    "        self.fc7 = nn.Linear(z_dim, h_dim5) #z_dim=2 to h_dim5=256\n",
    "        self.fc8 = nn.Linear(h_dim5, h_dim4) #h_dim5=256 to h_dim4=512\n",
    "        self.fc9 = nn.Linear(h_dim4, h_dim3) #h_dim4=512 to h_dim3=1024\n",
    "        self.fc10 = nn.Linear(h_dim3, h_dim2) #h_dim3=1024 to h_dim2=2048\n",
    "        self.fc11 = nn.Linear(h_dim2, h_dim1) #h_dim2=2048 to h_dim1=4096\n",
    "        self.fc12 = nn.Linear(h_dim1, x_dim)  #h_dim1=4096 to x_dim=10000\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        h = F.relu(self.fc3(h))\n",
    "        h = F.relu(self.fc4(h))\n",
    "        h = F.relu(self.fc5(h))\n",
    "        return self.fc61(h), self.fc62(h) # mu, log_var\n",
    "    \n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu) # return z sample\n",
    "        \n",
    "    def decoder(self, z):\n",
    "        h = F.relu(self.fc7(z))\n",
    "        h = F.relu(self.fc8(h))\n",
    "        h = F.relu(self.fc9(h))\n",
    "        h = F.relu(self.fc10(h))\n",
    "        h = F.relu(self.fc11(h))\n",
    "        return F.sigmoid(self.fc12(h)) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encoder(x.view(-1, 10000))\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z), mu, log_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "vae = VAE(x_dim=10000, h_dim1= 4096, h_dim2=2048, h_dim3=1024, h_dim4=512, h_dim5=256, z_dim=2)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    vae.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(vae.parameters())\n",
    "# return reconstruction error + KL divergence losses\n",
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 10000), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + KLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    print(\"----------------------------------------------\")\n",
    "    print(\"Training Epoch:\"+str(epoch))\n",
    "    print(\"----------------------------------------------\")\n",
    "    data=torch.zeros(100, 1, 100, 100).cpu\n",
    "    for j in range(0,198):\n",
    "        data = torch.from_numpy(X[j*100:(j+1)*100,:,:])\n",
    "        data = data.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        recon_batch, mu, log_var = vae(data.float())\n",
    "        loss = loss_function(recon_batch.float(), data.float(), mu, log_var)\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "          \n",
    "        print(\"% Data Trained:\"+str(round(((j/197.0)*100.0),1))+\"%, Loss:\"+str(loss.item() / len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "Training Epoch:1\n",
      "----------------------------------------------\n",
      "% Data Trained:0.0%, Loss:6931.388125\n",
      "% Data Trained:0.5%, Loss:6238.729375\n",
      "% Data Trained:1.0%, Loss:3183.219375\n",
      "% Data Trained:1.5%, Loss:537.2292578125\n",
      "% Data Trained:2.0%, Loss:627.231015625\n",
      "% Data Trained:2.5%, Loss:722.180234375\n",
      "% Data Trained:3.0%, Loss:687.847734375\n",
      "% Data Trained:3.6%, Loss:869.659296875\n",
      "% Data Trained:4.1%, Loss:1129.873125\n",
      "% Data Trained:4.6%, Loss:1456.84875\n",
      "% Data Trained:5.1%, Loss:1024.0525\n",
      "% Data Trained:5.6%, Loss:1163.1565625\n",
      "% Data Trained:6.1%, Loss:1185.375859375\n",
      "% Data Trained:6.6%, Loss:1039.26546875\n",
      "% Data Trained:7.1%, Loss:1116.875546875\n",
      "% Data Trained:7.6%, Loss:1015.373828125\n",
      "% Data Trained:8.1%, Loss:1011.842265625\n",
      "% Data Trained:8.6%, Loss:986.94\n",
      "% Data Trained:9.1%, Loss:868.1834375\n",
      "% Data Trained:9.6%, Loss:789.5978125\n",
      "% Data Trained:10.2%, Loss:1034.817421875\n",
      "% Data Trained:10.7%, Loss:612.541953125\n",
      "% Data Trained:11.2%, Loss:438.134453125\n",
      "% Data Trained:11.7%, Loss:368.26140625\n",
      "% Data Trained:12.2%, Loss:259.8273828125\n",
      "% Data Trained:12.7%, Loss:240.3670703125\n",
      "% Data Trained:13.2%, Loss:365.86734375\n",
      "% Data Trained:13.7%, Loss:229.05625\n",
      "% Data Trained:14.2%, Loss:233.33376953125\n",
      "% Data Trained:14.7%, Loss:198.5740625\n",
      "% Data Trained:15.2%, Loss:244.064375\n",
      "% Data Trained:15.7%, Loss:231.198359375\n",
      "% Data Trained:16.2%, Loss:182.3229296875\n",
      "% Data Trained:16.8%, Loss:211.87759765625\n",
      "% Data Trained:17.3%, Loss:167.89595703125\n",
      "% Data Trained:17.8%, Loss:192.89826171875\n",
      "% Data Trained:18.3%, Loss:177.58904296875\n",
      "% Data Trained:18.8%, Loss:181.2632421875\n",
      "% Data Trained:19.3%, Loss:202.1553515625\n",
      "% Data Trained:19.8%, Loss:235.28787109375\n",
      "% Data Trained:20.3%, Loss:182.1133984375\n",
      "% Data Trained:20.8%, Loss:188.828515625\n",
      "% Data Trained:21.3%, Loss:183.91748046875\n",
      "% Data Trained:21.8%, Loss:226.55712890625\n",
      "% Data Trained:22.3%, Loss:195.63056640625\n",
      "% Data Trained:22.8%, Loss:175.5577734375\n",
      "% Data Trained:23.4%, Loss:149.813525390625\n",
      "% Data Trained:23.9%, Loss:189.9575390625\n",
      "% Data Trained:24.4%, Loss:169.40994140625\n",
      "% Data Trained:24.9%, Loss:186.55396484375\n",
      "% Data Trained:25.4%, Loss:169.9948046875\n",
      "% Data Trained:25.9%, Loss:227.10283203125\n",
      "% Data Trained:26.4%, Loss:186.56109375\n",
      "% Data Trained:26.9%, Loss:152.028828125\n",
      "% Data Trained:27.4%, Loss:171.403671875\n",
      "% Data Trained:27.9%, Loss:179.45357421875\n",
      "% Data Trained:28.4%, Loss:170.45666015625\n",
      "% Data Trained:28.9%, Loss:158.1550390625\n",
      "% Data Trained:29.4%, Loss:166.97849609375\n",
      "% Data Trained:29.9%, Loss:170.391796875\n",
      "% Data Trained:30.5%, Loss:152.824267578125\n",
      "% Data Trained:31.0%, Loss:152.67833984375\n",
      "% Data Trained:31.5%, Loss:141.72380859375\n",
      "% Data Trained:32.0%, Loss:174.8639453125\n",
      "% Data Trained:32.5%, Loss:153.16423828125\n",
      "% Data Trained:33.0%, Loss:140.489345703125\n",
      "% Data Trained:33.5%, Loss:167.6097265625\n",
      "% Data Trained:34.0%, Loss:177.04646484375\n",
      "% Data Trained:34.5%, Loss:159.9680859375\n",
      "% Data Trained:35.0%, Loss:143.642197265625\n",
      "% Data Trained:35.5%, Loss:174.3073828125\n",
      "% Data Trained:36.0%, Loss:158.784404296875\n",
      "% Data Trained:36.5%, Loss:142.840576171875\n",
      "% Data Trained:37.1%, Loss:165.7575390625\n",
      "% Data Trained:37.6%, Loss:170.03572265625\n",
      "% Data Trained:38.1%, Loss:173.34162109375\n",
      "% Data Trained:38.6%, Loss:161.439091796875\n",
      "% Data Trained:39.1%, Loss:148.686982421875\n",
      "% Data Trained:39.6%, Loss:200.95759765625\n",
      "% Data Trained:40.1%, Loss:172.330390625\n",
      "% Data Trained:40.6%, Loss:189.55408203125\n",
      "% Data Trained:41.1%, Loss:171.00154296875\n",
      "% Data Trained:41.6%, Loss:165.70765625\n",
      "% Data Trained:42.1%, Loss:158.993818359375\n",
      "% Data Trained:42.6%, Loss:198.876875\n",
      "% Data Trained:43.1%, Loss:145.96646484375\n",
      "% Data Trained:43.7%, Loss:174.29373046875\n",
      "% Data Trained:44.2%, Loss:153.467412109375\n",
      "% Data Trained:44.7%, Loss:177.93984375\n",
      "% Data Trained:45.2%, Loss:164.401875\n",
      "% Data Trained:45.7%, Loss:204.609765625\n",
      "% Data Trained:46.2%, Loss:170.854609375\n",
      "% Data Trained:46.7%, Loss:183.352265625\n",
      "% Data Trained:47.2%, Loss:149.409921875\n",
      "% Data Trained:47.7%, Loss:182.73404296875\n",
      "% Data Trained:48.2%, Loss:129.543369140625\n",
      "% Data Trained:48.7%, Loss:160.18732421875\n",
      "% Data Trained:49.2%, Loss:148.2522265625\n",
      "% Data Trained:49.7%, Loss:179.7051953125\n",
      "% Data Trained:50.3%, Loss:187.96181640625\n",
      "% Data Trained:50.8%, Loss:137.877109375\n",
      "% Data Trained:51.3%, Loss:148.14095703125\n",
      "% Data Trained:51.8%, Loss:204.59166015625\n",
      "% Data Trained:52.3%, Loss:171.08296875\n",
      "% Data Trained:52.8%, Loss:148.58927734375\n",
      "% Data Trained:53.3%, Loss:158.312333984375\n",
      "% Data Trained:53.8%, Loss:163.8017578125\n",
      "% Data Trained:54.3%, Loss:187.0008984375\n",
      "% Data Trained:54.8%, Loss:164.13392578125\n",
      "% Data Trained:55.3%, Loss:176.7570703125\n",
      "% Data Trained:55.8%, Loss:172.11146484375\n",
      "% Data Trained:56.3%, Loss:162.089345703125\n",
      "% Data Trained:56.9%, Loss:179.98572265625\n",
      "% Data Trained:57.4%, Loss:177.1314453125\n",
      "% Data Trained:57.9%, Loss:157.65966796875\n",
      "% Data Trained:58.4%, Loss:161.22943359375\n",
      "% Data Trained:58.9%, Loss:179.9630859375\n",
      "% Data Trained:59.4%, Loss:156.01248046875\n",
      "% Data Trained:59.9%, Loss:136.06677734375\n",
      "% Data Trained:60.4%, Loss:162.45330078125\n",
      "% Data Trained:60.9%, Loss:157.337392578125\n",
      "% Data Trained:61.4%, Loss:188.13470703125\n",
      "% Data Trained:61.9%, Loss:196.955078125\n",
      "% Data Trained:62.4%, Loss:159.22955078125\n",
      "% Data Trained:62.9%, Loss:138.684130859375\n",
      "% Data Trained:63.5%, Loss:186.00416015625\n",
      "% Data Trained:64.0%, Loss:175.0745703125\n",
      "% Data Trained:64.5%, Loss:162.379521484375\n",
      "% Data Trained:65.0%, Loss:164.435703125\n",
      "% Data Trained:65.5%, Loss:147.8696484375\n",
      "% Data Trained:66.0%, Loss:176.17953125\n",
      "% Data Trained:66.5%, Loss:182.925\n",
      "% Data Trained:67.0%, Loss:158.116826171875\n",
      "% Data Trained:67.5%, Loss:167.3414453125\n",
      "% Data Trained:68.0%, Loss:177.98251953125\n",
      "% Data Trained:68.5%, Loss:176.0584375\n",
      "% Data Trained:69.0%, Loss:145.20548828125\n",
      "% Data Trained:69.5%, Loss:166.60658203125\n",
      "% Data Trained:70.1%, Loss:163.675732421875\n",
      "% Data Trained:70.6%, Loss:210.84865234375\n",
      "% Data Trained:71.1%, Loss:163.2670703125\n",
      "% Data Trained:71.6%, Loss:175.4188671875\n",
      "% Data Trained:72.1%, Loss:156.9055078125\n",
      "% Data Trained:72.6%, Loss:193.16513671875\n",
      "% Data Trained:73.1%, Loss:171.51708984375\n",
      "% Data Trained:73.6%, Loss:182.5271875\n",
      "% Data Trained:74.1%, Loss:204.68455078125\n",
      "% Data Trained:74.6%, Loss:162.758759765625\n",
      "% Data Trained:75.1%, Loss:161.478896484375\n",
      "% Data Trained:75.6%, Loss:150.76048828125\n",
      "% Data Trained:76.1%, Loss:150.915546875\n",
      "% Data Trained:76.6%, Loss:183.3803515625\n",
      "% Data Trained:77.2%, Loss:159.326865234375\n",
      "% Data Trained:77.7%, Loss:174.52595703125\n",
      "% Data Trained:78.2%, Loss:160.414091796875\n",
      "% Data Trained:78.7%, Loss:173.77013671875\n",
      "% Data Trained:79.2%, Loss:166.4156640625\n",
      "% Data Trained:79.7%, Loss:166.62673828125\n",
      "% Data Trained:80.2%, Loss:176.1397265625\n",
      "% Data Trained:80.7%, Loss:154.562587890625\n",
      "% Data Trained:81.2%, Loss:172.3396875\n",
      "% Data Trained:81.7%, Loss:180.615\n",
      "% Data Trained:82.2%, Loss:167.62767578125\n",
      "% Data Trained:82.7%, Loss:164.0203515625\n",
      "% Data Trained:83.2%, Loss:160.737978515625\n",
      "% Data Trained:83.8%, Loss:178.37763671875\n",
      "% Data Trained:84.3%, Loss:164.63064453125\n",
      "% Data Trained:84.8%, Loss:196.056328125\n",
      "% Data Trained:85.3%, Loss:155.24341796875\n",
      "% Data Trained:85.8%, Loss:176.44697265625\n",
      "% Data Trained:86.3%, Loss:169.9894140625\n",
      "% Data Trained:86.8%, Loss:172.2997265625\n",
      "% Data Trained:87.3%, Loss:162.14076171875\n",
      "% Data Trained:87.8%, Loss:177.9766796875\n",
      "% Data Trained:88.3%, Loss:170.5712890625\n",
      "% Data Trained:88.8%, Loss:158.0267578125\n",
      "% Data Trained:89.3%, Loss:200.17076171875\n",
      "% Data Trained:89.8%, Loss:161.56779296875\n",
      "% Data Trained:90.4%, Loss:147.369677734375\n",
      "% Data Trained:90.9%, Loss:168.06087890625\n",
      "% Data Trained:91.4%, Loss:165.074140625\n",
      "% Data Trained:91.9%, Loss:152.293330078125\n",
      "% Data Trained:92.4%, Loss:170.3396875\n",
      "% Data Trained:92.9%, Loss:164.56890625\n",
      "% Data Trained:93.4%, Loss:148.28583984375\n",
      "% Data Trained:93.9%, Loss:199.10052734375\n",
      "% Data Trained:94.4%, Loss:156.71953125\n",
      "% Data Trained:94.9%, Loss:157.2522265625\n",
      "% Data Trained:95.4%, Loss:153.89345703125\n",
      "% Data Trained:95.9%, Loss:155.78265625\n",
      "% Data Trained:96.4%, Loss:182.5893359375\n",
      "% Data Trained:97.0%, Loss:141.853203125\n",
      "% Data Trained:97.5%, Loss:160.70236328125\n",
      "% Data Trained:98.0%, Loss:168.0980078125\n",
      "% Data Trained:98.5%, Loss:174.0102734375\n",
      "% Data Trained:99.0%, Loss:174.77287109375\n",
      "% Data Trained:99.5%, Loss:161.16087890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Data Trained:100.0%, Loss:181.2208984375\n",
      "----------------------------------------------\n",
      "Training Epoch:2\n",
      "----------------------------------------------\n",
      "% Data Trained:0.0%, Loss:154.548388671875\n",
      "% Data Trained:0.5%, Loss:161.687353515625\n",
      "% Data Trained:1.0%, Loss:155.370888671875\n",
      "% Data Trained:1.5%, Loss:165.02583984375\n",
      "% Data Trained:2.0%, Loss:147.7911328125\n",
      "% Data Trained:2.5%, Loss:155.797216796875\n",
      "% Data Trained:3.0%, Loss:147.0498046875\n",
      "% Data Trained:3.6%, Loss:157.40025390625\n",
      "% Data Trained:4.1%, Loss:154.1704296875\n",
      "% Data Trained:4.6%, Loss:190.60283203125\n",
      "% Data Trained:5.1%, Loss:147.23408203125\n",
      "% Data Trained:5.6%, Loss:175.5264453125\n",
      "% Data Trained:6.1%, Loss:174.0746484375\n",
      "% Data Trained:6.6%, Loss:167.15669921875\n",
      "% Data Trained:7.1%, Loss:173.597265625\n",
      "% Data Trained:7.6%, Loss:158.231552734375\n",
      "% Data Trained:8.1%, Loss:159.34408203125\n",
      "% Data Trained:8.6%, Loss:160.022138671875\n",
      "% Data Trained:9.1%, Loss:157.140576171875\n",
      "% Data Trained:9.6%, Loss:155.812841796875\n",
      "% Data Trained:10.2%, Loss:205.49455078125\n",
      "% Data Trained:10.7%, Loss:165.82884765625\n",
      "% Data Trained:11.2%, Loss:149.684375\n",
      "% Data Trained:11.7%, Loss:166.26439453125\n",
      "% Data Trained:12.2%, Loss:159.66130859375\n",
      "% Data Trained:12.7%, Loss:156.425634765625\n",
      "% Data Trained:13.2%, Loss:185.938671875\n",
      "% Data Trained:13.7%, Loss:164.61470703125\n",
      "% Data Trained:14.2%, Loss:170.70732421875\n",
      "% Data Trained:14.7%, Loss:153.063623046875\n",
      "% Data Trained:15.2%, Loss:171.58576171875\n",
      "% Data Trained:15.7%, Loss:161.5891796875\n",
      "% Data Trained:16.2%, Loss:139.98046875\n",
      "% Data Trained:16.8%, Loss:163.067431640625\n",
      "% Data Trained:17.3%, Loss:143.3885546875\n",
      "% Data Trained:17.8%, Loss:161.5930859375\n",
      "% Data Trained:18.3%, Loss:155.3825\n",
      "% Data Trained:18.8%, Loss:155.021064453125\n",
      "% Data Trained:19.3%, Loss:168.62828125\n",
      "% Data Trained:19.8%, Loss:199.17716796875\n",
      "% Data Trained:20.3%, Loss:169.0737109375\n",
      "% Data Trained:20.8%, Loss:155.310244140625\n",
      "% Data Trained:21.3%, Loss:167.99130859375\n",
      "% Data Trained:21.8%, Loss:201.21736328125\n",
      "% Data Trained:22.3%, Loss:179.939921875\n",
      "% Data Trained:22.8%, Loss:161.93203125\n",
      "% Data Trained:23.4%, Loss:142.256728515625\n",
      "% Data Trained:23.9%, Loss:183.2697265625\n",
      "% Data Trained:24.4%, Loss:157.437080078125\n",
      "% Data Trained:24.9%, Loss:176.22447265625\n",
      "% Data Trained:25.4%, Loss:161.197275390625\n",
      "% Data Trained:25.9%, Loss:205.35146484375\n",
      "% Data Trained:26.4%, Loss:178.534609375\n",
      "% Data Trained:26.9%, Loss:143.16453125\n",
      "% Data Trained:27.4%, Loss:163.5104296875\n",
      "% Data Trained:27.9%, Loss:171.572265625\n",
      "% Data Trained:28.4%, Loss:159.295283203125\n",
      "% Data Trained:28.9%, Loss:148.344501953125\n",
      "% Data Trained:29.4%, Loss:161.96328125\n",
      "% Data Trained:29.9%, Loss:160.126376953125\n",
      "% Data Trained:30.5%, Loss:145.363564453125\n",
      "% Data Trained:31.0%, Loss:145.376318359375\n",
      "% Data Trained:31.5%, Loss:135.614228515625\n",
      "% Data Trained:32.0%, Loss:161.83166015625\n",
      "% Data Trained:32.5%, Loss:147.18564453125\n",
      "% Data Trained:33.0%, Loss:134.05943359375\n",
      "% Data Trained:33.5%, Loss:163.506337890625\n",
      "% Data Trained:34.0%, Loss:170.98216796875\n",
      "% Data Trained:34.5%, Loss:152.271025390625\n",
      "% Data Trained:35.0%, Loss:136.964658203125\n",
      "% Data Trained:35.5%, Loss:166.68935546875\n",
      "% Data Trained:36.0%, Loss:153.18275390625\n",
      "% Data Trained:36.5%, Loss:138.463603515625\n",
      "% Data Trained:37.1%, Loss:159.36361328125\n",
      "% Data Trained:37.6%, Loss:163.2446484375\n",
      "% Data Trained:38.1%, Loss:165.971953125\n",
      "% Data Trained:38.6%, Loss:155.60056640625\n",
      "% Data Trained:39.1%, Loss:144.046494140625\n",
      "% Data Trained:39.6%, Loss:191.9267578125\n",
      "% Data Trained:40.1%, Loss:166.0884375\n",
      "% Data Trained:40.6%, Loss:182.7580078125\n",
      "% Data Trained:41.1%, Loss:166.41517578125\n",
      "% Data Trained:41.6%, Loss:159.008759765625\n",
      "% Data Trained:42.1%, Loss:153.06849609375\n",
      "% Data Trained:42.6%, Loss:192.8284765625\n",
      "% Data Trained:43.1%, Loss:142.06384765625\n",
      "% Data Trained:43.7%, Loss:165.13576171875\n",
      "% Data Trained:44.2%, Loss:149.01763671875\n",
      "% Data Trained:44.7%, Loss:171.92392578125\n",
      "% Data Trained:45.2%, Loss:160.314990234375\n",
      "% Data Trained:45.7%, Loss:199.35482421875\n",
      "% Data Trained:46.2%, Loss:166.22078125\n",
      "% Data Trained:46.7%, Loss:176.18\n",
      "% Data Trained:47.2%, Loss:144.59923828125\n",
      "% Data Trained:47.7%, Loss:177.90193359375\n",
      "% Data Trained:48.2%, Loss:124.979287109375\n",
      "% Data Trained:48.7%, Loss:155.346484375\n",
      "% Data Trained:49.2%, Loss:143.2432421875\n",
      "% Data Trained:49.7%, Loss:173.75267578125\n",
      "% Data Trained:50.3%, Loss:181.1567578125\n",
      "% Data Trained:50.8%, Loss:133.159833984375\n",
      "% Data Trained:51.3%, Loss:143.709599609375\n",
      "% Data Trained:51.8%, Loss:197.06736328125\n",
      "% Data Trained:52.3%, Loss:166.67478515625\n",
      "% Data Trained:52.8%, Loss:144.28525390625\n",
      "% Data Trained:53.3%, Loss:152.663056640625\n",
      "% Data Trained:53.8%, Loss:159.510390625\n",
      "% Data Trained:54.3%, Loss:181.81966796875\n",
      "% Data Trained:54.8%, Loss:159.244072265625\n",
      "% Data Trained:55.3%, Loss:170.9951171875\n",
      "% Data Trained:55.8%, Loss:167.512265625\n",
      "% Data Trained:56.3%, Loss:155.44533203125\n",
      "% Data Trained:56.9%, Loss:174.47669921875\n",
      "% Data Trained:57.4%, Loss:172.3444921875\n",
      "% Data Trained:57.9%, Loss:152.957734375\n",
      "% Data Trained:58.4%, Loss:156.364599609375\n",
      "% Data Trained:58.9%, Loss:174.733125\n",
      "% Data Trained:59.4%, Loss:152.09640625\n",
      "% Data Trained:59.9%, Loss:130.282646484375\n",
      "% Data Trained:60.4%, Loss:157.059775390625\n",
      "% Data Trained:60.9%, Loss:152.636416015625\n",
      "% Data Trained:61.4%, Loss:184.38412109375\n",
      "% Data Trained:61.9%, Loss:192.884609375\n",
      "% Data Trained:62.4%, Loss:155.0667578125\n",
      "% Data Trained:62.9%, Loss:134.567919921875\n",
      "% Data Trained:63.5%, Loss:181.92109375\n",
      "% Data Trained:64.0%, Loss:169.2205859375\n",
      "% Data Trained:64.5%, Loss:159.667998046875\n",
      "% Data Trained:65.0%, Loss:159.419375\n",
      "% Data Trained:65.5%, Loss:143.152099609375\n",
      "% Data Trained:66.0%, Loss:171.81658203125\n",
      "% Data Trained:66.5%, Loss:177.3165625\n",
      "% Data Trained:67.0%, Loss:152.648603515625\n",
      "% Data Trained:67.5%, Loss:163.752587890625\n",
      "% Data Trained:68.0%, Loss:174.4991015625\n",
      "% Data Trained:68.5%, Loss:172.37955078125\n",
      "% Data Trained:69.0%, Loss:141.633544921875\n",
      "% Data Trained:69.5%, Loss:161.373369140625\n",
      "% Data Trained:70.1%, Loss:160.415625\n",
      "% Data Trained:70.6%, Loss:207.52197265625\n",
      "% Data Trained:71.1%, Loss:161.525\n",
      "% Data Trained:71.6%, Loss:171.44595703125\n",
      "% Data Trained:72.1%, Loss:153.65046875\n",
      "% Data Trained:72.6%, Loss:188.6264453125\n",
      "% Data Trained:73.1%, Loss:168.72306640625\n",
      "% Data Trained:73.6%, Loss:178.65345703125\n",
      "% Data Trained:74.1%, Loss:201.01728515625\n",
      "% Data Trained:74.6%, Loss:158.8475390625\n",
      "% Data Trained:75.1%, Loss:157.9955078125\n",
      "% Data Trained:75.6%, Loss:147.525830078125\n",
      "% Data Trained:76.1%, Loss:147.368486328125\n",
      "% Data Trained:76.6%, Loss:179.66794921875\n",
      "% Data Trained:77.2%, Loss:156.9012109375\n",
      "% Data Trained:77.7%, Loss:171.30185546875\n",
      "% Data Trained:78.2%, Loss:157.127998046875\n",
      "% Data Trained:78.7%, Loss:172.06873046875\n",
      "% Data Trained:79.2%, Loss:163.813701171875\n",
      "% Data Trained:79.7%, Loss:162.97443359375\n",
      "% Data Trained:80.2%, Loss:173.00048828125\n",
      "% Data Trained:80.7%, Loss:151.387734375\n",
      "% Data Trained:81.2%, Loss:170.6678125\n",
      "% Data Trained:81.7%, Loss:177.23966796875\n",
      "% Data Trained:82.2%, Loss:164.5127734375\n",
      "% Data Trained:82.7%, Loss:162.54431640625\n",
      "% Data Trained:83.2%, Loss:157.81298828125\n",
      "% Data Trained:83.8%, Loss:175.7960546875\n",
      "% Data Trained:84.3%, Loss:162.933642578125\n",
      "% Data Trained:84.8%, Loss:191.95986328125\n",
      "% Data Trained:85.3%, Loss:153.4944921875\n",
      "% Data Trained:85.8%, Loss:174.87373046875\n",
      "% Data Trained:86.3%, Loss:166.8480859375\n",
      "% Data Trained:86.8%, Loss:170.06130859375\n",
      "% Data Trained:87.3%, Loss:159.51119140625\n",
      "% Data Trained:87.8%, Loss:175.10263671875\n",
      "% Data Trained:88.3%, Loss:169.3288671875\n",
      "% Data Trained:88.8%, Loss:156.058984375\n",
      "% Data Trained:89.3%, Loss:198.00234375\n",
      "% Data Trained:89.8%, Loss:159.618720703125\n",
      "% Data Trained:90.4%, Loss:145.776103515625\n",
      "% Data Trained:90.9%, Loss:165.5781640625\n",
      "% Data Trained:91.4%, Loss:163.046376953125\n",
      "% Data Trained:91.9%, Loss:150.482939453125\n",
      "% Data Trained:92.4%, Loss:168.20341796875\n",
      "% Data Trained:92.9%, Loss:163.3199609375\n",
      "% Data Trained:93.4%, Loss:146.269375\n",
      "% Data Trained:93.9%, Loss:197.37220703125\n",
      "% Data Trained:94.4%, Loss:155.526630859375\n",
      "% Data Trained:94.9%, Loss:155.33994140625\n",
      "% Data Trained:95.4%, Loss:152.423916015625\n",
      "% Data Trained:95.9%, Loss:153.9739453125\n",
      "% Data Trained:96.4%, Loss:181.47837890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Data Trained:97.0%, Loss:140.754892578125\n",
      "% Data Trained:97.5%, Loss:158.958837890625\n",
      "% Data Trained:98.0%, Loss:166.70544921875\n",
      "% Data Trained:98.5%, Loss:172.39291015625\n",
      "% Data Trained:99.0%, Loss:173.56361328125\n",
      "% Data Trained:99.5%, Loss:160.332236328125\n",
      "% Data Trained:100.0%, Loss:179.3815234375\n",
      "----------------------------------------------\n",
      "Training Epoch:3\n",
      "----------------------------------------------\n",
      "% Data Trained:0.0%, Loss:153.004609375\n",
      "% Data Trained:0.5%, Loss:160.8253515625\n",
      "% Data Trained:1.0%, Loss:154.101181640625\n",
      "% Data Trained:1.5%, Loss:163.532958984375\n",
      "% Data Trained:2.0%, Loss:147.432763671875\n",
      "% Data Trained:2.5%, Loss:154.188466796875\n",
      "% Data Trained:3.0%, Loss:145.51013671875\n",
      "% Data Trained:3.6%, Loss:157.418720703125\n",
      "% Data Trained:4.1%, Loss:153.103798828125\n",
      "% Data Trained:4.6%, Loss:188.6296484375\n",
      "% Data Trained:5.1%, Loss:146.704892578125\n",
      "% Data Trained:5.6%, Loss:173.1227734375\n",
      "% Data Trained:6.1%, Loss:173.112578125\n",
      "% Data Trained:6.6%, Loss:166.78005859375\n",
      "% Data Trained:7.1%, Loss:172.495859375\n",
      "% Data Trained:7.6%, Loss:157.5216796875\n",
      "% Data Trained:8.1%, Loss:158.462998046875\n",
      "% Data Trained:8.6%, Loss:159.159365234375\n",
      "% Data Trained:9.1%, Loss:155.64951171875\n",
      "% Data Trained:9.6%, Loss:155.266455078125\n",
      "% Data Trained:10.2%, Loss:205.051015625\n",
      "% Data Trained:10.7%, Loss:164.9602734375\n",
      "% Data Trained:11.2%, Loss:148.973935546875\n",
      "% Data Trained:11.7%, Loss:164.85984375\n",
      "% Data Trained:12.2%, Loss:158.71203125\n",
      "% Data Trained:12.7%, Loss:155.660458984375\n",
      "% Data Trained:13.2%, Loss:185.26677734375\n",
      "% Data Trained:13.7%, Loss:163.79279296875\n",
      "% Data Trained:14.2%, Loss:170.21666015625\n",
      "% Data Trained:14.7%, Loss:152.406806640625\n",
      "% Data Trained:15.2%, Loss:171.29587890625\n",
      "% Data Trained:15.7%, Loss:161.016376953125\n",
      "% Data Trained:16.2%, Loss:139.105576171875\n",
      "% Data Trained:16.8%, Loss:163.419912109375\n",
      "% Data Trained:17.3%, Loss:142.589873046875\n",
      "% Data Trained:17.8%, Loss:160.962509765625\n",
      "% Data Trained:18.3%, Loss:155.03662109375\n",
      "% Data Trained:18.8%, Loss:154.64171875\n",
      "% Data Trained:19.3%, Loss:168.30658203125\n",
      "% Data Trained:19.8%, Loss:200.05484375\n",
      "% Data Trained:20.3%, Loss:168.43771484375\n",
      "% Data Trained:20.8%, Loss:155.04234375\n",
      "% Data Trained:21.3%, Loss:167.14103515625\n",
      "% Data Trained:21.8%, Loss:201.0664453125\n",
      "% Data Trained:22.3%, Loss:179.36474609375\n",
      "% Data Trained:22.8%, Loss:161.6576171875\n",
      "% Data Trained:23.4%, Loss:140.94171875\n",
      "% Data Trained:23.9%, Loss:183.12267578125\n",
      "% Data Trained:24.4%, Loss:157.178212890625\n",
      "% Data Trained:24.9%, Loss:176.14787109375\n",
      "% Data Trained:25.4%, Loss:160.854921875\n",
      "% Data Trained:25.9%, Loss:204.78208984375\n",
      "% Data Trained:26.4%, Loss:178.1079296875\n",
      "% Data Trained:26.9%, Loss:142.747001953125\n",
      "% Data Trained:27.4%, Loss:162.86716796875\n",
      "% Data Trained:27.9%, Loss:171.6946484375\n",
      "% Data Trained:28.4%, Loss:159.297060546875\n",
      "% Data Trained:28.9%, Loss:148.3543359375\n",
      "% Data Trained:29.4%, Loss:161.502216796875\n",
      "% Data Trained:29.9%, Loss:159.579375\n",
      "% Data Trained:30.5%, Loss:145.156181640625\n",
      "% Data Trained:31.0%, Loss:145.112373046875\n",
      "% Data Trained:31.5%, Loss:134.87251953125\n",
      "% Data Trained:32.0%, Loss:161.2125\n",
      "% Data Trained:32.5%, Loss:146.504453125\n",
      "% Data Trained:33.0%, Loss:133.99095703125\n",
      "% Data Trained:33.5%, Loss:163.243486328125\n",
      "% Data Trained:34.0%, Loss:170.56822265625\n",
      "% Data Trained:34.5%, Loss:151.755029296875\n",
      "% Data Trained:35.0%, Loss:136.96765625\n",
      "% Data Trained:35.5%, Loss:165.95828125\n",
      "% Data Trained:36.0%, Loss:152.9626171875\n",
      "% Data Trained:36.5%, Loss:138.161240234375\n",
      "% Data Trained:37.1%, Loss:158.58896484375\n",
      "% Data Trained:37.6%, Loss:162.0919921875\n",
      "% Data Trained:38.1%, Loss:165.09095703125\n",
      "% Data Trained:38.6%, Loss:155.11421875\n",
      "% Data Trained:39.1%, Loss:143.8348828125\n",
      "% Data Trained:39.6%, Loss:192.30583984375\n",
      "% Data Trained:40.1%, Loss:165.7562109375\n",
      "% Data Trained:40.6%, Loss:181.667421875\n",
      "% Data Trained:41.1%, Loss:165.802109375\n",
      "% Data Trained:41.6%, Loss:158.86373046875\n",
      "% Data Trained:42.1%, Loss:152.32390625\n",
      "% Data Trained:42.6%, Loss:191.84396484375\n",
      "% Data Trained:43.1%, Loss:141.260693359375\n",
      "% Data Trained:43.7%, Loss:164.11021484375\n",
      "% Data Trained:44.2%, Loss:148.7763671875\n",
      "% Data Trained:44.7%, Loss:171.80599609375\n",
      "% Data Trained:45.2%, Loss:158.681572265625\n",
      "% Data Trained:45.7%, Loss:199.24404296875\n",
      "% Data Trained:46.2%, Loss:165.5403515625\n",
      "% Data Trained:46.7%, Loss:175.53962890625\n",
      "% Data Trained:47.2%, Loss:144.700732421875\n",
      "% Data Trained:47.7%, Loss:176.369921875\n",
      "% Data Trained:48.2%, Loss:124.4979296875\n",
      "% Data Trained:48.7%, Loss:155.27171875\n",
      "% Data Trained:49.2%, Loss:142.326787109375\n",
      "% Data Trained:49.7%, Loss:173.2359375\n",
      "% Data Trained:50.3%, Loss:180.76689453125\n",
      "% Data Trained:50.8%, Loss:132.66748046875\n",
      "% Data Trained:51.3%, Loss:142.83349609375\n",
      "% Data Trained:51.8%, Loss:196.60177734375\n",
      "% Data Trained:52.3%, Loss:166.349765625\n",
      "% Data Trained:52.8%, Loss:143.145693359375\n",
      "% Data Trained:53.3%, Loss:152.1375\n",
      "% Data Trained:53.8%, Loss:158.74798828125\n",
      "% Data Trained:54.3%, Loss:181.62427734375\n",
      "% Data Trained:54.8%, Loss:158.611259765625\n",
      "% Data Trained:55.3%, Loss:169.87318359375\n",
      "% Data Trained:55.8%, Loss:166.9573828125\n",
      "% Data Trained:56.3%, Loss:155.388662109375\n",
      "% Data Trained:56.9%, Loss:173.783125\n",
      "% Data Trained:57.4%, Loss:171.81068359375\n",
      "% Data Trained:57.9%, Loss:152.427978515625\n",
      "% Data Trained:58.4%, Loss:155.858671875\n",
      "% Data Trained:58.9%, Loss:174.08544921875\n",
      "% Data Trained:59.4%, Loss:152.01951171875\n",
      "% Data Trained:59.9%, Loss:130.072041015625\n",
      "% Data Trained:60.4%, Loss:156.2590625\n",
      "% Data Trained:60.9%, Loss:152.3286328125\n",
      "% Data Trained:61.4%, Loss:184.483046875\n",
      "% Data Trained:61.9%, Loss:192.85712890625\n",
      "% Data Trained:62.4%, Loss:155.378994140625\n",
      "% Data Trained:62.9%, Loss:134.25220703125\n",
      "% Data Trained:63.5%, Loss:181.671015625\n",
      "% Data Trained:64.0%, Loss:169.18720703125\n",
      "% Data Trained:64.5%, Loss:159.114912109375\n",
      "% Data Trained:65.0%, Loss:159.006982421875\n",
      "% Data Trained:65.5%, Loss:142.78044921875\n",
      "% Data Trained:66.0%, Loss:171.10828125\n",
      "% Data Trained:66.5%, Loss:177.32197265625\n",
      "% Data Trained:67.0%, Loss:151.649677734375\n",
      "% Data Trained:67.5%, Loss:163.78900390625\n",
      "% Data Trained:68.0%, Loss:173.840703125\n",
      "% Data Trained:68.5%, Loss:172.592890625\n",
      "% Data Trained:69.0%, Loss:141.444443359375\n",
      "% Data Trained:69.5%, Loss:161.296533203125\n",
      "% Data Trained:70.1%, Loss:160.11619140625\n",
      "% Data Trained:70.6%, Loss:207.101171875\n",
      "% Data Trained:71.1%, Loss:160.51619140625\n",
      "% Data Trained:71.6%, Loss:171.04720703125\n",
      "% Data Trained:72.1%, Loss:152.93453125\n",
      "% Data Trained:72.6%, Loss:187.9230859375\n",
      "% Data Trained:73.1%, Loss:168.40263671875\n",
      "% Data Trained:73.6%, Loss:178.6731640625\n",
      "% Data Trained:74.1%, Loss:201.14830078125\n",
      "% Data Trained:74.6%, Loss:157.82685546875\n",
      "% Data Trained:75.1%, Loss:157.909892578125\n",
      "% Data Trained:75.6%, Loss:146.727783203125\n",
      "% Data Trained:76.1%, Loss:146.719912109375\n",
      "% Data Trained:76.6%, Loss:179.68701171875\n",
      "% Data Trained:77.2%, Loss:156.598056640625\n",
      "% Data Trained:77.7%, Loss:170.634765625\n",
      "% Data Trained:78.2%, Loss:157.53486328125\n",
      "% Data Trained:78.7%, Loss:170.59380859375\n",
      "% Data Trained:79.2%, Loss:163.241484375\n",
      "% Data Trained:79.7%, Loss:162.935625\n",
      "% Data Trained:80.2%, Loss:172.32431640625\n",
      "% Data Trained:80.7%, Loss:151.518623046875\n",
      "% Data Trained:81.2%, Loss:167.67740234375\n",
      "% Data Trained:81.7%, Loss:177.0955859375\n",
      "% Data Trained:82.2%, Loss:163.95029296875\n",
      "% Data Trained:82.7%, Loss:161.251904296875\n",
      "% Data Trained:83.2%, Loss:158.060009765625\n",
      "% Data Trained:83.8%, Loss:174.5701171875\n",
      "% Data Trained:84.3%, Loss:161.315537109375\n",
      "% Data Trained:84.8%, Loss:192.42578125\n",
      "% Data Trained:85.3%, Loss:151.712470703125\n",
      "% Data Trained:85.8%, Loss:174.27875\n",
      "% Data Trained:86.3%, Loss:166.41689453125\n",
      "% Data Trained:86.8%, Loss:169.0764453125\n",
      "% Data Trained:87.3%, Loss:159.57771484375\n",
      "% Data Trained:87.8%, Loss:174.52359375\n",
      "% Data Trained:88.3%, Loss:168.3672265625\n",
      "% Data Trained:88.8%, Loss:155.95623046875\n",
      "% Data Trained:89.3%, Loss:197.24576171875\n",
      "% Data Trained:89.8%, Loss:158.863701171875\n",
      "% Data Trained:90.4%, Loss:145.59615234375\n",
      "% Data Trained:90.9%, Loss:164.7598828125\n",
      "% Data Trained:91.4%, Loss:162.301328125\n",
      "% Data Trained:91.9%, Loss:150.12826171875\n",
      "% Data Trained:92.4%, Loss:167.72693359375\n",
      "% Data Trained:92.9%, Loss:162.63576171875\n",
      "% Data Trained:93.4%, Loss:146.39462890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Data Trained:93.9%, Loss:197.51595703125\n",
      "% Data Trained:94.4%, Loss:155.0824609375\n",
      "% Data Trained:94.9%, Loss:155.570654296875\n",
      "% Data Trained:95.4%, Loss:152.76091796875\n",
      "% Data Trained:95.9%, Loss:154.42158203125\n",
      "% Data Trained:96.4%, Loss:181.0014453125\n",
      "% Data Trained:97.0%, Loss:139.680791015625\n",
      "% Data Trained:97.5%, Loss:159.722783203125\n",
      "% Data Trained:98.0%, Loss:167.00296875\n",
      "% Data Trained:98.5%, Loss:172.10455078125\n",
      "% Data Trained:99.0%, Loss:173.7453125\n",
      "% Data Trained:99.5%, Loss:160.281591796875\n",
      "% Data Trained:100.0%, Loss:179.617890625\n",
      "----------------------------------------------\n",
      "Training Epoch:4\n",
      "----------------------------------------------\n",
      "% Data Trained:0.0%, Loss:153.112734375\n",
      "% Data Trained:0.5%, Loss:160.60697265625\n",
      "% Data Trained:1.0%, Loss:154.209921875\n",
      "% Data Trained:1.5%, Loss:163.7662109375\n",
      "% Data Trained:2.0%, Loss:147.136533203125\n",
      "% Data Trained:2.5%, Loss:154.5501953125\n",
      "% Data Trained:3.0%, Loss:145.518720703125\n",
      "% Data Trained:3.6%, Loss:156.8441015625\n",
      "% Data Trained:4.1%, Loss:153.09234375\n",
      "% Data Trained:4.6%, Loss:188.17267578125\n",
      "% Data Trained:5.1%, Loss:146.38349609375\n",
      "% Data Trained:5.6%, Loss:173.72609375\n",
      "% Data Trained:6.1%, Loss:172.90548828125\n",
      "% Data Trained:6.6%, Loss:167.004140625\n",
      "% Data Trained:7.1%, Loss:172.6665234375\n",
      "% Data Trained:7.6%, Loss:157.176015625\n",
      "% Data Trained:8.1%, Loss:158.737275390625\n",
      "% Data Trained:8.6%, Loss:159.08294921875\n",
      "% Data Trained:9.1%, Loss:155.936298828125\n",
      "% Data Trained:9.6%, Loss:155.4866015625\n",
      "% Data Trained:10.2%, Loss:205.08390625\n",
      "% Data Trained:10.7%, Loss:165.23416015625\n",
      "% Data Trained:11.2%, Loss:149.816650390625\n",
      "% Data Trained:11.7%, Loss:165.05310546875\n",
      "% Data Trained:12.2%, Loss:159.802177734375\n",
      "% Data Trained:12.7%, Loss:155.53712890625\n",
      "% Data Trained:13.2%, Loss:184.8906640625\n",
      "% Data Trained:13.7%, Loss:164.13166015625\n",
      "% Data Trained:14.2%, Loss:170.64623046875\n",
      "% Data Trained:14.7%, Loss:152.669072265625\n",
      "% Data Trained:15.2%, Loss:170.935390625\n",
      "% Data Trained:15.7%, Loss:161.4598046875\n",
      "% Data Trained:16.2%, Loss:139.203125\n",
      "% Data Trained:16.8%, Loss:163.905390625\n",
      "% Data Trained:17.3%, Loss:142.761328125\n",
      "% Data Trained:17.8%, Loss:161.240654296875\n",
      "% Data Trained:18.3%, Loss:155.315537109375\n",
      "% Data Trained:18.8%, Loss:154.33833984375\n",
      "% Data Trained:19.3%, Loss:168.10677734375\n",
      "% Data Trained:19.8%, Loss:199.29666015625\n",
      "% Data Trained:20.3%, Loss:168.9167578125\n",
      "% Data Trained:20.8%, Loss:155.374873046875\n",
      "% Data Trained:21.3%, Loss:167.318828125\n",
      "% Data Trained:21.8%, Loss:200.72185546875\n",
      "% Data Trained:22.3%, Loss:179.269609375\n",
      "% Data Trained:22.8%, Loss:161.627490234375\n",
      "% Data Trained:23.4%, Loss:140.8494921875\n",
      "% Data Trained:23.9%, Loss:183.0015625\n",
      "% Data Trained:24.4%, Loss:157.264951171875\n",
      "% Data Trained:24.9%, Loss:175.9157421875\n",
      "% Data Trained:25.4%, Loss:161.000634765625\n",
      "% Data Trained:25.9%, Loss:204.02072265625\n",
      "% Data Trained:26.4%, Loss:178.23638671875\n",
      "% Data Trained:26.9%, Loss:142.19123046875\n",
      "% Data Trained:27.4%, Loss:163.12044921875\n",
      "% Data Trained:27.9%, Loss:171.44\n",
      "% Data Trained:28.4%, Loss:159.571787109375\n",
      "% Data Trained:28.9%, Loss:148.10421875\n",
      "% Data Trained:29.4%, Loss:161.85181640625\n",
      "% Data Trained:29.9%, Loss:160.268828125\n",
      "% Data Trained:30.5%, Loss:145.05537109375\n",
      "% Data Trained:31.0%, Loss:145.288046875\n",
      "% Data Trained:31.5%, Loss:135.2109375\n",
      "% Data Trained:32.0%, Loss:161.02828125\n",
      "% Data Trained:32.5%, Loss:146.552314453125\n",
      "% Data Trained:33.0%, Loss:134.07767578125\n",
      "% Data Trained:33.5%, Loss:163.3967578125\n",
      "% Data Trained:34.0%, Loss:170.65208984375\n",
      "% Data Trained:34.5%, Loss:151.518408203125\n",
      "% Data Trained:35.0%, Loss:136.59390625\n",
      "% Data Trained:35.5%, Loss:166.274609375\n",
      "% Data Trained:36.0%, Loss:152.746884765625\n",
      "% Data Trained:36.5%, Loss:137.953896484375\n",
      "% Data Trained:37.1%, Loss:158.801806640625\n",
      "% Data Trained:37.6%, Loss:162.510185546875\n",
      "% Data Trained:38.1%, Loss:164.850859375\n",
      "% Data Trained:38.6%, Loss:155.329716796875\n",
      "% Data Trained:39.1%, Loss:143.964814453125\n",
      "% Data Trained:39.6%, Loss:191.86689453125\n",
      "% Data Trained:40.1%, Loss:165.628046875\n",
      "% Data Trained:40.6%, Loss:181.36439453125\n",
      "% Data Trained:41.1%, Loss:165.493046875\n",
      "% Data Trained:41.6%, Loss:158.33837890625\n",
      "% Data Trained:42.1%, Loss:151.560888671875\n",
      "% Data Trained:42.6%, Loss:191.49177734375\n",
      "% Data Trained:43.1%, Loss:140.817265625\n",
      "% Data Trained:43.7%, Loss:163.448505859375\n",
      "% Data Trained:44.2%, Loss:148.520166015625\n",
      "% Data Trained:44.7%, Loss:171.65607421875\n",
      "% Data Trained:45.2%, Loss:158.342255859375\n",
      "% Data Trained:45.7%, Loss:199.0969140625\n",
      "% Data Trained:46.2%, Loss:164.8502734375\n",
      "% Data Trained:46.7%, Loss:175.20623046875\n",
      "% Data Trained:47.2%, Loss:144.554423828125\n",
      "% Data Trained:47.7%, Loss:175.7959765625\n",
      "% Data Trained:48.2%, Loss:124.46126953125\n",
      "% Data Trained:48.7%, Loss:154.450615234375\n",
      "% Data Trained:49.2%, Loss:142.2059375\n",
      "% Data Trained:49.7%, Loss:173.01763671875\n",
      "% Data Trained:50.3%, Loss:180.853671875\n",
      "% Data Trained:50.8%, Loss:132.602294921875\n",
      "% Data Trained:51.3%, Loss:143.220126953125\n",
      "% Data Trained:51.8%, Loss:196.37712890625\n",
      "% Data Trained:52.3%, Loss:166.490859375\n",
      "% Data Trained:52.8%, Loss:143.232509765625\n",
      "% Data Trained:53.3%, Loss:151.865390625\n",
      "% Data Trained:53.8%, Loss:158.61455078125\n",
      "% Data Trained:54.3%, Loss:181.6261328125\n",
      "% Data Trained:54.8%, Loss:158.604677734375\n",
      "% Data Trained:55.3%, Loss:170.24416015625\n",
      "% Data Trained:55.8%, Loss:166.29974609375\n",
      "% Data Trained:56.3%, Loss:155.116767578125\n",
      "% Data Trained:56.9%, Loss:174.00783203125\n",
      "% Data Trained:57.4%, Loss:171.3909375\n",
      "% Data Trained:57.9%, Loss:152.420078125\n",
      "% Data Trained:58.4%, Loss:155.589267578125\n",
      "% Data Trained:58.9%, Loss:174.28435546875\n",
      "% Data Trained:59.4%, Loss:151.3728515625\n",
      "% Data Trained:59.9%, Loss:130.3976953125\n",
      "% Data Trained:60.4%, Loss:156.42048828125\n",
      "% Data Trained:60.9%, Loss:152.22103515625\n",
      "% Data Trained:61.4%, Loss:184.44728515625\n",
      "% Data Trained:61.9%, Loss:192.25287109375\n",
      "% Data Trained:62.4%, Loss:154.916845703125\n",
      "% Data Trained:62.9%, Loss:134.470849609375\n",
      "% Data Trained:63.5%, Loss:181.4476953125\n",
      "% Data Trained:64.0%, Loss:168.7178125\n",
      "% Data Trained:64.5%, Loss:158.871435546875\n",
      "% Data Trained:65.0%, Loss:158.926298828125\n",
      "% Data Trained:65.5%, Loss:142.554853515625\n",
      "% Data Trained:66.0%, Loss:170.966328125\n",
      "% Data Trained:66.5%, Loss:177.56025390625\n",
      "% Data Trained:67.0%, Loss:151.379423828125\n",
      "% Data Trained:67.5%, Loss:163.59736328125\n",
      "% Data Trained:68.0%, Loss:174.130390625\n",
      "% Data Trained:68.5%, Loss:171.982265625\n",
      "% Data Trained:69.0%, Loss:141.148583984375\n",
      "% Data Trained:69.5%, Loss:161.273193359375\n",
      "% Data Trained:70.1%, Loss:159.976669921875\n",
      "% Data Trained:70.6%, Loss:206.784921875\n",
      "% Data Trained:71.1%, Loss:160.753828125\n",
      "% Data Trained:71.6%, Loss:170.82814453125\n",
      "% Data Trained:72.1%, Loss:152.58515625\n",
      "% Data Trained:72.6%, Loss:187.863515625\n",
      "% Data Trained:73.1%, Loss:168.267578125\n",
      "% Data Trained:73.6%, Loss:178.7152734375\n",
      "% Data Trained:74.1%, Loss:200.64623046875\n",
      "% Data Trained:74.6%, Loss:158.093994140625\n",
      "% Data Trained:75.1%, Loss:157.04634765625\n",
      "% Data Trained:75.6%, Loss:146.59822265625\n",
      "% Data Trained:76.1%, Loss:146.896484375\n",
      "% Data Trained:76.6%, Loss:179.18193359375\n",
      "% Data Trained:77.2%, Loss:156.254384765625\n",
      "% Data Trained:77.7%, Loss:170.25740234375\n",
      "% Data Trained:78.2%, Loss:156.959619140625\n",
      "% Data Trained:78.7%, Loss:170.6346875\n",
      "% Data Trained:79.2%, Loss:163.189716796875\n",
      "% Data Trained:79.7%, Loss:162.34552734375\n",
      "% Data Trained:80.2%, Loss:171.874921875\n",
      "% Data Trained:80.7%, Loss:151.00568359375\n",
      "% Data Trained:81.2%, Loss:167.51900390625\n",
      "% Data Trained:81.7%, Loss:176.37119140625\n",
      "% Data Trained:82.2%, Loss:163.5211328125\n",
      "% Data Trained:82.7%, Loss:161.9734765625\n",
      "% Data Trained:83.2%, Loss:158.232236328125\n",
      "% Data Trained:83.8%, Loss:174.14484375\n",
      "% Data Trained:84.3%, Loss:161.594853515625\n",
      "% Data Trained:84.8%, Loss:191.91533203125\n",
      "% Data Trained:85.3%, Loss:151.733623046875\n",
      "% Data Trained:85.8%, Loss:174.20205078125\n",
      "% Data Trained:86.3%, Loss:165.96998046875\n",
      "% Data Trained:86.8%, Loss:169.20986328125\n",
      "% Data Trained:87.3%, Loss:159.132470703125\n",
      "% Data Trained:87.8%, Loss:174.48205078125\n",
      "% Data Trained:88.3%, Loss:168.29953125\n",
      "% Data Trained:88.8%, Loss:156.31484375\n",
      "% Data Trained:89.3%, Loss:196.8357421875\n",
      "% Data Trained:89.8%, Loss:158.8158203125\n",
      "% Data Trained:90.4%, Loss:145.41048828125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Data Trained:90.9%, Loss:164.30744140625\n",
      "% Data Trained:91.4%, Loss:162.56736328125\n",
      "% Data Trained:91.9%, Loss:149.707607421875\n",
      "% Data Trained:92.4%, Loss:167.50611328125\n",
      "% Data Trained:92.9%, Loss:162.070830078125\n",
      "% Data Trained:93.4%, Loss:146.29720703125\n",
      "% Data Trained:93.9%, Loss:197.38193359375\n",
      "% Data Trained:94.4%, Loss:155.19283203125\n",
      "% Data Trained:94.9%, Loss:155.115224609375\n",
      "% Data Trained:95.4%, Loss:152.27916015625\n",
      "% Data Trained:95.9%, Loss:153.535390625\n",
      "% Data Trained:96.4%, Loss:180.8907421875\n",
      "% Data Trained:97.0%, Loss:139.8755859375\n",
      "% Data Trained:97.5%, Loss:158.98708984375\n",
      "% Data Trained:98.0%, Loss:166.51359375\n",
      "% Data Trained:98.5%, Loss:172.01533203125\n",
      "% Data Trained:99.0%, Loss:173.362109375\n",
      "% Data Trained:99.5%, Loss:160.063837890625\n",
      "% Data Trained:100.0%, Loss:178.47685546875\n",
      "----------------------------------------------\n",
      "Training Epoch:5\n",
      "----------------------------------------------\n",
      "% Data Trained:0.0%, Loss:153.42287109375\n",
      "% Data Trained:0.5%, Loss:160.08888671875\n",
      "% Data Trained:1.0%, Loss:153.625966796875\n",
      "% Data Trained:1.5%, Loss:163.80830078125\n",
      "% Data Trained:2.0%, Loss:146.684921875\n",
      "% Data Trained:2.5%, Loss:153.809111328125\n",
      "% Data Trained:3.0%, Loss:146.086806640625\n",
      "% Data Trained:3.6%, Loss:156.88720703125\n",
      "% Data Trained:4.1%, Loss:153.4112890625\n",
      "% Data Trained:4.6%, Loss:188.74244140625\n",
      "% Data Trained:5.1%, Loss:146.17076171875\n",
      "% Data Trained:5.6%, Loss:173.84384765625\n",
      "% Data Trained:6.1%, Loss:172.6020703125\n",
      "% Data Trained:6.6%, Loss:166.68900390625\n",
      "% Data Trained:7.1%, Loss:172.702578125\n",
      "% Data Trained:7.6%, Loss:157.335107421875\n",
      "% Data Trained:8.1%, Loss:158.2676953125\n",
      "% Data Trained:8.6%, Loss:158.775390625\n",
      "% Data Trained:9.1%, Loss:155.508203125\n",
      "% Data Trained:9.6%, Loss:154.926279296875\n",
      "% Data Trained:10.2%, Loss:206.329609375\n",
      "% Data Trained:10.7%, Loss:164.8368359375\n",
      "% Data Trained:11.2%, Loss:150.801748046875\n",
      "% Data Trained:11.7%, Loss:165.258984375\n",
      "% Data Trained:12.2%, Loss:158.955234375\n",
      "% Data Trained:12.7%, Loss:156.74791015625\n",
      "% Data Trained:13.2%, Loss:184.7143359375\n",
      "% Data Trained:13.7%, Loss:164.0946875\n",
      "% Data Trained:14.2%, Loss:171.062109375\n",
      "% Data Trained:14.7%, Loss:152.0858203125\n",
      "% Data Trained:15.2%, Loss:171.02578125\n",
      "% Data Trained:15.7%, Loss:161.190986328125\n",
      "% Data Trained:16.2%, Loss:139.0125390625\n",
      "% Data Trained:16.8%, Loss:163.732138671875\n",
      "% Data Trained:17.3%, Loss:142.81365234375\n",
      "% Data Trained:17.8%, Loss:160.644521484375\n",
      "% Data Trained:18.3%, Loss:155.10462890625\n",
      "% Data Trained:18.8%, Loss:154.7347265625\n",
      "% Data Trained:19.3%, Loss:167.70875\n",
      "% Data Trained:19.8%, Loss:199.51078125\n",
      "% Data Trained:20.3%, Loss:168.95888671875\n",
      "% Data Trained:20.8%, Loss:154.68123046875\n",
      "% Data Trained:21.3%, Loss:168.25052734375\n",
      "% Data Trained:21.8%, Loss:201.06322265625\n",
      "% Data Trained:22.3%, Loss:179.015234375\n",
      "% Data Trained:22.8%, Loss:162.420576171875\n",
      "% Data Trained:23.4%, Loss:140.51359375\n",
      "% Data Trained:23.9%, Loss:182.5916015625\n",
      "% Data Trained:24.4%, Loss:158.07328125\n",
      "% Data Trained:24.9%, Loss:175.3451953125\n",
      "% Data Trained:25.4%, Loss:161.217919921875\n",
      "% Data Trained:25.9%, Loss:204.8297265625\n",
      "% Data Trained:26.4%, Loss:178.028359375\n",
      "% Data Trained:26.9%, Loss:142.4375\n",
      "% Data Trained:27.4%, Loss:164.04794921875\n",
      "% Data Trained:27.9%, Loss:171.04396484375\n",
      "% Data Trained:28.4%, Loss:159.701064453125\n",
      "% Data Trained:28.9%, Loss:148.28017578125\n",
      "% Data Trained:29.4%, Loss:161.18384765625\n",
      "% Data Trained:29.9%, Loss:160.312216796875\n",
      "% Data Trained:30.5%, Loss:145.226875\n",
      "% Data Trained:31.0%, Loss:144.8933203125\n",
      "% Data Trained:31.5%, Loss:135.409912109375\n",
      "% Data Trained:32.0%, Loss:161.699189453125\n",
      "% Data Trained:32.5%, Loss:146.29884765625\n",
      "% Data Trained:33.0%, Loss:135.338994140625\n",
      "% Data Trained:33.5%, Loss:164.33587890625\n",
      "% Data Trained:34.0%, Loss:170.25205078125\n",
      "% Data Trained:34.5%, Loss:152.98998046875\n",
      "% Data Trained:35.0%, Loss:136.60927734375\n",
      "% Data Trained:35.5%, Loss:166.16060546875\n",
      "% Data Trained:36.0%, Loss:153.352578125\n",
      "% Data Trained:36.5%, Loss:138.264677734375\n",
      "% Data Trained:37.1%, Loss:158.7144921875\n",
      "% Data Trained:37.6%, Loss:163.045859375\n",
      "% Data Trained:38.1%, Loss:164.9934375\n",
      "% Data Trained:38.6%, Loss:156.205068359375\n",
      "% Data Trained:39.1%, Loss:144.427275390625\n",
      "% Data Trained:39.6%, Loss:192.30701171875\n",
      "% Data Trained:40.1%, Loss:166.55279296875\n",
      "% Data Trained:40.6%, Loss:182.72140625\n",
      "% Data Trained:41.1%, Loss:165.58529296875\n",
      "% Data Trained:41.6%, Loss:159.818466796875\n",
      "% Data Trained:42.1%, Loss:152.9473828125\n",
      "% Data Trained:42.6%, Loss:191.52919921875\n",
      "% Data Trained:43.1%, Loss:141.223515625\n",
      "% Data Trained:43.7%, Loss:164.294453125\n",
      "% Data Trained:44.2%, Loss:148.23251953125\n",
      "% Data Trained:44.7%, Loss:172.070859375\n",
      "% Data Trained:45.2%, Loss:158.1996875\n",
      "% Data Trained:45.7%, Loss:198.58767578125\n",
      "% Data Trained:46.2%, Loss:164.7303515625\n",
      "% Data Trained:46.7%, Loss:174.63822265625\n",
      "% Data Trained:47.2%, Loss:144.8318359375\n",
      "% Data Trained:47.7%, Loss:176.256015625\n",
      "% Data Trained:48.2%, Loss:124.43380859375\n",
      "% Data Trained:48.7%, Loss:154.2903515625\n",
      "% Data Trained:49.2%, Loss:142.077177734375\n",
      "% Data Trained:49.7%, Loss:172.583359375\n",
      "% Data Trained:50.3%, Loss:181.220703125\n",
      "% Data Trained:50.8%, Loss:132.2934765625\n",
      "% Data Trained:51.3%, Loss:143.131669921875\n",
      "% Data Trained:51.8%, Loss:195.961171875\n",
      "% Data Trained:52.3%, Loss:166.0934375\n",
      "% Data Trained:52.8%, Loss:142.5308984375\n",
      "% Data Trained:53.3%, Loss:151.852841796875\n",
      "% Data Trained:53.8%, Loss:158.47583984375\n",
      "% Data Trained:54.3%, Loss:181.01244140625\n",
      "% Data Trained:54.8%, Loss:158.692275390625\n",
      "% Data Trained:55.3%, Loss:170.01400390625\n",
      "% Data Trained:55.8%, Loss:166.36193359375\n",
      "% Data Trained:56.3%, Loss:154.893857421875\n",
      "% Data Trained:56.9%, Loss:173.8198046875\n",
      "% Data Trained:57.4%, Loss:171.3700390625\n",
      "% Data Trained:57.9%, Loss:152.478193359375\n",
      "% Data Trained:58.4%, Loss:155.570908203125\n",
      "% Data Trained:58.9%, Loss:173.80765625\n",
      "% Data Trained:59.4%, Loss:151.438515625\n",
      "% Data Trained:59.9%, Loss:130.020849609375\n",
      "% Data Trained:60.4%, Loss:156.1270703125\n",
      "% Data Trained:60.9%, Loss:151.980673828125\n",
      "% Data Trained:61.4%, Loss:183.986875\n",
      "% Data Trained:61.9%, Loss:192.32978515625\n",
      "% Data Trained:62.4%, Loss:155.288388671875\n",
      "% Data Trained:62.9%, Loss:134.435361328125\n",
      "% Data Trained:63.5%, Loss:181.6619140625\n",
      "% Data Trained:64.0%, Loss:168.9130859375\n",
      "% Data Trained:64.5%, Loss:158.749775390625\n",
      "% Data Trained:65.0%, Loss:159.001640625\n",
      "% Data Trained:65.5%, Loss:142.48037109375\n",
      "% Data Trained:66.0%, Loss:172.09337890625\n",
      "% Data Trained:66.5%, Loss:177.17314453125\n",
      "% Data Trained:67.0%, Loss:151.94087890625\n",
      "% Data Trained:67.5%, Loss:163.282470703125\n",
      "% Data Trained:68.0%, Loss:174.3476171875\n",
      "% Data Trained:68.5%, Loss:171.77736328125\n",
      "% Data Trained:69.0%, Loss:141.241591796875\n",
      "% Data Trained:69.5%, Loss:161.311484375\n",
      "% Data Trained:70.1%, Loss:159.9724609375\n",
      "% Data Trained:70.6%, Loss:207.3099609375\n",
      "% Data Trained:71.1%, Loss:160.506357421875\n",
      "% Data Trained:71.6%, Loss:170.61197265625\n",
      "% Data Trained:72.1%, Loss:152.681923828125\n",
      "% Data Trained:72.6%, Loss:187.7571875\n",
      "% Data Trained:73.1%, Loss:167.99923828125\n",
      "% Data Trained:73.6%, Loss:178.61224609375\n",
      "% Data Trained:74.1%, Loss:200.19234375\n",
      "% Data Trained:74.6%, Loss:157.76140625\n",
      "% Data Trained:75.1%, Loss:157.17447265625\n",
      "% Data Trained:75.6%, Loss:146.12986328125\n",
      "% Data Trained:76.1%, Loss:146.19232421875\n",
      "% Data Trained:76.6%, Loss:179.70494140625\n",
      "% Data Trained:77.2%, Loss:156.145439453125\n",
      "% Data Trained:77.7%, Loss:170.02056640625\n",
      "% Data Trained:78.2%, Loss:156.802109375\n",
      "% Data Trained:78.7%, Loss:170.14251953125\n",
      "% Data Trained:79.2%, Loss:162.8871484375\n",
      "% Data Trained:79.7%, Loss:162.297509765625\n",
      "% Data Trained:80.2%, Loss:171.731875\n",
      "% Data Trained:80.7%, Loss:150.6085546875\n",
      "% Data Trained:81.2%, Loss:166.9013671875\n",
      "% Data Trained:81.7%, Loss:175.73533203125\n",
      "% Data Trained:82.2%, Loss:163.793369140625\n",
      "% Data Trained:82.7%, Loss:160.972724609375\n",
      "% Data Trained:83.2%, Loss:158.121181640625\n",
      "% Data Trained:83.8%, Loss:174.58087890625\n",
      "% Data Trained:84.3%, Loss:160.96423828125\n",
      "% Data Trained:84.8%, Loss:191.5642578125\n",
      "% Data Trained:85.3%, Loss:151.264306640625\n",
      "% Data Trained:85.8%, Loss:173.0894140625\n",
      "% Data Trained:86.3%, Loss:165.56962890625\n",
      "% Data Trained:86.8%, Loss:168.8682421875\n",
      "% Data Trained:87.3%, Loss:158.516484375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Data Trained:87.8%, Loss:174.47962890625\n",
      "% Data Trained:88.3%, Loss:167.59708984375\n",
      "% Data Trained:88.8%, Loss:156.453203125\n",
      "% Data Trained:89.3%, Loss:196.12791015625\n",
      "% Data Trained:89.8%, Loss:158.932587890625\n",
      "% Data Trained:90.4%, Loss:145.042509765625\n",
      "% Data Trained:90.9%, Loss:164.552890625\n",
      "% Data Trained:91.4%, Loss:162.45865234375\n",
      "% Data Trained:91.9%, Loss:149.14744140625\n",
      "% Data Trained:92.4%, Loss:167.47138671875\n",
      "% Data Trained:92.9%, Loss:162.042578125\n",
      "% Data Trained:93.4%, Loss:145.67517578125\n",
      "% Data Trained:93.9%, Loss:196.92607421875\n",
      "% Data Trained:94.4%, Loss:154.080234375\n",
      "% Data Trained:94.9%, Loss:154.8966015625\n",
      "% Data Trained:95.4%, Loss:152.1010546875\n",
      "% Data Trained:95.9%, Loss:153.045634765625\n",
      "% Data Trained:96.4%, Loss:180.1700390625\n",
      "% Data Trained:97.0%, Loss:139.542431640625\n",
      "% Data Trained:97.5%, Loss:159.279375\n",
      "% Data Trained:98.0%, Loss:166.31001953125\n",
      "% Data Trained:98.5%, Loss:172.025546875\n",
      "% Data Trained:99.0%, Loss:172.46298828125\n",
      "% Data Trained:99.5%, Loss:159.543193359375\n",
      "% Data Trained:100.0%, Loss:177.6597265625\n",
      "----------------------------------------------\n",
      "Training Epoch:6\n",
      "----------------------------------------------\n",
      "% Data Trained:0.0%, Loss:153.2128125\n",
      "% Data Trained:0.5%, Loss:160.23724609375\n",
      "% Data Trained:1.0%, Loss:153.300693359375\n",
      "% Data Trained:1.5%, Loss:163.557255859375\n",
      "% Data Trained:2.0%, Loss:147.16814453125\n",
      "% Data Trained:2.5%, Loss:154.389912109375\n",
      "% Data Trained:3.0%, Loss:145.81125\n",
      "% Data Trained:3.6%, Loss:156.99619140625\n",
      "% Data Trained:4.1%, Loss:152.731474609375\n",
      "% Data Trained:4.6%, Loss:189.195390625\n",
      "% Data Trained:5.1%, Loss:146.82330078125\n",
      "% Data Trained:5.6%, Loss:174.14779296875\n",
      "% Data Trained:6.1%, Loss:172.388046875\n",
      "% Data Trained:6.6%, Loss:165.7087109375\n",
      "% Data Trained:7.1%, Loss:172.441171875\n",
      "% Data Trained:7.6%, Loss:157.955986328125\n",
      "% Data Trained:8.1%, Loss:158.218466796875\n",
      "% Data Trained:8.6%, Loss:159.401103515625\n",
      "% Data Trained:9.1%, Loss:155.00029296875\n",
      "% Data Trained:9.6%, Loss:155.08544921875\n",
      "% Data Trained:10.2%, Loss:205.61255859375\n",
      "% Data Trained:10.7%, Loss:165.32392578125\n",
      "% Data Trained:11.2%, Loss:148.715458984375\n",
      "% Data Trained:11.7%, Loss:165.15208984375\n",
      "% Data Trained:12.2%, Loss:158.18341796875\n",
      "% Data Trained:12.7%, Loss:155.920615234375\n",
      "% Data Trained:13.2%, Loss:185.2412890625\n",
      "% Data Trained:13.7%, Loss:163.614931640625\n",
      "% Data Trained:14.2%, Loss:171.7590625\n",
      "% Data Trained:14.7%, Loss:152.15392578125\n",
      "% Data Trained:15.2%, Loss:170.3587890625\n",
      "% Data Trained:15.7%, Loss:162.77380859375\n",
      "% Data Trained:16.2%, Loss:138.880615234375\n",
      "% Data Trained:16.8%, Loss:163.5328125\n",
      "% Data Trained:17.3%, Loss:142.9412109375\n",
      "% Data Trained:17.8%, Loss:161.159267578125\n",
      "% Data Trained:18.3%, Loss:155.114755859375\n",
      "% Data Trained:18.8%, Loss:154.397900390625\n",
      "% Data Trained:19.3%, Loss:167.6990234375\n",
      "% Data Trained:19.8%, Loss:199.27740234375\n",
      "% Data Trained:20.3%, Loss:169.152734375\n",
      "% Data Trained:20.8%, Loss:154.344970703125\n",
      "% Data Trained:21.3%, Loss:167.5526171875\n",
      "% Data Trained:21.8%, Loss:201.3259375\n",
      "% Data Trained:22.3%, Loss:178.94900390625\n",
      "% Data Trained:22.8%, Loss:162.531201171875\n",
      "% Data Trained:23.4%, Loss:140.717294921875\n",
      "% Data Trained:23.9%, Loss:182.4412890625\n",
      "% Data Trained:24.4%, Loss:158.058662109375\n",
      "% Data Trained:24.9%, Loss:175.10841796875\n",
      "% Data Trained:25.4%, Loss:161.32607421875\n",
      "% Data Trained:25.9%, Loss:204.75359375\n",
      "% Data Trained:26.4%, Loss:177.14646484375\n",
      "% Data Trained:26.9%, Loss:142.037421875\n",
      "% Data Trained:27.4%, Loss:164.9782421875\n",
      "% Data Trained:27.9%, Loss:172.21052734375\n",
      "% Data Trained:28.4%, Loss:160.245\n",
      "% Data Trained:28.9%, Loss:148.769609375\n",
      "% Data Trained:29.4%, Loss:160.87552734375\n",
      "% Data Trained:29.9%, Loss:159.968388671875\n",
      "% Data Trained:30.5%, Loss:146.12013671875\n",
      "% Data Trained:31.0%, Loss:145.152412109375\n",
      "% Data Trained:31.5%, Loss:135.24109375\n",
      "% Data Trained:32.0%, Loss:162.502861328125\n",
      "% Data Trained:32.5%, Loss:146.129306640625\n",
      "% Data Trained:33.0%, Loss:134.3424609375\n",
      "% Data Trained:33.5%, Loss:164.74279296875\n",
      "% Data Trained:34.0%, Loss:170.45341796875\n",
      "% Data Trained:34.5%, Loss:152.607861328125\n",
      "% Data Trained:35.0%, Loss:137.6017578125\n",
      "% Data Trained:35.5%, Loss:166.34970703125\n",
      "% Data Trained:36.0%, Loss:153.619658203125\n",
      "% Data Trained:36.5%, Loss:139.076865234375\n",
      "% Data Trained:37.1%, Loss:159.241669921875\n",
      "% Data Trained:37.6%, Loss:162.50990234375\n",
      "% Data Trained:38.1%, Loss:165.442890625\n",
      "% Data Trained:38.6%, Loss:154.97384765625\n",
      "% Data Trained:39.1%, Loss:143.461123046875\n",
      "% Data Trained:39.6%, Loss:194.1548046875\n",
      "% Data Trained:40.1%, Loss:166.2023828125\n",
      "% Data Trained:40.6%, Loss:181.5987890625\n",
      "% Data Trained:41.1%, Loss:167.91669921875\n",
      "% Data Trained:41.6%, Loss:158.4798828125\n",
      "% Data Trained:42.1%, Loss:152.638408203125\n",
      "% Data Trained:42.6%, Loss:194.93322265625\n",
      "% Data Trained:43.1%, Loss:141.45560546875\n",
      "% Data Trained:43.7%, Loss:165.49724609375\n",
      "% Data Trained:44.2%, Loss:149.39296875\n",
      "% Data Trained:44.7%, Loss:171.738125\n",
      "% Data Trained:45.2%, Loss:160.28458984375\n",
      "% Data Trained:45.7%, Loss:199.9974609375\n",
      "% Data Trained:46.2%, Loss:164.9191015625\n",
      "% Data Trained:46.7%, Loss:176.3801953125\n",
      "% Data Trained:47.2%, Loss:146.18349609375\n",
      "% Data Trained:47.7%, Loss:176.88162109375\n",
      "% Data Trained:48.2%, Loss:125.4916796875\n",
      "% Data Trained:48.7%, Loss:154.50642578125\n",
      "% Data Trained:49.2%, Loss:142.24255859375\n",
      "% Data Trained:49.7%, Loss:172.938671875\n",
      "% Data Trained:50.3%, Loss:182.22013671875\n",
      "% Data Trained:50.8%, Loss:132.340615234375\n",
      "% Data Trained:51.3%, Loss:143.0958203125\n",
      "% Data Trained:51.8%, Loss:197.7712890625\n",
      "% Data Trained:52.3%, Loss:166.53458984375\n",
      "% Data Trained:52.8%, Loss:143.070615234375\n",
      "% Data Trained:53.3%, Loss:152.28982421875\n",
      "% Data Trained:53.8%, Loss:158.47361328125\n",
      "% Data Trained:54.3%, Loss:181.426875\n",
      "% Data Trained:54.8%, Loss:159.36185546875\n",
      "% Data Trained:55.3%, Loss:169.82154296875\n",
      "% Data Trained:55.8%, Loss:165.9122265625\n",
      "% Data Trained:56.3%, Loss:155.221474609375\n",
      "% Data Trained:56.9%, Loss:174.3323828125\n",
      "% Data Trained:57.4%, Loss:170.92228515625\n",
      "% Data Trained:57.9%, Loss:152.643515625\n",
      "% Data Trained:58.4%, Loss:156.62875\n",
      "% Data Trained:58.9%, Loss:174.27998046875\n",
      "% Data Trained:59.4%, Loss:151.46287109375\n",
      "% Data Trained:59.9%, Loss:130.08615234375\n",
      "% Data Trained:60.4%, Loss:156.2401171875\n",
      "% Data Trained:60.9%, Loss:151.819091796875\n",
      "% Data Trained:61.4%, Loss:184.08837890625\n",
      "% Data Trained:61.9%, Loss:192.1586328125\n",
      "% Data Trained:62.4%, Loss:155.046181640625\n",
      "% Data Trained:62.9%, Loss:134.7640234375\n",
      "% Data Trained:63.5%, Loss:181.5198046875\n",
      "% Data Trained:64.0%, Loss:169.102578125\n",
      "% Data Trained:64.5%, Loss:158.942763671875\n",
      "% Data Trained:65.0%, Loss:158.5000390625\n",
      "% Data Trained:65.5%, Loss:142.292783203125\n",
      "% Data Trained:66.0%, Loss:171.3833203125\n",
      "% Data Trained:66.5%, Loss:176.92669921875\n",
      "% Data Trained:67.0%, Loss:151.49234375\n",
      "% Data Trained:67.5%, Loss:162.83259765625\n",
      "% Data Trained:68.0%, Loss:173.72642578125\n",
      "% Data Trained:68.5%, Loss:171.2674609375\n",
      "% Data Trained:69.0%, Loss:141.354951171875\n",
      "% Data Trained:69.5%, Loss:161.03630859375\n",
      "% Data Trained:70.1%, Loss:159.415537109375\n",
      "% Data Trained:70.6%, Loss:207.66357421875\n",
      "% Data Trained:71.1%, Loss:160.575380859375\n",
      "% Data Trained:71.6%, Loss:170.96419921875\n",
      "% Data Trained:72.1%, Loss:152.42080078125\n",
      "% Data Trained:72.6%, Loss:187.40927734375\n",
      "% Data Trained:73.1%, Loss:168.4853515625\n",
      "% Data Trained:73.6%, Loss:178.3415234375\n",
      "% Data Trained:74.1%, Loss:200.53421875\n",
      "% Data Trained:74.6%, Loss:158.626123046875\n",
      "% Data Trained:75.1%, Loss:156.781318359375\n",
      "% Data Trained:75.6%, Loss:146.349736328125\n",
      "% Data Trained:76.1%, Loss:146.69544921875\n",
      "% Data Trained:76.6%, Loss:180.59154296875\n",
      "% Data Trained:77.2%, Loss:156.64931640625\n",
      "% Data Trained:77.7%, Loss:171.436640625\n",
      "% Data Trained:78.2%, Loss:156.7617578125\n",
      "% Data Trained:78.7%, Loss:170.7834375\n",
      "% Data Trained:79.2%, Loss:164.03017578125\n",
      "% Data Trained:79.7%, Loss:161.867099609375\n",
      "% Data Trained:80.2%, Loss:172.37986328125\n",
      "% Data Trained:80.7%, Loss:151.22431640625\n",
      "% Data Trained:81.2%, Loss:166.97947265625\n",
      "% Data Trained:81.7%, Loss:175.77046875\n",
      "% Data Trained:82.2%, Loss:164.983125\n",
      "% Data Trained:82.7%, Loss:160.686982421875\n",
      "% Data Trained:83.2%, Loss:158.904443359375\n",
      "% Data Trained:83.8%, Loss:174.41150390625\n",
      "% Data Trained:84.3%, Loss:160.89228515625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Data Trained:84.8%, Loss:191.71162109375\n",
      "% Data Trained:85.3%, Loss:151.4354296875\n",
      "% Data Trained:85.8%, Loss:172.7721875\n",
      "% Data Trained:86.3%, Loss:166.054375\n",
      "% Data Trained:86.8%, Loss:168.7397265625\n",
      "% Data Trained:87.3%, Loss:157.89552734375\n",
      "% Data Trained:87.8%, Loss:175.27548828125\n",
      "% Data Trained:88.3%, Loss:167.02548828125\n",
      "% Data Trained:88.8%, Loss:156.330322265625\n",
      "% Data Trained:89.3%, Loss:196.68552734375\n",
      "% Data Trained:89.8%, Loss:158.330048828125\n",
      "% Data Trained:90.4%, Loss:145.095419921875\n",
      "% Data Trained:90.9%, Loss:165.14962890625\n",
      "% Data Trained:91.4%, Loss:161.426611328125\n",
      "% Data Trained:91.9%, Loss:148.48091796875\n",
      "% Data Trained:92.4%, Loss:168.88291015625\n",
      "% Data Trained:92.9%, Loss:162.200625\n",
      "% Data Trained:93.4%, Loss:145.8366796875\n",
      "% Data Trained:93.9%, Loss:198.54908203125\n",
      "% Data Trained:94.4%, Loss:154.334443359375\n",
      "% Data Trained:94.9%, Loss:155.87771484375\n",
      "% Data Trained:95.4%, Loss:152.358896484375\n",
      "% Data Trained:95.9%, Loss:153.394326171875\n",
      "% Data Trained:96.4%, Loss:180.47794921875\n",
      "% Data Trained:97.0%, Loss:140.554287109375\n",
      "% Data Trained:97.5%, Loss:158.353642578125\n",
      "% Data Trained:98.0%, Loss:166.49494140625\n",
      "% Data Trained:98.5%, Loss:172.1985546875\n",
      "% Data Trained:99.0%, Loss:172.090078125\n",
      "% Data Trained:99.5%, Loss:159.735205078125\n",
      "% Data Trained:100.0%, Loss:178.2111328125\n",
      "----------------------------------------------\n",
      "Training Epoch:7\n",
      "----------------------------------------------\n",
      "% Data Trained:0.0%, Loss:152.547548828125\n",
      "% Data Trained:0.5%, Loss:159.581640625\n",
      "% Data Trained:1.0%, Loss:153.344931640625\n",
      "% Data Trained:1.5%, Loss:163.215703125\n",
      "% Data Trained:2.0%, Loss:146.804697265625\n",
      "% Data Trained:2.5%, Loss:153.9851171875\n",
      "% Data Trained:3.0%, Loss:145.49138671875\n",
      "% Data Trained:3.6%, Loss:157.713984375\n",
      "% Data Trained:4.1%, Loss:152.755703125\n",
      "% Data Trained:4.6%, Loss:188.048984375\n",
      "% Data Trained:5.1%, Loss:147.279765625\n",
      "% Data Trained:5.6%, Loss:172.36412109375\n",
      "% Data Trained:6.1%, Loss:171.50666015625\n",
      "% Data Trained:6.6%, Loss:165.360703125\n",
      "% Data Trained:7.1%, Loss:171.38630859375\n",
      "% Data Trained:7.6%, Loss:158.465791015625\n",
      "% Data Trained:8.1%, Loss:158.338154296875\n",
      "% Data Trained:8.6%, Loss:157.676396484375\n",
      "% Data Trained:9.1%, Loss:154.971005859375\n",
      "% Data Trained:9.6%, Loss:155.43181640625\n",
      "% Data Trained:10.2%, Loss:205.7733984375\n",
      "% Data Trained:10.7%, Loss:165.13072265625\n",
      "% Data Trained:11.2%, Loss:147.9203515625\n",
      "% Data Trained:11.7%, Loss:164.3741796875\n",
      "% Data Trained:12.2%, Loss:158.768544921875\n",
      "% Data Trained:12.7%, Loss:155.1326953125\n",
      "% Data Trained:13.2%, Loss:185.803515625\n",
      "% Data Trained:13.7%, Loss:163.02080078125\n",
      "% Data Trained:14.2%, Loss:170.9643359375\n",
      "% Data Trained:14.7%, Loss:152.64833984375\n",
      "% Data Trained:15.2%, Loss:171.040078125\n",
      "% Data Trained:15.7%, Loss:160.7074609375\n",
      "% Data Trained:16.2%, Loss:139.823193359375\n",
      "% Data Trained:16.8%, Loss:164.17453125\n",
      "% Data Trained:17.3%, Loss:141.642470703125\n",
      "% Data Trained:17.8%, Loss:162.6703515625\n",
      "% Data Trained:18.3%, Loss:154.855791015625\n",
      "% Data Trained:18.8%, Loss:154.580380859375\n",
      "% Data Trained:19.3%, Loss:170.29033203125\n",
      "% Data Trained:19.8%, Loss:199.76421875\n",
      "% Data Trained:20.3%, Loss:171.5242578125\n",
      "% Data Trained:20.8%, Loss:157.012802734375\n",
      "% Data Trained:21.3%, Loss:167.52359375\n",
      "% Data Trained:21.8%, Loss:204.43998046875\n",
      "% Data Trained:22.3%, Loss:180.5578515625\n",
      "% Data Trained:22.8%, Loss:160.662802734375\n",
      "% Data Trained:23.4%, Loss:143.480595703125\n",
      "% Data Trained:23.9%, Loss:184.2523046875\n",
      "% Data Trained:24.4%, Loss:157.836005859375\n",
      "% Data Trained:24.9%, Loss:177.6571875\n",
      "% Data Trained:25.4%, Loss:165.32884765625\n",
      "% Data Trained:25.9%, Loss:206.114140625\n",
      "% Data Trained:26.4%, Loss:180.6568359375\n",
      "% Data Trained:26.9%, Loss:145.23462890625\n",
      "% Data Trained:27.4%, Loss:163.29724609375\n",
      "% Data Trained:27.9%, Loss:173.11546875\n",
      "% Data Trained:28.4%, Loss:160.820712890625\n",
      "% Data Trained:28.9%, Loss:148.6122265625\n",
      "% Data Trained:29.4%, Loss:162.94576171875\n",
      "% Data Trained:29.9%, Loss:162.084169921875\n",
      "% Data Trained:30.5%, Loss:145.3273828125\n",
      "% Data Trained:31.0%, Loss:146.20998046875\n",
      "% Data Trained:31.5%, Loss:135.9225390625\n",
      "% Data Trained:32.0%, Loss:162.95615234375\n",
      "% Data Trained:32.5%, Loss:146.4094140625\n",
      "% Data Trained:33.0%, Loss:135.3983984375\n",
      "% Data Trained:33.5%, Loss:164.89275390625\n",
      "% Data Trained:34.0%, Loss:172.35341796875\n",
      "% Data Trained:34.5%, Loss:152.816650390625\n",
      "% Data Trained:35.0%, Loss:136.275908203125\n",
      "% Data Trained:35.5%, Loss:166.51759765625\n",
      "% Data Trained:36.0%, Loss:153.52470703125\n",
      "% Data Trained:36.5%, Loss:138.618203125\n",
      "% Data Trained:37.1%, Loss:158.576865234375\n",
      "% Data Trained:37.6%, Loss:162.098017578125\n",
      "% Data Trained:38.1%, Loss:164.57859375\n",
      "% Data Trained:38.6%, Loss:155.391259765625\n",
      "% Data Trained:39.1%, Loss:144.5375\n",
      "% Data Trained:39.6%, Loss:192.4660546875\n",
      "% Data Trained:40.1%, Loss:165.89185546875\n",
      "% Data Trained:40.6%, Loss:182.22212890625\n",
      "% Data Trained:41.1%, Loss:165.72517578125\n",
      "% Data Trained:41.6%, Loss:159.151142578125\n",
      "% Data Trained:42.1%, Loss:152.0012890625\n",
      "% Data Trained:42.6%, Loss:191.73849609375\n",
      "% Data Trained:43.1%, Loss:140.43931640625\n",
      "% Data Trained:43.7%, Loss:163.98720703125\n",
      "% Data Trained:44.2%, Loss:148.317646484375\n",
      "% Data Trained:44.7%, Loss:172.10947265625\n",
      "% Data Trained:45.2%, Loss:158.254912109375\n",
      "% Data Trained:45.7%, Loss:198.2635546875\n",
      "% Data Trained:46.2%, Loss:165.111171875\n",
      "% Data Trained:46.7%, Loss:174.3544140625\n",
      "% Data Trained:47.2%, Loss:144.522861328125\n",
      "% Data Trained:47.7%, Loss:175.34044921875\n",
      "% Data Trained:48.2%, Loss:124.521806640625\n",
      "% Data Trained:48.7%, Loss:154.910390625\n",
      "% Data Trained:49.2%, Loss:142.011748046875\n",
      "% Data Trained:49.7%, Loss:172.48658203125\n",
      "% Data Trained:50.3%, Loss:181.3166015625\n",
      "% Data Trained:50.8%, Loss:132.122734375\n",
      "% Data Trained:51.3%, Loss:142.648310546875\n",
      "% Data Trained:51.8%, Loss:196.73490234375\n",
      "% Data Trained:52.3%, Loss:165.3855078125\n",
      "% Data Trained:52.8%, Loss:142.7726171875\n",
      "% Data Trained:53.3%, Loss:152.187197265625\n",
      "% Data Trained:53.8%, Loss:158.8576953125\n",
      "% Data Trained:54.3%, Loss:181.482265625\n",
      "% Data Trained:54.8%, Loss:159.85919921875\n",
      "% Data Trained:55.3%, Loss:170.10056640625\n",
      "% Data Trained:55.8%, Loss:165.90626953125\n",
      "% Data Trained:56.3%, Loss:156.110654296875\n",
      "% Data Trained:56.9%, Loss:174.5191015625\n",
      "% Data Trained:57.4%, Loss:171.0271484375\n",
      "% Data Trained:57.9%, Loss:153.03953125\n",
      "% Data Trained:58.4%, Loss:155.642626953125\n",
      "% Data Trained:58.9%, Loss:174.26095703125\n",
      "% Data Trained:59.4%, Loss:151.730537109375\n",
      "% Data Trained:59.9%, Loss:130.047724609375\n",
      "% Data Trained:60.4%, Loss:155.779619140625\n",
      "% Data Trained:60.9%, Loss:151.71505859375\n",
      "% Data Trained:61.4%, Loss:183.8212109375\n",
      "% Data Trained:61.9%, Loss:192.02689453125\n",
      "% Data Trained:62.4%, Loss:155.40849609375\n",
      "% Data Trained:62.9%, Loss:134.478984375\n",
      "% Data Trained:63.5%, Loss:181.026015625\n",
      "% Data Trained:64.0%, Loss:168.79767578125\n",
      "% Data Trained:64.5%, Loss:158.9008203125\n",
      "% Data Trained:65.0%, Loss:158.81775390625\n",
      "% Data Trained:65.5%, Loss:141.892763671875\n",
      "% Data Trained:66.0%, Loss:171.29587890625\n",
      "% Data Trained:66.5%, Loss:176.541640625\n",
      "% Data Trained:67.0%, Loss:151.56962890625\n",
      "% Data Trained:67.5%, Loss:163.084306640625\n",
      "% Data Trained:68.0%, Loss:173.809453125\n",
      "% Data Trained:68.5%, Loss:170.9158203125\n",
      "% Data Trained:69.0%, Loss:141.446171875\n",
      "% Data Trained:69.5%, Loss:161.58552734375\n",
      "% Data Trained:70.1%, Loss:159.430478515625\n",
      "% Data Trained:70.6%, Loss:207.06837890625\n",
      "% Data Trained:71.1%, Loss:159.845048828125\n",
      "% Data Trained:71.6%, Loss:170.28779296875\n",
      "% Data Trained:72.1%, Loss:152.51587890625\n",
      "% Data Trained:72.6%, Loss:187.82294921875\n",
      "% Data Trained:73.1%, Loss:168.1106640625\n",
      "% Data Trained:73.6%, Loss:178.10076171875\n",
      "% Data Trained:74.1%, Loss:200.2030859375\n",
      "% Data Trained:74.6%, Loss:157.751533203125\n",
      "% Data Trained:75.1%, Loss:156.984892578125\n",
      "% Data Trained:75.6%, Loss:146.55951171875\n",
      "% Data Trained:76.1%, Loss:146.19607421875\n",
      "% Data Trained:76.6%, Loss:178.85982421875\n",
      "% Data Trained:77.2%, Loss:155.92349609375\n",
      "% Data Trained:77.7%, Loss:170.68458984375\n",
      "% Data Trained:78.2%, Loss:156.46505859375\n",
      "% Data Trained:78.7%, Loss:169.59056640625\n",
      "% Data Trained:79.2%, Loss:162.81275390625\n",
      "% Data Trained:79.7%, Loss:161.164150390625\n",
      "% Data Trained:80.2%, Loss:171.8208203125\n",
      "% Data Trained:80.7%, Loss:150.510322265625\n",
      "% Data Trained:81.2%, Loss:166.38439453125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Data Trained:81.7%, Loss:175.95439453125\n",
      "% Data Trained:82.2%, Loss:162.936748046875\n",
      "% Data Trained:82.7%, Loss:160.564892578125\n",
      "% Data Trained:83.2%, Loss:158.31765625\n",
      "% Data Trained:83.8%, Loss:174.03080078125\n",
      "% Data Trained:84.3%, Loss:160.781318359375\n",
      "% Data Trained:84.8%, Loss:191.5143359375\n",
      "% Data Trained:85.3%, Loss:151.347041015625\n",
      "% Data Trained:85.8%, Loss:172.49857421875\n",
      "% Data Trained:86.3%, Loss:165.723359375\n",
      "% Data Trained:86.8%, Loss:167.625078125\n",
      "% Data Trained:87.3%, Loss:157.650390625\n",
      "% Data Trained:87.8%, Loss:174.69119140625\n",
      "% Data Trained:88.3%, Loss:166.72486328125\n",
      "% Data Trained:88.8%, Loss:155.49498046875\n",
      "% Data Trained:89.3%, Loss:195.0891796875\n",
      "% Data Trained:89.8%, Loss:158.348203125\n",
      "% Data Trained:90.4%, Loss:145.038017578125\n",
      "% Data Trained:90.9%, Loss:163.950390625\n",
      "% Data Trained:91.4%, Loss:161.1237890625\n",
      "% Data Trained:91.9%, Loss:148.942470703125\n",
      "% Data Trained:92.4%, Loss:167.891484375\n",
      "% Data Trained:92.9%, Loss:162.51734375\n",
      "% Data Trained:93.4%, Loss:145.03572265625\n",
      "% Data Trained:93.9%, Loss:195.553046875\n",
      "% Data Trained:94.4%, Loss:153.741337890625\n",
      "% Data Trained:94.9%, Loss:154.65302734375\n",
      "% Data Trained:95.4%, Loss:151.453427734375\n",
      "% Data Trained:95.9%, Loss:152.320078125\n",
      "% Data Trained:96.4%, Loss:178.79169921875\n",
      "% Data Trained:97.0%, Loss:139.07873046875\n",
      "% Data Trained:97.5%, Loss:157.937626953125\n",
      "% Data Trained:98.0%, Loss:165.54951171875\n",
      "% Data Trained:98.5%, Loss:171.05490234375\n",
      "% Data Trained:99.0%, Loss:170.2331640625\n",
      "% Data Trained:99.5%, Loss:159.540791015625\n",
      "% Data Trained:100.0%, Loss:177.3615234375\n",
      "----------------------------------------------\n",
      "Training Epoch:8\n",
      "----------------------------------------------\n",
      "% Data Trained:0.0%, Loss:151.922021484375\n",
      "% Data Trained:0.5%, Loss:159.136884765625\n",
      "% Data Trained:1.0%, Loss:152.812119140625\n",
      "% Data Trained:1.5%, Loss:162.682587890625\n",
      "% Data Trained:2.0%, Loss:146.4427734375\n",
      "% Data Trained:2.5%, Loss:153.512314453125\n",
      "% Data Trained:3.0%, Loss:145.580078125\n",
      "% Data Trained:3.6%, Loss:156.5010546875\n",
      "% Data Trained:4.1%, Loss:152.152705078125\n",
      "% Data Trained:4.6%, Loss:187.37765625\n",
      "% Data Trained:5.1%, Loss:146.72037109375\n",
      "% Data Trained:5.6%, Loss:171.45181640625\n",
      "% Data Trained:6.1%, Loss:171.37884765625\n",
      "% Data Trained:6.6%, Loss:164.249296875\n",
      "% Data Trained:7.1%, Loss:171.4427734375\n",
      "% Data Trained:7.6%, Loss:157.870693359375\n",
      "% Data Trained:8.1%, Loss:157.35185546875\n",
      "% Data Trained:8.6%, Loss:157.605419921875\n",
      "% Data Trained:9.1%, Loss:154.5142578125\n",
      "% Data Trained:9.6%, Loss:154.573662109375\n",
      "% Data Trained:10.2%, Loss:205.18802734375\n",
      "% Data Trained:10.7%, Loss:163.8542578125\n",
      "% Data Trained:11.2%, Loss:147.908408203125\n",
      "% Data Trained:11.7%, Loss:163.7858203125\n",
      "% Data Trained:12.2%, Loss:158.122333984375\n",
      "% Data Trained:12.7%, Loss:155.735107421875\n",
      "% Data Trained:13.2%, Loss:184.2898046875\n",
      "% Data Trained:13.7%, Loss:164.1523046875\n",
      "% Data Trained:14.2%, Loss:170.39369140625\n",
      "% Data Trained:14.7%, Loss:152.1679296875\n",
      "% Data Trained:15.2%, Loss:169.80650390625\n",
      "% Data Trained:15.7%, Loss:159.966650390625\n",
      "% Data Trained:16.2%, Loss:139.22224609375\n",
      "% Data Trained:16.8%, Loss:164.2214453125\n",
      "% Data Trained:17.3%, Loss:140.467568359375\n",
      "% Data Trained:17.8%, Loss:162.384658203125\n",
      "% Data Trained:18.3%, Loss:155.0530859375\n",
      "% Data Trained:18.8%, Loss:155.9738671875\n",
      "% Data Trained:19.3%, Loss:168.95173828125\n",
      "% Data Trained:19.8%, Loss:199.5093359375\n",
      "% Data Trained:20.3%, Loss:169.6030859375\n",
      "% Data Trained:20.8%, Loss:153.5811328125\n",
      "% Data Trained:21.3%, Loss:168.25091796875\n",
      "% Data Trained:21.8%, Loss:202.88404296875\n",
      "% Data Trained:22.3%, Loss:180.13595703125\n",
      "% Data Trained:22.8%, Loss:162.213857421875\n",
      "% Data Trained:23.4%, Loss:141.699482421875\n",
      "% Data Trained:23.9%, Loss:185.36029296875\n",
      "% Data Trained:24.4%, Loss:159.588544921875\n",
      "% Data Trained:24.9%, Loss:175.943515625\n",
      "% Data Trained:25.4%, Loss:163.534189453125\n",
      "% Data Trained:25.9%, Loss:203.95744140625\n",
      "% Data Trained:26.4%, Loss:180.18763671875\n",
      "% Data Trained:26.9%, Loss:143.670595703125\n",
      "% Data Trained:27.4%, Loss:163.8234375\n",
      "% Data Trained:27.9%, Loss:173.65326171875\n",
      "% Data Trained:28.4%, Loss:159.014453125\n",
      "% Data Trained:28.9%, Loss:147.973759765625\n",
      "% Data Trained:29.4%, Loss:160.5534765625\n",
      "% Data Trained:29.9%, Loss:161.18208984375\n",
      "% Data Trained:30.5%, Loss:144.681318359375\n",
      "% Data Trained:31.0%, Loss:146.549375\n",
      "% Data Trained:31.5%, Loss:136.1269921875\n",
      "% Data Trained:32.0%, Loss:162.726337890625\n",
      "% Data Trained:32.5%, Loss:146.983896484375\n",
      "% Data Trained:33.0%, Loss:134.951123046875\n",
      "% Data Trained:33.5%, Loss:163.11138671875\n",
      "% Data Trained:34.0%, Loss:170.566953125\n",
      "% Data Trained:34.5%, Loss:153.030244140625\n",
      "% Data Trained:35.0%, Loss:136.574453125\n",
      "% Data Trained:35.5%, Loss:166.332578125\n",
      "% Data Trained:36.0%, Loss:152.950810546875\n",
      "% Data Trained:36.5%, Loss:138.116484375\n",
      "% Data Trained:37.1%, Loss:158.145673828125\n",
      "% Data Trained:37.6%, Loss:163.0620703125\n",
      "% Data Trained:38.1%, Loss:164.28716796875\n",
      "% Data Trained:38.6%, Loss:154.864912109375\n",
      "% Data Trained:39.1%, Loss:143.15515625\n",
      "% Data Trained:39.6%, Loss:192.210625\n",
      "% Data Trained:40.1%, Loss:165.8198046875\n",
      "% Data Trained:40.6%, Loss:181.9605078125\n",
      "% Data Trained:41.1%, Loss:165.3013671875\n",
      "% Data Trained:41.6%, Loss:158.28171875\n",
      "% Data Trained:42.1%, Loss:151.65728515625\n",
      "% Data Trained:42.6%, Loss:190.75037109375\n",
      "% Data Trained:43.1%, Loss:141.4039453125\n",
      "% Data Trained:43.7%, Loss:162.34369140625\n",
      "% Data Trained:44.2%, Loss:148.426845703125\n",
      "% Data Trained:44.7%, Loss:171.51513671875\n",
      "% Data Trained:45.2%, Loss:157.799755859375\n",
      "% Data Trained:45.7%, Loss:196.50232421875\n",
      "% Data Trained:46.2%, Loss:164.94673828125\n",
      "% Data Trained:46.7%, Loss:174.35220703125\n",
      "% Data Trained:47.2%, Loss:144.35884765625\n",
      "% Data Trained:47.7%, Loss:174.32369140625\n",
      "% Data Trained:48.2%, Loss:124.239228515625\n",
      "% Data Trained:48.7%, Loss:154.0779296875\n",
      "% Data Trained:49.2%, Loss:141.706845703125\n",
      "% Data Trained:49.7%, Loss:172.65478515625\n",
      "% Data Trained:50.3%, Loss:180.2617578125\n",
      "% Data Trained:50.8%, Loss:131.9401953125\n",
      "% Data Trained:51.3%, Loss:142.644677734375\n",
      "% Data Trained:51.8%, Loss:195.577421875\n",
      "% Data Trained:52.3%, Loss:165.49685546875\n",
      "% Data Trained:52.8%, Loss:142.142099609375\n",
      "% Data Trained:53.3%, Loss:151.230439453125\n",
      "% Data Trained:53.8%, Loss:158.245947265625\n",
      "% Data Trained:54.3%, Loss:180.24810546875\n",
      "% Data Trained:54.8%, Loss:158.31203125\n",
      "% Data Trained:55.3%, Loss:169.92259765625\n",
      "% Data Trained:55.8%, Loss:165.89294921875\n",
      "% Data Trained:56.3%, Loss:154.788134765625\n",
      "% Data Trained:56.9%, Loss:173.6570703125\n",
      "% Data Trained:57.4%, Loss:170.84349609375\n",
      "% Data Trained:57.9%, Loss:151.892138671875\n",
      "% Data Trained:58.4%, Loss:155.42853515625\n",
      "% Data Trained:58.9%, Loss:173.42859375\n",
      "% Data Trained:59.4%, Loss:151.668095703125\n",
      "% Data Trained:59.9%, Loss:129.778251953125\n",
      "% Data Trained:60.4%, Loss:155.3689453125\n",
      "% Data Trained:60.9%, Loss:152.10701171875\n",
      "% Data Trained:61.4%, Loss:183.1020703125\n",
      "% Data Trained:61.9%, Loss:192.21814453125\n",
      "% Data Trained:62.4%, Loss:154.568388671875\n",
      "% Data Trained:62.9%, Loss:134.66044921875\n",
      "% Data Trained:63.5%, Loss:180.33203125\n",
      "% Data Trained:64.0%, Loss:167.956484375\n",
      "% Data Trained:64.5%, Loss:160.906748046875\n",
      "% Data Trained:65.0%, Loss:158.413720703125\n",
      "% Data Trained:65.5%, Loss:141.828642578125\n",
      "% Data Trained:66.0%, Loss:171.97841796875\n",
      "% Data Trained:66.5%, Loss:176.26654296875\n",
      "% Data Trained:67.0%, Loss:151.97203125\n",
      "% Data Trained:67.5%, Loss:162.83296875\n",
      "% Data Trained:68.0%, Loss:173.5328125\n",
      "% Data Trained:68.5%, Loss:170.85998046875\n",
      "% Data Trained:69.0%, Loss:140.700341796875\n",
      "% Data Trained:69.5%, Loss:160.006630859375\n",
      "% Data Trained:70.1%, Loss:160.10513671875\n",
      "% Data Trained:70.6%, Loss:207.4341015625\n",
      "% Data Trained:71.1%, Loss:159.1481640625\n",
      "% Data Trained:71.6%, Loss:171.557265625\n",
      "% Data Trained:72.1%, Loss:152.502998046875\n",
      "% Data Trained:72.6%, Loss:187.10796875\n",
      "% Data Trained:73.1%, Loss:170.45939453125\n",
      "% Data Trained:73.6%, Loss:178.28583984375\n",
      "% Data Trained:74.1%, Loss:199.95693359375\n",
      "% Data Trained:74.6%, Loss:159.45478515625\n",
      "% Data Trained:75.1%, Loss:155.63984375\n",
      "% Data Trained:75.6%, Loss:145.41396484375\n",
      "% Data Trained:76.1%, Loss:146.7330859375\n",
      "% Data Trained:76.6%, Loss:179.186875\n",
      "% Data Trained:77.2%, Loss:156.018271484375\n",
      "% Data Trained:77.7%, Loss:169.114765625\n",
      "% Data Trained:78.2%, Loss:155.826259765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Data Trained:78.7%, Loss:169.57591796875\n",
      "% Data Trained:79.2%, Loss:162.155634765625\n",
      "% Data Trained:79.7%, Loss:162.81619140625\n",
      "% Data Trained:80.2%, Loss:171.4309765625\n",
      "% Data Trained:80.7%, Loss:150.285673828125\n",
      "% Data Trained:81.2%, Loss:166.61275390625\n",
      "% Data Trained:81.7%, Loss:175.021796875\n",
      "% Data Trained:82.2%, Loss:163.873046875\n",
      "% Data Trained:82.7%, Loss:161.28291015625\n",
      "% Data Trained:83.2%, Loss:157.36607421875\n",
      "% Data Trained:83.8%, Loss:174.58798828125\n",
      "% Data Trained:84.3%, Loss:160.5287890625\n",
      "% Data Trained:84.8%, Loss:189.58533203125\n",
      "% Data Trained:85.3%, Loss:152.5477734375\n",
      "% Data Trained:85.8%, Loss:173.29908203125\n",
      "% Data Trained:86.3%, Loss:164.92056640625\n",
      "% Data Trained:86.8%, Loss:167.38041015625\n",
      "% Data Trained:87.3%, Loss:159.343662109375\n",
      "% Data Trained:87.8%, Loss:172.09107421875\n",
      "% Data Trained:88.3%, Loss:167.99005859375\n",
      "% Data Trained:88.8%, Loss:155.735478515625\n",
      "% Data Trained:89.3%, Loss:195.44751953125\n",
      "% Data Trained:89.8%, Loss:157.84365234375\n",
      "% Data Trained:90.4%, Loss:145.1473046875\n",
      "% Data Trained:90.9%, Loss:163.8701171875\n",
      "% Data Trained:91.4%, Loss:160.340625\n",
      "% Data Trained:91.9%, Loss:147.96189453125\n",
      "% Data Trained:92.4%, Loss:168.09005859375\n",
      "% Data Trained:92.9%, Loss:161.446494140625\n",
      "% Data Trained:93.4%, Loss:145.68177734375\n",
      "% Data Trained:93.9%, Loss:198.08365234375\n",
      "% Data Trained:94.4%, Loss:153.221015625\n",
      "% Data Trained:94.9%, Loss:153.51052734375\n",
      "% Data Trained:95.4%, Loss:152.0164453125\n",
      "% Data Trained:95.9%, Loss:152.53388671875\n",
      "% Data Trained:96.4%, Loss:179.38953125\n",
      "% Data Trained:97.0%, Loss:138.7536328125\n",
      "% Data Trained:97.5%, Loss:157.78908203125\n",
      "% Data Trained:98.0%, Loss:163.8515625\n",
      "% Data Trained:98.5%, Loss:170.86724609375\n",
      "% Data Trained:99.0%, Loss:171.02515625\n",
      "% Data Trained:99.5%, Loss:158.58951171875\n",
      "% Data Trained:100.0%, Loss:178.55421875\n",
      "----------------------------------------------\n",
      "Training Epoch:9\n",
      "----------------------------------------------\n",
      "% Data Trained:0.0%, Loss:151.9076171875\n",
      "% Data Trained:0.5%, Loss:157.81080078125\n",
      "% Data Trained:1.0%, Loss:151.543095703125\n",
      "% Data Trained:1.5%, Loss:161.951943359375\n",
      "% Data Trained:2.0%, Loss:144.701259765625\n",
      "% Data Trained:2.5%, Loss:153.071630859375\n",
      "% Data Trained:3.0%, Loss:144.9766796875\n",
      "% Data Trained:3.6%, Loss:154.785751953125\n",
      "% Data Trained:4.1%, Loss:151.713369140625\n",
      "% Data Trained:4.6%, Loss:188.12232421875\n",
      "% Data Trained:5.1%, Loss:144.706015625\n",
      "% Data Trained:5.6%, Loss:171.0080078125\n",
      "% Data Trained:6.1%, Loss:172.18853515625\n",
      "% Data Trained:6.6%, Loss:165.217421875\n",
      "% Data Trained:7.1%, Loss:173.0573828125\n",
      "% Data Trained:7.6%, Loss:156.61130859375\n",
      "% Data Trained:8.1%, Loss:157.1301171875\n",
      "% Data Trained:8.6%, Loss:158.21806640625\n",
      "% Data Trained:9.1%, Loss:154.950888671875\n",
      "% Data Trained:9.6%, Loss:154.21060546875\n",
      "% Data Trained:10.2%, Loss:204.81103515625\n",
      "% Data Trained:10.7%, Loss:164.31056640625\n",
      "% Data Trained:11.2%, Loss:147.341435546875\n",
      "% Data Trained:11.7%, Loss:164.0909375\n",
      "% Data Trained:12.2%, Loss:157.85818359375\n",
      "% Data Trained:12.7%, Loss:153.92888671875\n",
      "% Data Trained:13.2%, Loss:184.1253125\n",
      "% Data Trained:13.7%, Loss:161.600068359375\n",
      "% Data Trained:14.2%, Loss:169.95115234375\n",
      "% Data Trained:14.7%, Loss:150.446474609375\n",
      "% Data Trained:15.2%, Loss:169.28435546875\n",
      "% Data Trained:15.7%, Loss:159.9008984375\n",
      "% Data Trained:16.2%, Loss:137.45244140625\n",
      "% Data Trained:16.8%, Loss:163.266123046875\n",
      "% Data Trained:17.3%, Loss:140.9869921875\n",
      "% Data Trained:17.8%, Loss:160.124150390625\n",
      "% Data Trained:18.3%, Loss:153.587900390625\n",
      "% Data Trained:18.8%, Loss:151.54400390625\n",
      "% Data Trained:19.3%, Loss:167.5212109375\n",
      "% Data Trained:19.8%, Loss:197.54279296875\n",
      "% Data Trained:20.3%, Loss:167.2958984375\n",
      "% Data Trained:20.8%, Loss:154.132646484375\n",
      "% Data Trained:21.3%, Loss:166.3473828125\n",
      "% Data Trained:21.8%, Loss:200.95248046875\n",
      "% Data Trained:22.3%, Loss:179.79556640625\n",
      "% Data Trained:22.8%, Loss:161.47068359375\n",
      "% Data Trained:23.4%, Loss:140.297607421875\n",
      "% Data Trained:23.9%, Loss:182.22611328125\n",
      "% Data Trained:24.4%, Loss:155.932744140625\n",
      "% Data Trained:24.9%, Loss:174.65390625\n",
      "% Data Trained:25.4%, Loss:160.924541015625\n",
      "% Data Trained:25.9%, Loss:202.86548828125\n",
      "% Data Trained:26.4%, Loss:176.79927734375\n",
      "% Data Trained:26.9%, Loss:140.4846875\n",
      "% Data Trained:27.4%, Loss:162.634482421875\n",
      "% Data Trained:27.9%, Loss:169.4592578125\n",
      "% Data Trained:28.4%, Loss:157.267666015625\n",
      "% Data Trained:28.9%, Loss:146.671318359375\n",
      "% Data Trained:29.4%, Loss:158.423017578125\n",
      "% Data Trained:29.9%, Loss:157.934384765625\n",
      "% Data Trained:30.5%, Loss:142.85185546875\n",
      "% Data Trained:31.0%, Loss:144.0190625\n",
      "% Data Trained:31.5%, Loss:133.412109375\n",
      "% Data Trained:32.0%, Loss:159.73982421875\n",
      "% Data Trained:32.5%, Loss:143.9030859375\n",
      "% Data Trained:33.0%, Loss:131.047001953125\n",
      "% Data Trained:33.5%, Loss:162.11478515625\n",
      "% Data Trained:34.0%, Loss:170.6765234375\n",
      "% Data Trained:34.5%, Loss:151.172392578125\n",
      "% Data Trained:35.0%, Loss:136.334765625\n",
      "% Data Trained:35.5%, Loss:167.04033203125\n",
      "% Data Trained:36.0%, Loss:152.844609375\n",
      "% Data Trained:36.5%, Loss:139.094169921875\n",
      "% Data Trained:37.1%, Loss:157.71552734375\n",
      "% Data Trained:37.6%, Loss:161.785595703125\n",
      "% Data Trained:38.1%, Loss:164.6993359375\n",
      "% Data Trained:38.6%, Loss:153.604248046875\n",
      "% Data Trained:39.1%, Loss:143.817412109375\n",
      "% Data Trained:39.6%, Loss:192.85689453125\n",
      "% Data Trained:40.1%, Loss:164.53275390625\n",
      "% Data Trained:40.6%, Loss:179.86416015625\n",
      "% Data Trained:41.1%, Loss:163.5576171875\n",
      "% Data Trained:41.6%, Loss:158.87916015625\n",
      "% Data Trained:42.1%, Loss:150.49453125\n",
      "% Data Trained:42.6%, Loss:191.44130859375\n",
      "% Data Trained:43.1%, Loss:139.712255859375\n",
      "% Data Trained:43.7%, Loss:163.311103515625\n",
      "% Data Trained:44.2%, Loss:146.15478515625\n",
      "% Data Trained:44.7%, Loss:169.9946875\n",
      "% Data Trained:45.2%, Loss:154.538037109375\n",
      "% Data Trained:45.7%, Loss:197.15033203125\n",
      "% Data Trained:46.2%, Loss:162.85451171875\n",
      "% Data Trained:46.7%, Loss:173.011796875\n",
      "% Data Trained:47.2%, Loss:141.8337109375\n",
      "% Data Trained:47.7%, Loss:172.955390625\n",
      "% Data Trained:48.2%, Loss:123.55958984375\n",
      "% Data Trained:48.7%, Loss:152.256953125\n",
      "% Data Trained:49.2%, Loss:138.511025390625\n",
      "% Data Trained:49.7%, Loss:169.37544921875\n",
      "% Data Trained:50.3%, Loss:177.3296484375\n",
      "% Data Trained:50.8%, Loss:131.038515625\n",
      "% Data Trained:51.3%, Loss:139.479423828125\n",
      "% Data Trained:51.8%, Loss:191.569296875\n",
      "% Data Trained:52.3%, Loss:163.620576171875\n",
      "% Data Trained:52.8%, Loss:141.157353515625\n",
      "% Data Trained:53.3%, Loss:148.606904296875\n",
      "% Data Trained:53.8%, Loss:155.559384765625\n",
      "% Data Trained:54.3%, Loss:178.09509765625\n",
      "% Data Trained:54.8%, Loss:154.87759765625\n",
      "% Data Trained:55.3%, Loss:167.28654296875\n",
      "% Data Trained:55.8%, Loss:164.43044921875\n",
      "% Data Trained:56.3%, Loss:152.394599609375\n",
      "% Data Trained:56.9%, Loss:171.69568359375\n",
      "% Data Trained:57.4%, Loss:168.101015625\n",
      "% Data Trained:57.9%, Loss:149.469677734375\n",
      "% Data Trained:58.4%, Loss:152.237744140625\n",
      "% Data Trained:58.9%, Loss:170.56900390625\n",
      "% Data Trained:59.4%, Loss:148.49203125\n",
      "% Data Trained:59.9%, Loss:128.17439453125\n",
      "% Data Trained:60.4%, Loss:153.44265625\n",
      "% Data Trained:60.9%, Loss:149.61177734375\n",
      "% Data Trained:61.4%, Loss:181.3177734375\n",
      "% Data Trained:61.9%, Loss:186.38310546875\n",
      "% Data Trained:62.4%, Loss:152.276982421875\n",
      "% Data Trained:62.9%, Loss:132.604541015625\n",
      "% Data Trained:63.5%, Loss:178.3377734375\n",
      "% Data Trained:64.0%, Loss:164.43951171875\n",
      "% Data Trained:64.5%, Loss:155.617841796875\n",
      "% Data Trained:65.0%, Loss:156.32267578125\n",
      "% Data Trained:65.5%, Loss:140.716728515625\n",
      "% Data Trained:66.0%, Loss:168.1854296875\n",
      "% Data Trained:66.5%, Loss:171.124453125\n",
      "% Data Trained:67.0%, Loss:148.778251953125\n",
      "% Data Trained:67.5%, Loss:162.2297265625\n",
      "% Data Trained:68.0%, Loss:170.89642578125\n",
      "% Data Trained:68.5%, Loss:169.26859375\n",
      "% Data Trained:69.0%, Loss:140.225380859375\n",
      "% Data Trained:69.5%, Loss:157.47119140625\n",
      "% Data Trained:70.1%, Loss:155.30646484375\n",
      "% Data Trained:70.6%, Loss:201.8970703125\n",
      "% Data Trained:71.1%, Loss:156.30484375\n",
      "% Data Trained:71.6%, Loss:171.19931640625\n",
      "% Data Trained:72.1%, Loss:149.43013671875\n",
      "% Data Trained:72.6%, Loss:183.3891015625\n",
      "% Data Trained:73.1%, Loss:165.2453515625\n",
      "% Data Trained:73.6%, Loss:174.45818359375\n",
      "% Data Trained:74.1%, Loss:197.1407421875\n",
      "% Data Trained:74.6%, Loss:152.686162109375\n",
      "% Data Trained:75.1%, Loss:155.3733203125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Data Trained:75.6%, Loss:146.944189453125\n",
      "% Data Trained:76.1%, Loss:144.30421875\n",
      "% Data Trained:76.6%, Loss:174.11580078125\n",
      "% Data Trained:77.2%, Loss:153.948037109375\n",
      "% Data Trained:77.7%, Loss:164.8001953125\n",
      "% Data Trained:78.2%, Loss:152.342216796875\n",
      "% Data Trained:78.7%, Loss:169.9855078125\n",
      "% Data Trained:79.2%, Loss:159.87701171875\n",
      "% Data Trained:79.7%, Loss:157.111142578125\n",
      "% Data Trained:80.2%, Loss:167.22876953125\n",
      "% Data Trained:80.7%, Loss:147.0623046875\n",
      "% Data Trained:81.2%, Loss:161.886328125\n",
      "% Data Trained:81.7%, Loss:171.81935546875\n",
      "% Data Trained:82.2%, Loss:158.73416015625\n",
      "% Data Trained:82.7%, Loss:158.898408203125\n",
      "% Data Trained:83.2%, Loss:154.951240234375\n",
      "% Data Trained:83.8%, Loss:171.2517578125\n",
      "% Data Trained:84.3%, Loss:155.60064453125\n",
      "% Data Trained:84.8%, Loss:186.75390625\n",
      "% Data Trained:85.3%, Loss:147.315283203125\n",
      "% Data Trained:85.8%, Loss:169.70875\n",
      "% Data Trained:86.3%, Loss:161.6978515625\n",
      "% Data Trained:86.8%, Loss:164.1523828125\n",
      "% Data Trained:87.3%, Loss:152.91240234375\n",
      "% Data Trained:87.8%, Loss:167.2752734375\n",
      "% Data Trained:88.3%, Loss:161.966240234375\n",
      "% Data Trained:88.8%, Loss:150.162412109375\n",
      "% Data Trained:89.3%, Loss:192.73615234375\n",
      "% Data Trained:89.8%, Loss:153.90541015625\n",
      "% Data Trained:90.4%, Loss:140.7423046875\n",
      "% Data Trained:90.9%, Loss:156.808671875\n",
      "% Data Trained:91.4%, Loss:156.666064453125\n",
      "% Data Trained:91.9%, Loss:145.234580078125\n",
      "% Data Trained:92.4%, Loss:162.32634765625\n",
      "% Data Trained:92.9%, Loss:155.799013671875\n",
      "% Data Trained:93.4%, Loss:142.444248046875\n",
      "% Data Trained:93.9%, Loss:189.10365234375\n",
      "% Data Trained:94.4%, Loss:148.21853515625\n",
      "% Data Trained:94.9%, Loss:149.561494140625\n",
      "% Data Trained:95.4%, Loss:146.931611328125\n",
      "% Data Trained:95.9%, Loss:146.71921875\n",
      "% Data Trained:96.4%, Loss:171.59853515625\n",
      "% Data Trained:97.0%, Loss:134.329658203125\n",
      "% Data Trained:97.5%, Loss:152.51044921875\n",
      "% Data Trained:98.0%, Loss:159.53705078125\n",
      "% Data Trained:98.5%, Loss:164.75806640625\n",
      "% Data Trained:99.0%, Loss:165.157265625\n",
      "% Data Trained:99.5%, Loss:151.789794921875\n",
      "% Data Trained:100.0%, Loss:171.7185546875\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 10):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample the images from the latent space z, sample range between $-4.0 \\leqslant z_1 \\leqslant 4.0$ and $-4.0 \\leqslant z_2 \\leqslant 4.0$. Image are sampled at steps of 0.2 in both directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    z = torch.randn(1600, 2).cuda()\n",
    "    count_x=-4.0\n",
    "    count_y=-4.0\n",
    "    x=0\n",
    "    y=0\n",
    "    for i in range (0,40):\n",
    "        for j in range (0,40):\n",
    "            z[x,0]=count_x\n",
    "            z[x,1]=count_y\n",
    "            x=x+1\n",
    "            count_y=count_y+0.2\n",
    "        y=y+1\n",
    "        count_x=count_x+0.2\n",
    "        count_y=-4.0\n",
    "        \n",
    "    sample = vae.decoder(z).cuda()\n",
    "    \n",
    "    save_image(sample.view(1600, 1, 100, 100), 'sample_z_space_' +str(epoch)+'.png',nrow=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    z = torch.randn(100, 2).cuda()    \n",
    "    sample = vae.decoder(z).cuda()\n",
    "    save_image(sample.view(100, 1, 100, 100), 'sample_random_' +str(epoch)+'.png',nrow=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
